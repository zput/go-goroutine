---
title: part3.2.3.1:运行用户代码时间过长调度
date: 2020-05-04 12:35:00
tags:
  - golang
  - goroutine
categories:
    - golang调度
toc: true
---

# 运行时间过长被调度的情况

- 运行用户代码时间过长;
- 因为系统调用,导致时间过长.


## 系统监控

> 我们能想到有系统监控才能查看是否代码是否过长.

![sysmon](https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20200923134823.png)

### 开启系统监控

- sysmon永远for循环```src/runtime/proc.go```

```go
func main() {
    //...

	if GOARCH != "wasm" { // no threads on wasm yet, so no sysmon
		systemstack(func() {
			newm(sysmon, nil)
		})
    }

    //...
}
```

![被动调度;唤醒](https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20200917111914.png)
上一章我们也分析了这个```newm```函数;所以在进入schedule()函数之前,会先执行这个```sysmon```函数.


## sysmon

这里我们只看抢占相关的,当retake返回非0,那么代表所有P不是空闲的状态,所以```idle=0```==>```usleep(delay)```只是休眠最少的时间,只有20us

```go
// Always runs without a P, so write barriers are not allowed.
//
//go:nowritebarrierrec
func sysmon() {
	lock(&sched.lock)
	sched.nmsys++ //增加记录系统线程的值的个数
	checkdead()
	unlock(&sched.lock)

	lasttrace := int64(0)
	idle := 0 // how many cycles in succession we had not wokeup somebody
	delay := uint32(0)
	for {
		if idle == 0 { // start with 20us sleep...
			delay = 20
		} else if idle > 50 { // start doubling the sleep after 1ms...
			delay *= 2
		}
		if delay > 10*1000 { // up to 10ms
			delay = 10 * 1000
		}
        usleep(delay)
        
        //...

		// retake P's blocked in syscalls
		// and preempt long running G's
		// 抢占被系统调用阻塞的P和抢占长期运行的G
		if retake(now) != 0 {
			idle = 0
		} else {
			idle++
		}
        // check if we need to force a GC
        //...
	}
}
```


## retake

- 只有P是```_Prunning or _Psyscall```,才会进行抢占
  - ```_Prunning```
    - 连续运行超过10毫秒了，设置抢占请求.
  - ```_Psyscall```: 当程序没有工作需要做,且系统调用没有超过10ms就不进行系统调用抢占.
    - 1和2说明这个程序没有工作需要做;
    - 3说明系统调用还没超过10m


```go
func retake(now int64) uint32 {
	n := 0
	// Prevent allp slice changes. This lock will be completely
	// uncontended unless we're already stopping the world.
	lock(&allpLock)
	// We can't use a range loop over allp because we may
	// temporarily drop the allpLock. Hence, we need to re-fetch
	// allp each time around the loop.
	for i := 0; i < len(allp); i++ { //遍历所有的P
		_p_ := allp[i]
		if _p_ == nil {
			// This can happen if procresize has grown
			// allp but not yet created new Ps.
			continue
		}
		pd := &_p_.sysmontick // 最后一次被sysmon观察到的tick
		s := _p_.status
		sysretake := false
		if s == _Prunning || s == _Psyscall { //只有当p处于 _Prunning 或 _Psyscall 状态时才会进行抢占
			// Preempt G if it's running for too long.
			t := int64(_p_.schedtick)  // _p_.schedtick：每发生一次调度，调度器对该值加一
			if int64(pd.schedtick) != t { // 监控线程监控到一次新的调度，所以重置跟sysmon相关的schedtick和schedwhen变量
				pd.schedtick = uint32(t)
				pd.schedwhen = now
			} else if pd.schedwhen+forcePreemptNS <= now { //  1. 没有进第一个if语句内,说明:pd.schedtick == t; 说明(pd.schedwhen ～ now)这段时间未发生过调度;
				preemptone(_p_)                            //  2. 但是这个_P_上面的某个Goroutine被执行,一直在执行这个Goroutiine; 中间没有切换其他Goroutine,因为如果切会导致_P_.schedtick增长,导致进入第一个if语句内;
				// In case of syscall, preemptone() doesn't // 3. 连续运行超过10毫秒了，设置抢占请求.
				// work, because there is no M wired to P.
				sysretake = true   // 需要系统抢占
			}
		}
		if s == _Psyscall { // P处于系统调用之中，需要检查是否需要抢占
			// Retake P from syscall if it's there for more than 1 sysmon tick (at least 20us).
			t := int64(_p_.syscalltick) // 用于记录系统调用的次数，主要由工作线程在完成系统调用之后加一
			if !sysretake && int64(pd.syscalltick) != t { // 不相等---说明已经不是上次观察到的系统调用,开始了一个新的系统调用,所以重置一下
				pd.syscalltick = uint32(t)
				pd.syscallwhen = now
				continue
			}
			// On the one hand we don't want to retake Ps if there is no other work to do,
			// but on the other hand we want to retake them eventually
			// because they can prevent the sysmon thread from deep sleep.

			// 1.  _p_的本地运行队列没有Gs; runqempty(_p_)返回true
			// 2. 有空闲的P,或者有正在自旋状态的M(正在偷其他P队列的Gs); atomic.Load(&sched.nmspinning)+atomic.Load(&sched.npidle) > 0返回true
			// 3. 上次观测到的系统调用还没有超过10毫秒; pd.syscallwhen+10*1000*1000 > now返回true
			// - concluing: 当程序没有工作需要做,且系统调用没有超过10ms就不进行系统调用抢占.
			//   - 1和2说明这个程序没有工作需要做;
			//   - 3说明系统调用还没超过10ms
			if runqempty(_p_) && atomic.Load(&sched.nmspinning)+atomic.Load(&sched.npidle) > 0 && pd.syscallwhen+10*1000*1000 > now {
				continue
			}
			// Drop allpLock so we can take sched.lock.
			unlock(&allpLock)
			// Need to decrement number of idle locked M's
			// (pretending that one more is running) before the CAS.
			// Otherwise the M from which we retake can exit the syscall,
			// increment nmidle and report deadlock.
			incidlelocked(-1)
			if atomic.Cas(&_p_.status, s, _Pidle) { // 需要抢占，则通过使用cas修改p的状态来获取p的使用权
				if trace.enabled {                  // CAS: 工作线程此时此刻可能正好从系统调用返回了，也正在获取p的使用权
					traceGoSysBlock(_p_)
					traceProcStop(_p_)
				}
				n++
				_p_.syscalltick++
				handoffp(_p_)  // 寻找一个新的m出来接管P
			}
			incidlelocked(1)
			lock(&allpLock)
		}
	}
	unlock(&allpLock)
	return uint32(n)
}
```

#### 执行用户代码抢占


> ```preemptone```设置了被抢占goroutine对应的g结构体中的 preempt成员为true和stackguard0成员为stackPreempt.

```go
// Tell the goroutine running on processor P to stop.
// This function is purely best-effort. It can incorrectly fail to inform the
// goroutine. It can send inform the wrong goroutine. Even if it informs the
// correct goroutine, that goroutine might ignore the request if it is
// simultaneously executing newstack.
// No lock needs to be held.
// Returns true if preemption request was issued.
// The actual preemption will happen at some point in the future
// and will be indicated by the gp->status no longer being
// Grunning
func preemptone(_p_ *p) bool {
	mp := _p_.m.ptr()
	if mp == nil || mp == getg().m {
		return false
	}
	gp := mp.curg // gp == 被抢占的goroutine
	if gp == nil || gp == mp.g0 {
		return false
	}

	gp.preempt = true // 设置抢占信号preempt == true

	// Every call in a go routine checks for stack overflow by
	// comparing the current stack pointer to gp->stackguard0.
	// Setting gp->stackguard0 to StackPreempt folds
	// preemption into the normal stack overflow check.

	// (1<<(8*sys.PtrSize) - 1) & -1314 ---> 0xfffffffffffffade, 很大的数
	gp.stackguard0 = stackPreempt //stackguard0==很大的数; 使被抢占的goroutine;在进行函数调用会去检查栈溢出;去处理抢占请求
	return true
}
```

#### 如何读取标志,然后进行抢占

> 通过```stackguard0```以及```preempt```可以找到这个链路:```morestack_noctxt()->morestack()->newstack()```.

// TODO zxc: reference part0:汇编基础.md/go编译器加的函数头的部分.

##### runtime·morestack

```
// morestack but not preserving ctxt.
TEXT runtime·morestack_noctxt(SB),NOSPLIT,$0
	MOVL	$0, DX
	JMP	runtime·morestack(SB)



/*
 * support for morestack
 */

// Called during function prolog when more stack is needed.
//
// The traceback routines see morestack on a g0 as being
// the top of a stack (for example, morestack calling newstack
// calling the scheduler calling newm calling gc), so we must
// record an argument size. For that purpose, it has no arguments.
TEXT runtime·morestack(SB),NOSPLIT,$0-0	
//开始是进行一些判断
	// Cannot grow scheduler stack (m->g0).
	//...
	// Cannot grow signal stack (m->gsignal).
	//...

//设置m->morebuf的PC，SP，g为相对应的'main'
	// Called from f.
	// Set m->morebuf to f's caller.
	NOP	SP	// tell vet SP changed - stop checking offsets
	MOVQ	8(SP), AX	// f's caller's PC // 这里的路径比如我的：  'main'--->'sub_function'。
                                           // 但是抢占了，所以走下面的路径:->morestack_noctxt()->morestack()->newstack()
                                           // 所以这里的f在我这里应该是main.
                                           // 需要注意morestack_noctxt与morestack使用的栈大小都是0，且他们的跳转没用call指令，使用的是JMP
	MOVQ	AX, (m_morebuf+gobuf_pc)(BX)
	LEAQ	16(SP), AX	// f's caller's SP
	MOVQ	AX, (m_morebuf+gobuf_sp)(BX)
	get_tls(CX)  //...
	MOVQ	g(CX), SI
	MOVQ	SI, (m_morebuf+gobuf_g)(BX)

//保存当前的寄存器信息到g->sched中
	// Set g->sched to context in f.
	MOVQ	0(SP), AX // f's PC
	MOVQ	AX, (g_sched+gobuf_pc)(SI)
	MOVQ	SI, (g_sched+gobuf_g)(SI)
	LEAQ	8(SP), AX // f's SP
	MOVQ	AX, (g_sched+gobuf_sp)(SI) // 在morestack里面就已经保存了sp的值。
	MOVQ	BP, (g_sched+gobuf_bp)(SI)
	MOVQ	DX, (g_sched+gobuf_ctxt)(SI)

//把g0设置为m当前运行的G; 把g0->sched->sp恢复到SP寄存器中;
	// Call newstack on m->g0's stack.
	MOVQ	m_g0(BX), BX
	MOVQ	BX, g(CX)
	MOVQ	(g_sched+gobuf_sp)(BX), SP // 把g0的栈SP寄存器恢复到实际的寄存器中。所以下面就使用了g0的栈。
//调用newstack
	CALL	runtime·newstack(SB)
	CALL	runtime·abort(SB)	// crash if newstack returns
	RET
```

- 总结就是：
  - 如果它下一次被调度起来了，那么执行PC，又会重新到本函数头部执行，从上面分析也可以知道，这里的风险就是，如果执行过程没有调用其他函数，那么无法进行抢占，这个就是基于插入抢占，1.14基于信号抢占。
  - morestack类似于mcall
    - 保存调用morestack函数的goroutine到它的sched成员。
	- 将当前工作线程的g0与线程TLS关联；
	- 将当前工作线程的g0栈恢复到CPU寄存器。
    - 在g0栈中执行传入的参数。--->这里是```runtime·newstack(SB)```函数。


##### newstack(SB)

```go
func newstack() {
	thisg := getg() //到这里我们又是在g0栈里面。

	//...

	gp := thisg.m.curg //这个就是原来的Goroutine.

	//...

	// NOTE: stackguard0 may change underfoot, if another thread
	// is about to try to preempt gp. Read it just once and use that same
	// value now and below.
	preempt := atomic.Loaduintptr(&gp.stackguard0) == stackPreempt //这里判断是否是抢占 打了stackguard0;

	// Be conservative about where we preempt.
	// We are interested in preempting user Go code, not runtime code.
	// If we're holding locks, mallocing, or preemption is disabled, don't
	// preempt.
	// This check is very early in newstack so that even the status change
	// from Grunning to Gwaiting and back doesn't happen in this case.
	// That status change by itself can be viewed as a small preemption,
	// because the GC might change Gwaiting to Gscanwaiting, and then
	// this goroutine has to wait for the GC to finish before continuing.
	// If the GC is in some way dependent on this goroutine (for example,
	// it needs a lock held by the goroutine), that small preemption turns
	// into a real deadlock.
	if preempt {
		// 这里还检查了一系列的状态，如果满足就不抢占它了， 让它继续执行。
		if thisg.m.locks != 0 || thisg.m.mallocing != 0 || thisg.m.preemptoff != "" || thisg.m.p.ptr().status != _Prunning {
			// Let the goroutine keep running for now.
			// gp->preempt is set, so it will be preempted next time.
			gp.stackguard0 = gp.stack.lo + _StackGuard //还原stackguard0为正常值，表示我们已经处理过抢占请求了
			gogo(&gp.sched) // never return
		}
	}

	if gp.stack.lo == 0 {
		throw("missing stack in newstack")
	}
	sp := gp.sched.sp
	if sys.ArchFamily == sys.AMD64 || sys.ArchFamily == sys.I386 || sys.ArchFamily == sys.WASM {
		// The call to morestack cost a word.
		sp -= sys.PtrSize
	}
	if stackDebug >= 1 || sp < gp.stack.lo {
		print("runtime: newstack sp=", hex(sp), " stack=[", hex(gp.stack.lo), ", ", hex(gp.stack.hi), "]\n",
			"\tmorebuf={pc:", hex(morebuf.pc), " sp:", hex(morebuf.sp), " lr:", hex(morebuf.lr), "}\n",
			"\tsched={pc:", hex(gp.sched.pc), " sp:", hex(gp.sched.sp), " lr:", hex(gp.sched.lr), " ctxt:", gp.sched.ctxt, "}\n")
	}
	if sp < gp.stack.lo {
		print("runtime: gp=", gp, ", goid=", gp.goid, ", gp->status=", hex(readgstatus(gp)), "\n ")
		print("runtime: split stack overflow: ", hex(sp), " < ", hex(gp.stack.lo), "\n")
		throw("runtime: split stack overflow")
	}

	if preempt {
		if gp == thisg.m.g0 {
			throw("runtime: preempt g0")
		}
		if thisg.m.p == 0 && thisg.m.locks == 0 {
			throw("runtime: g is running but p is not")
		}
		// Synchronize with scang.
		casgstatus(gp, _Grunning, _Gwaiting) // 设置gp状态变为等待状态。处理gc时把gp的状态修改成_Gwaiting
		if gp.preemptscan { //gc相关，暂时忽略。
			for !castogscanstatus(gp, _Gwaiting, _Gscanwaiting) {
				// Likely to be racing with the GC as
				// it sees a _Gwaiting and does the
				// stack scan. If so, gcworkdone will
				// be set and gcphasework will simply
				// return.
			}
			if !gp.gcscandone {
				// gcw is safe because we're on the
				// system stack.
				gcw := &gp.m.p.ptr().gcw
				scanstack(gp, gcw)
				gp.gcscandone = true
			}
			gp.preemptscan = false
			gp.preempt = false
			casfrom_Gscanstatus(gp, _Gscanwaiting, _Gwaiting)
			// This clears gcscanvalid.
			casgstatus(gp, _Gwaiting, _Grunning)
			gp.stackguard0 = gp.stack.lo + _StackGuard
			gogo(&gp.sched) // never return
		}

		// Act like goroutine called runtime.Gosched.
		casgstatus(gp, _Gwaiting, _Grunning) //恢复状态。
		gopreempt_m(gp) // 放入全局队列，重新schedule(); never return === gopreempt_m(gp)---call--->goschedImpl(gp)----call-->globrunqput()放入全局队列/schedule()
	}

	//...
}
```

> 这里就继续上面的，把替换掉的Goroutine重新放入全局队列：
>> ```gopreempt_m(gp)---call--->goschedImpl(gp)----call-->globrunqput()放入全局队列/schedule()```


## 栈增长相关代码

```go
func newstack() {
	//...省略抢占的代码

	// Allocate a bigger segment and move the stack.
	oldsize := gp.stack.hi - gp.stack.lo
	newsize := oldsize * 2  // 新的栈大小直接*2
	if newsize > maxstacksize {
		print("runtime: goroutine stack exceeds ", maxstacksize, "-byte limit\n")
		throw("stack overflow")
	}

	// The goroutine must be executing in order to call newstack,
	// so it must be Grunning (or Gscanrunning).
	casgstatus(gp, _Grunning, _Gcopystack)

	// The concurrent GC will not scan the stack while we are doing the copy since
	// the gp is in a Gcopystack status.
	copystack(gp, newsize, true)
	if stackDebug >= 1 {
		print("stack grow done\n")
	}
	casgstatus(gp, _Gcopystack, _Grunning)
	gogo(&gp.sched)
}
```

```go
func copystack(gp *g, newsize uintptr, sync bool) {
	//...

	// allocate new stack
	new := stackalloc(uint32(newsize))

	//...
}
```

- stackalloc

```go
// stackalloc allocates an n byte stack.
//
// stackalloc must run on the system stack because it uses per-P
// resources and must not split the stack.
//
//go:systemstack
func stackalloc(n uint32) stack {

	// Small stacks are allocated with a fixed-size free-list allocator.
	// If we need a stack of a bigger size, we fall back on allocating
	// a dedicated span.
	var v unsafe.Pointer
	if n < _FixedStack<<_NumStackOrders && n < _StackCacheSize {
		//小堆栈用固定大小的自由列表分配器进行分配。
	} else {
		//...
		if s == nil {
			// 如果我们需要一个更大的堆栈，我们会重新分配一个span.
			// Allocate a new stack from the heap.
			s = mheap_.allocManual(npage, &memstats.stacks_inuse)
			if s == nil {
				throw("out of memory")
			}
			osStackAlloc(s)
			s.elemsize = uintptr(n)
		}
		//...
	}
	//...
}
```
https://medium.com/a-journey-with-go/go-how-does-the-goroutine-stack-size-evolve-447fc02085e5#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjAzYjJkMjJjMmZlY2Y4NzNlZDE5ZTViOGNmNzA0YWZiN2UyZWQ0YmUiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MTIxNjkyMDEsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwMzk2OTgxODc3ODk3MjQyMjU0OSIsImVtYWlsIjoiZmZ6eGMuZG9AZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJ0aW0gWmhhbyIsInBpY3R1cmUiOiJodHRwczovL2xoNS5nb29nbGV1c2VyY29udGVudC5jb20vLVVrRk9jak5NWUJzL0FBQUFBQUFBQUFJL0FBQUFBQUFBQUFBL0FNWnV1Y25LajAwY21MVkhuZGpYakxGVVZWX1JHWnJ0OXcvczk2LWMvcGhvdG8uanBnIiwiZ2l2ZW5fbmFtZSI6InRpbSIsImZhbWlseV9uYW1lIjoiWmhhbyIsImlhdCI6MTYxMjE2OTUwMSwiZXhwIjoxNjEyMTczMTAxLCJqdGkiOiI5ZWZhMzYwMDUwOGNhZjg0MWMyYjQ5YmE1NDQxMWNkMGQzNmE0YzliIn0.qaSr0IXzZ3BXiIndb18-qLZpwNMDi3fEGlR-OtbQryxR8MkhONlgB-BTN5vHjYuvmWdCdGywJn26T71jPqBRVuIXMNlriZLFwPvHKdTBRrvpkPoFs8LprEpsCyZbTN7qNU5-CfDzcC1fHZji2j7992Ngo3XVd9v-6LiKCGUeolZ7CGH6KvT1e67ckiCzEN2oG5q6v7zKW32FmF7cajuOHl12p2pn6LaHxlYm3o38N3O9c96cO04meQ7WzZKn6QVoZeDIvmzq1iIKYOCbU0edZiOXgRgGHkBeBOowi-DHCz9kSJ1HkNrdzyjEC2nKUqerVGTCAc08w9BjtA8o_RmA8g




#### 执行系统调用抢占

> handoffp函数:判断是否需要启动工作线程来接管_p_，如果不需要则把_p_放入P的全局空闲队列.


```go
// Hands off P from syscall or locked M.
// Always runs without a P, so write barriers are not allowed.
//go:nowritebarrierrec
func handoffp(_p_ *p) {
	// handoffp must start an M in any situation where
	// findrunnable would return a G to run on _p_.

	// if it has local work, start it straight away
	if !runqempty(_p_) || sched.runqsize != 0 {  // 运行队列不为空，需要获得一个m来接管,而不是创建一个M结构体,和创建一个线程;
		startm(_p_, false) // 这个我们前面讨论过,
		return
	}
	// if it has GC work, start it straight away
	if gcBlackenEnabled != 0 && gcMarkWorkAvailable(_p_) {  // 如果有GC工作，就立即开始
		startm(_p_, false)
		return
	}
	// no local work, check that there are no spinning/idle M's,
	// otherwise our help is not required
	// 没有空闲的P,没有自旋状态的Ms;所有其它p都在运行goroutine，说明系统比较忙，需要启动m
	if atomic.Load(&sched.nmspinning)+atomic.Load(&sched.npidle) == 0 && atomic.Cas(&sched.nmspinning, 0, 1) { // TODO: fast atomic
		startm(_p_, true)
		return
	}
	lock(&sched.lock)
	if sched.gcwaiting != 0 {
		_p_.status = _Pgcstop
		sched.stopwait--
		if sched.stopwait == 0 {
			notewakeup(&sched.stopnote)
		}
		unlock(&sched.lock)
		return
	}
	if _p_.runSafePointFn != 0 && atomic.Cas(&_p_.runSafePointFn, 1, 0) {
		sched.safePointFn(_p_)
		sched.safePointWait--
		if sched.safePointWait == 0 {
			notewakeup(&sched.safePointNote)
		}
	}
	if sched.runqsize != 0 { // 全局运行队列大小不是0; 说明Goroutine需要运行,有工作要做.
		unlock(&sched.lock)
		startm(_p_, false)
		return
	}
	// If this is the last running P and nobody is polling network,
	// need to wakeup another M to poll network.
	// 所有其它P都已经处于空闲状态,只有自己一个P还在运行;
	// 且这时候需要监控网络连接读写事件，则需要启动新的m来poll网络连接
	if sched.npidle == uint32(gomaxprocs-1) && atomic.Load64(&sched.lastpoll) != 0 {
		unlock(&sched.lock)
		startm(_p_, false)
		return
	}
	pidleput(_p_)  // 无事可做，把p放入全局空闲队列
	unlock(&sched.lock)
}
```

> 需要启动工作线程来接管P.
1. _p_的本地运行队列或全局运行队列里面有待运行的goroutine；
2. 需要帮助gc完成标记工作；
3. 系统比较忙，所有其它_p_都在运行goroutine，需要帮忙；
4. 所有其它P都已经处于空闲状态，如果需要监控网络连接读写事件，则需要启动新的m来poll网络连接。



其中startm函数我们前面介绍过.
![startm](https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20200917153130.png)






## 额外的例子



### 定义程序

> main.go

```go
package main

import "fmt"

func call_some_job() {

	fmt.Println("complete this job")
}

func main() {
	for i:=0; i<100000; i++{
		i=i
	}
	call_some_job()
}
```
### gdb调试前准备

### 编译程序

编译一下源代码: ```go build  -gcflags "-N -l" -o test .```.

#### 准备mcall函数断点的文件

- gdb
  - ```list /usr/lib/golang/src/runtime/proc.go:267```
  - ```list /tmp/kubernets/test_preempt/main.go:1```
  - ```list /usr/lib/golang/src/runtime/asm_amd64.s:454```

#### gdb调试自定义函数

```gdb
define zxc
info threads
info register rbp rsp pc
end
```


#### gdb

```test
[root@gitlab test_preempt]# gdb ./test
GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-119.el7
Copyright (C) 2013 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-redhat-linux-gnu".
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>...
Reading symbols from /tmp/kubernets/test_preempt/test...done.
Loading Go Runtime support.
(gdb) list
1	package main
2
3	import "fmt"
4
5	func call_some_job() {
6
7		fmt.Println("complete this job")
8	}
9
10	func main() {
(gdb)
11		call_some_job()
12	}
(gdb) b 10
Breakpoint 1 at 0x48cf90: file /tmp/kubernets/test_preempt/main.go, line 10.
(gdb) run
Starting program: /tmp/kubernets/test_preempt/./test

Breakpoint 1, main.main () at /tmp/kubernets/test_preempt/main.go:10
10	func main() {
(gdb) disas
Dump of assembler code for function main.main:
=> 0x000000000048cf90 <+0>:	    mov    %fs:0xfffffffffffffff8,%rcx     --------------------------------here
   0x000000000048cf99 <+9>: 	cmp    0x10(%rcx),%rsp                     --------------------------------here
   0x000000000048cf9d <+13>:	jbe    0x48cfb9 <main.main+41>
   0x000000000048cf9f <+15>:	sub    $0x8,%rsp
   0x000000000048cfa3 <+19>:	mov    %rbp,(%rsp)
   0x000000000048cfa7 <+23>:	lea    (%rsp),%rbp
   0x000000000048cfab <+27>:	callq  0x48cef0 <main.call_some_job>
   0x000000000048cfb0 <+32>:	mov    (%rsp),%rbp
   0x000000000048cfb4 <+36>:	add    $0x8,%rsp
   0x000000000048cfb8 <+40>:	retq
   0x000000000048cfb9 <+41>:	callq  0x4517d0 <runtime.morestack_noctxt> --------------------------------here
   0x000000000048cfbe <+46>:	jmp    0x48cf90 <main.main>
End of assembler dump.
```

上面三个```--------------------------------here```,前面我们说的很清楚,就是g.stack.stackguard0与sp寄存器进行比较,如果sp小于g.stack.stackguard0
就跳转到```runtime.morestack_noctxt```;而我们前面设置preempt:```gp.stackguard0 = stackPreempt //stackguard0==很大的数; 使被抢占的goroutine;在进行函数调用会去检查栈溢出;去处理抢占请求```,它必定比sp要大,所以肯定跳转到了```runtime.morestack_noctxt```




> ```MOVQ	0(SP), AX // f's PC```,就是caller's pc是因为它的rbp在那一步还没有保存到callee‘s stack空间.

![MOVQ	0(SP), AX // f's PC](https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20200918212350.png)


那继续来看如果如果调用```<runtime.morestack_noctxt>```,它的下一个PC就是```jmp    0x48cf90 <main.main>```又重新跳回来了.
看这个
![disas](https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20200921193622.png)


