[{"id":0,"href":"/docs/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E8%B0%83%E5%BA%A6/%E4%BA%8C-%E8%B0%83%E5%BA%A6%E7%9A%84%E6%97%B6%E6%9C%BA/4.2.1-%E4%B8%BB%E5%8A%A8%E8%B0%83%E5%BA%A6/","title":"4.2.1 goroutine主动调度","section":"二 调度的时机","content":" 探寻runtime.Gosched() # 我们的Goroutine主动调度(runtime.Gosched()),就是主动放弃与M的关联,放入全局空闲G队列.\nsrc/runtime/proc.go:267\n// Gosched yields the processor, allowing other goroutines to run. It does not // suspend the current goroutine, so execution resumes automatically. func Gosched() { checkTimeouts() mcall(gosched_m) } 定义程序 # main.go\npackage main import ( \u0026#34;runtime\u0026#34; \u0026#34;sync/atomic\u0026#34; ) var existFlag int32 = 2 // the function\u0026#39;s body is empty func addAssemble(x, y int64) int64 func add(a int64){ addAssemble(a, a) atomic.AddInt32(\u0026amp;existFlag, -1) } func main() { runtime.GOMAXPROCS(1) go add(1) go add(2) for { if atomic.LoadInt32(\u0026amp;existFlag) \u0026lt;=0{ break } runtime.Gosched() } } add_amd.s\nTEXT ·addAssemble(SB),$0-24 MOVQ x+0(FP), BX MOVQ y+8(FP), BP ADDQ BP, BX MOVQ BX, ret+16(FP) RET gdb调试前准备 # 编译程序 # 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n准备mcall函数断点的文件 # gdb list /usr/lib/golang/src/runtime/proc.go:267 gdb调试自定义函数 # define zxc info threads info register rbp rsp pc step continue end gdb # gdb总结 # mcall 把主动放弃调度的Goroutine的保存寄存器的值到sched的sp,bp,pc; 也包括g指针自己. 把g0\u0026rsquo;s sched.SP恢复到寄存器SP,[这里没有pc,我们前面已经很详细的讨论过这个] 根据传入进来的函数指针,执行对应函数[在这里就是gosched_m函数],从这里开始就是使用的g0的栈. gosched_m函数 # // Gosched continuation on g0. func gosched_m(gp *g) { if trace.enabled { traceGoSched() } goschedImpl(gp) } func goschedImpl(gp *g) { status := readgstatus(gp) if status\u0026amp;^_Gscan != _Grunning { dumpgstatus(gp) throw(\u0026#34;bad g status\u0026#34;) } casgstatus(gp, _Grunning, _Grunnable)//改成可运行,而不是运行中的状态了 dropg() // m.curg = nil, gp.m = nil互相不关联 lock(\u0026amp;sched.lock) //因为要操作全局队列先加锁 globrunqput(gp) unlock(\u0026amp;sched.lock) //unlock schedule() //进入调度 } 这里要注意的一点就是是把这个Goroutine放入全局队列,而不是本地队列\n附录 # golang全局Goroutine队列 # 这里扩展一下什么是全局运行队列\nGlobal runnable queue.\n这个队列里面的Goroutine的状态都是_Grunnable,可以运行,随时可运行,只要能被调度起来,调度起来就变成了运行中_Grunning.\nvar ( allglen uintptr allm *m allp []*p // len(allp) == gomaxprocs; may change at safe points, otherwise immutable allpLock mutex // Protects P-less reads of allp and all writes gomaxprocs int32 ncpu int32 sched schedt //... ) type schedt struct { //... // Global runnable queue. 全局运行队列 runq gQueue runqsize int32 //... } // A gQueue is a dequeue of Gs linked through g.schedlink. A G can only // be on one gQueue or gList at a time. type gQueue struct { head guintptr tail guintptr } 全局可运行队列全部展现出来了,那么他们是怎么关联的,有头有尾是链表类型.\n全局可运行队列关联结构 # 我们回到最上面可以的globrunqput\n// Put gp on the global runnable queue. // Sched must be locked. // May run during STW, so write barriers are not allowed. //go:nowritebarrierrec func globrunqput(gp *g) { sched.runq.pushBack(gp) sched.runqsize++ } // pushBack adds gp to the tail of q. func (q *gQueue) pushBack(gp *g) { gp.schedlink = 0 if q.tail != 0 { q.tail.ptr().schedlink.set(gp) } else { q.head.set(gp) } q.tail.set(gp) } "},{"id":1,"href":"/docs/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E8%B0%83%E5%BA%A6/%E4%BA%8C-%E8%B0%83%E5%BA%A6%E7%9A%84%E6%97%B6%E6%9C%BA/4.2.2-%E8%A2%AB%E5%8A%A8%E8%B0%83%E5%BA%A6/","title":"4.2.2 goroutine被动调度","section":"二 调度的时机","content":" 什么是被动调度 # 被动调度: goroutine执行某个操作因条件不满足需要等待而发生的调度； goroutine进入睡眠[比如是Goroutine N发送数据到无缓冲chan上面,当没有其他Goroutine从chan上面读数据的时候,Goroutine N阻塞在chan上面. 此刻睡眠含义:进入chan的缓存读取队列(goroutine链表)]; 重新运行schedule() 唤醒睡眠中的goroutine; 唤醒空闲的P和唤醒创建工作线程; goroutine(被创建出来后/创建运行了一段时间后)如何放入运行队列[P中]; 探寻被动调度,如何进入睡眠 # goroutine因某个条件而阻塞 chan waitGroup 等这些都会发生阻塞 定义程序 # 先来看一个例子,\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;sync/atomic\u0026#34; ) func main() { var n int32 var wg sync.WaitGroup runtime.GOMAXPROCS(2) wg.Add(1) go func() { wg.Done() for{ atomic.AddInt32(\u0026amp;n, 1) } }() wg.Wait() fmt.Println(atomic.LoadInt32(\u0026amp;n)) // 1 } gdb调试前准备 # 编译程序 # 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n准备mcall函数断点的文件 # 进入wg.Wait()\n睡眠 # 我们都知道main函数里面的wg.Wait()这个只有当wg的计数器到0后才会继续执行, 我们来看看当wg还没有到0的时候,这里会发生什么呢?\nsrc/sync/waitgroup.go\n// Wait blocks until the WaitGroup counter is zero. func (wg *WaitGroup) Wait() { statep, semap := wg.state() //....省去race代码 for { state := atomic.LoadUint64(statep) v := int32(state \u0026gt;\u0026gt; 32) w := uint32(state) if v == 0 { // Counter is 0, no need to wait. //....省去race代码 return } // Increment waiters count. if atomic.CompareAndSwapUint64(statep, state, state+1) { //....省去race代码 runtime_Semacquire(semap) // PV操作 ---------------------here if *statep != 0 { panic(\u0026#34;sync: WaitGroup is reused before previous Wait has returned\u0026#34;) } if race.Enabled { race.Enable() race.Acquire(unsafe.Pointer(wg)) } return } } } runtime_Semacquire # src/sync/runtime.go\n// Semacquire waits until *s \u0026gt; 0 and then atomically decrements it. // It is intended as a simple sleep primitive for use by the synchronization // library and should not be used directly. func runtime_Semacquire(s *uint32) 这里类似PV操作, 但是: P:等待直到(*s)大于0, 然后才自动减. [话句话如果是等于0, 那么就会休眠]; v:首先自动增加(*s),唤醒被等待的goroutine. src/sync/runtime.go\n// Semacquire waits until *s \u0026gt; 0 and then atomically decrements it. // It is intended as a simple sleep primitive for use by the synchronization // library and should not be used directly. func runtime_Semacquire(s *uint32) 继续下去发现找不到定义了, 我们gdb查找一下:\nlist /usr/lib/golang/src/sync/waitgroup.go:130 b 130 list /usr/lib/golang/src/sync/runtime.go:1 b 130 (gdb) info break Num Type Disp Enb Address What 1 breakpoint keep y 0x0000000000466bfb in sync.(*WaitGroup).Wait at /usr/lib/golang/src/sync/waitgroup.go:130 2 breakpoint keep y 0x00000000004669c0 in sync.init.1 at /usr/lib/golang/src/sync/runtime.go:14 (gdb) (gdb) step sync.runtime_Semacquire (addr=0xc00007e014) at /usr/lib/golang/src/runtime/sema.go:55 55\tfunc sync_runtime_Semacquire(addr *uint32) { (gdb) frame #0 sync.runtime_Semacquire (addr=0xc00007e014) at /usr/lib/golang/src/runtime/sema.go:55 55\tfunc sync_runtime_Semacquire(addr *uint32) { sync_runtime_Semacquire # /src/runtime/sema.go\n//go:linkname sync_runtime_Semacquire sync.runtime_Semacquire func sync_runtime_Semacquire(addr *uint32) { semacquire1(addr, false, semaBlockProfile, 0) } func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int) { gp := getg() if gp != gp.m.curg { throw(\u0026#34;semacquire not on the G stack\u0026#34;) } // Easy case. if cansemacquire(addr) { return } // Harder case: //\tincrement waiter count //\ttry cansemacquire one more time, return if succeeded //\tenqueue itself as a waiter //\tsleep //\t(waiter descriptor is dequeued by signaler) s := acquireSudog() root := semroot(addr) t0 := int64(0) s.releasetime = 0 s.acquiretime = 0 s.ticket = 0 if profile\u0026amp;semaBlockProfile != 0 \u0026amp;\u0026amp; blockprofilerate \u0026gt; 0 { t0 = cputicks() s.releasetime = -1 } if profile\u0026amp;semaMutexProfile != 0 \u0026amp;\u0026amp; mutexprofilerate \u0026gt; 0 { if t0 == 0 { t0 = cputicks() } s.acquiretime = t0 } for { lock(\u0026amp;root.lock) // Add ourselves to nwait to disable \u0026#34;easy case\u0026#34; in semrelease. atomic.Xadd(\u0026amp;root.nwait, 1) // Check cansemacquire to avoid missed wakeup. if cansemacquire(addr) { atomic.Xadd(\u0026amp;root.nwait, -1) unlock(\u0026amp;root.lock) break } // Any semrelease after the cansemacquire knows we\u0026#39;re waiting // (we set nwait above), so go to sleep. root.queue(addr, s, lifo) goparkunlock(\u0026amp;root.lock, waitReasonSemacquire, traceEvGoBlockSync, 4+skipframes) // 进入睡眠 ---------------------here if s.ticket != 0 || cansemacquire(addr) { break } } if s.releasetime \u0026gt; 0 { blockevent(s.releasetime-t0, 3+skipframes) } releaseSudog(s) } 这里的sema变量, 如果是等于0(它是无符号整形,不会为负数),就代表它没有获取到sema,需要等待, 如果sema变量是大于0,可以直接运行,不需要等待\nfunc cansemacquire(addr *uint32) bool { for { v := atomic.Load(addr) if v == 0 { return false } if atomic.Cas(addr, v, v-1) { return true } } } 我们知道sync.WaitGroup()函数, 一般是先Add(), 然后Wait()等待所有任务Done()[Add(-1)] sema开始值是0,按照我们前面说的,他无法获得sema,会进行休眠,同理如果有多个Wait(), 那么它们都会 休眠; 当Add(-1)后到达0后, 就是所有任务都已经完成,它会调用runtime_Semrelease(semap, false, 0) // 进行V操作进行sema值加一, 然后唤醒一个goroutine, 当这个goroutine醒来[在一个loop里面], 发现sema不等于0了, 直接减一,然后跳出了semacquire1\n关于WaitGroup的分析, 参看我其他的文章.继续分析这个休眠和唤醒的;\ngopark # src/runtime/proc.go\n// Puts the current goroutine into a waiting state and unlocks the lock. // The goroutine can be made runnable again by calling goready(gp). func goparkunlock(lock *mutex, reason waitReason, traceEv byte, traceskip int) { gopark(parkunlock_c, unsafe.Pointer(lock), reason, traceEv, traceskip) } // Puts the current goroutine into a waiting state and calls unlockf. // If unlockf returns false, the goroutine is resumed. // unlockf must not access this G\u0026#39;s stack, as it may be moved between // the call to gopark and the call to unlockf. // Reason explains why the goroutine has been parked. // It is displayed in stack traces and heap dumps. // Reasons should be unique and descriptive. // Do not re-use reasons, add new ones. func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int) { if reason != waitReasonSleep { checkTimeouts() // timeouts may expire while two goroutines keep the scheduler busy } mp := acquirem() // 这里的作用是什么? ---------wait gp := mp.curg // 得到当前的Goroutine status := readgstatus(gp) if status != _Grunning \u0026amp;\u0026amp; status != _Gscanrunning { throw(\u0026#34;gopark: bad g status\u0026#34;) } mp.waitlock = lock mp.waitunlockf = unlockf gp.waitreason = reason mp.waittraceev = traceEv mp.waittraceskip = traceskip releasem(mp) // ---------wait // can\u0026#39;t do anything that might move the G between Ms here. mcall(park_m) } mcall就是切换到g0栈上去,gN的cpu寄存器等保存到g\u0026rsquo;sched,然后把gN作为实参给mcall的fn形参, 继续看park_m.\n// park continuation on g0. func park_m(gp *g) { _g_ := getg() // 得到g0 if trace.enabled { traceGoPark(_g_.m.waittraceev, _g_.m.waittraceskip) } casgstatus(gp, _Grunning, _Gwaiting) dropg()//解除gN与m的关系 if fn := _g_.m.waitunlockf; fn != nil { ok := fn(gp, _g_.m.waitlock) //解除lock _g_.m.waitunlockf = nil _g_.m.waitlock = nil if !ok { if trace.enabled { traceGoUnpark(gp, 2) } casgstatus(gp, _Gwaiting, _Grunnable) execute(gp, true) // Schedule it back, never returns. } } schedule() //重新进入调度 } dropg就是:g0---\u0026gt;m--X--\u0026gt;curg--X--\u0026gt;m这里的curg就是前面的gN,解除gN与m的关系.\nfunc dropg() { _g_ := getg() setMNoWB(\u0026amp;_g_.m.curg.m, nil) setGNoWB(\u0026amp;_g_.m.curg, nil) } 进入睡眠的步骤:goparkunlock --\u0026gt; gopark --\u0026gt; mcall(park_m) --\u0026gt; schedule.\n唤醒睡眠中的goroutine # func (wg *WaitGroup) Add(delta int) { statep, semap := wg.state() if race.Enabled { _ = *statep // trigger nil deref early if delta \u0026lt; 0 { // Synchronize decrements with Wait. race.ReleaseMerge(unsafe.Pointer(wg)) } race.Disable() defer race.Enable() } state := atomic.AddUint64(statep, uint64(delta)\u0026lt;\u0026lt;32) v := int32(state \u0026gt;\u0026gt; 32) w := uint32(state) if race.Enabled \u0026amp;\u0026amp; delta \u0026gt; 0 \u0026amp;\u0026amp; v == int32(delta) { // The first increment must be synchronized with Wait. // Need to model this as a read, because there can be // several concurrent wg.counter transitions from 0. race.Read(unsafe.Pointer(semap)) } if v \u0026lt; 0 { panic(\u0026#34;sync: negative WaitGroup counter\u0026#34;) } if w != 0 \u0026amp;\u0026amp; delta \u0026gt; 0 \u0026amp;\u0026amp; v == int32(delta) { panic(\u0026#34;sync: WaitGroup misuse: Add called concurrently with Wait\u0026#34;) } if v \u0026gt; 0 || w == 0 { return } // This goroutine has set counter to 0 when waiters \u0026gt; 0. // Now there can\u0026#39;t be concurrent mutations of state: // - Adds must not happen concurrently with Wait, // - Wait does not increment waiters if it sees counter == 0. // Still do a cheap sanity check to detect WaitGroup misuse. if *statep != state { panic(\u0026#34;sync: WaitGroup misuse: Add called concurrently with Wait\u0026#34;) } // Reset waiters count to 0. *statep = 0 for ; w != 0; w-- { runtime_Semrelease(semap, false, 0) // 进行V操作 --------------------------------here } } runtime_Semrelease # src/sync/runtime.go\n//go:linkname sync_runtime_Semrelease sync.runtime_Semrelease func sync_runtime_Semrelease(addr *uint32, handoff bool, skipframes int) { semrelease1(addr, handoff, skipframes) } func semrelease1(addr *uint32, handoff bool, skipframes int) { root := semroot(addr) atomic.Xadd(addr, 1) // Easy case: no waiters? // This check must happen after the xadd, to avoid a missed wakeup // (see loop in semacquire). if atomic.Load(\u0026amp;root.nwait) == 0 { return } // Harder case: search for a waiter and wake it. lock(\u0026amp;root.lock) if atomic.Load(\u0026amp;root.nwait) == 0 { // The count is already consumed by another goroutine, // so no need to wake up another goroutine. unlock(\u0026amp;root.lock) return } s, t0 := root.dequeue(addr) if s != nil { atomic.Xadd(\u0026amp;root.nwait, -1) } unlock(\u0026amp;root.lock) if s != nil { // May be slow, so unlock first acquiretime := s.acquiretime if acquiretime != 0 { mutexevent(t0-acquiretime, 3+skipframes) } if s.ticket != 0 { throw(\u0026#34;corrupted semaphore ticket\u0026#34;) } if handoff \u0026amp;\u0026amp; cansemacquire(addr) { s.ticket = 1 } readyWithTime(s, 5+skipframes) // ------------------------------here } } readyWithTime # func readyWithTime(s *sudog, traceskip int) { if s.releasetime != 0 { s.releasetime = cputicks() } goready(s.g, traceskip) } func goready(gp *g, traceskip int) { systemstack(func() { ready(gp, traceskip, true) }) } 前面都很好理解, 这里有两个函数, 一个是systemstack, 一个是ready函数\nready # // Mark gp ready to run. func ready(gp *g, traceskip int, next bool) { if trace.enabled { //忽略 traceGoUnpark(gp, traceskip) } status := readgstatus(gp) // return atomic.Load(\u0026amp;gp.atomicstatus)读atomicstatus的状态 // Mark runnable. _g_ := getg() mp := acquirem() // disable preemption because it can be holding p in a local var if status\u0026amp;^_Gscan != _Gwaiting { dumpgstatus(gp) throw(\u0026#34;bad g-\u0026gt;status in ready\u0026#34;) } // status is Gwaiting or Gscanwaiting, make Grunnable and put on runq casgstatus(gp, _Gwaiting, _Grunnable) // 修改状态到_Grunnable runqput(_g_.m.p.ptr(), gp, next) // 放入全局或本地队列,等待调度 if atomic.Load(\u0026amp;sched.npidle) != 0 \u0026amp;\u0026amp; atomic.Load(\u0026amp;sched.nmspinning) == 0 { wakep() // 有空闲的P,并且没有正在自旋状态的M(偷取其他线程的goroutine);那么需要唤醒P.万一就把上面的待运行的goroutine调度起来了呢? } releasem(mp) } sync_runtime_Semrelease ---\u0026gt; semrelease1 ---\u0026gt; readyWithTime ---\u0026gt; goready --systemstack-\u0026gt; ready ready---\u0026gt; runqput ---\u0026gt; releasem \\---\u0026gt; wakep --/\u0026gt; runqput # 其中runqput是,当从s, t0 := root.dequeue(addr)得到睡眠中的Goroutine,修改状态为_Grunnable,然后放入本地或全局队列\n// runqput tries to put g on the local runnable queue. // If next is false, runqput adds g to the tail of the runnable queue. // If next is true, runqput puts g in the _p_.runnext slot. // If the run queue is full, runnext puts g on the global queue. // Executed only by the owner P. func runqput(_p_ *p, gp *g, next bool) { if randomizeScheduler \u0026amp;\u0026amp; next \u0026amp;\u0026amp; fastrand()%2 == 0 { next = false } if next { retryNext: oldnext := _p_.runnext if !_p_.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) { goto retryNext } if oldnext == 0 { return } // Kick the old runnext out to the regular run queue. gp = oldnext.ptr() } retry: h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with consumers t := _p_.runqtail if t-h \u0026lt; uint32(len(_p_.runq)) { // TODO计算这个 _p_.runq[t%uint32(len(_p_.runq))].set(gp) atomic.StoreRel(\u0026amp;_p_.runqtail, t+1) // store-release, makes the item available for consumption return } if runqputslow(_p_, gp, h, t) { return } // the queue is not full, now the put above must succeed goto retry } runqput函数流程很清晰，它首先尝试把gp放入_p_的本地运行队列，如果本地队列满了，则通过runqputslow函数把gp放入全局运行队列。\n// Put g and a batch of work from local runnable queue on global queue. // Executed only by the owner P. func runqputslow(_p_ *p, gp *g, h, t uint32) bool { var batch [len(_p_.runq)/2 + 1]*g // 256/2+1 = 129个goroutine // First, grab a batch from local queue. n := t - h n = n / 2 if n != uint32(len(_p_.runq)/2) { // 得到现有队列中的一半G throw(\u0026#34;runqputslow: queue is not full\u0026#34;) } for i := uint32(0); i \u0026lt; n; i++ { batch[i] = _p_.runq[(h+i)%uint32(len(_p_.runq))].ptr() /* --------- | | | --------- head[1] tail[2], TODO 应该head, tail应该不是地址 */ } if !atomic.CasRel(\u0026amp;_p_.runqhead, h, h+n) { // cas-release, commits consume return false } batch[n] = gp if randomizeScheduler { // 打乱将要插入全局的G for i := uint32(1); i \u0026lt;= n; i++ { j := fastrandn(i + 1) batch[i], batch[j] = batch[j], batch[i] } } // Link the goroutines. for i := uint32(0); i \u0026lt; n; i++ { batch[i].schedlink.set(batch[i+1]) // TODO 通过schedlink来进行连接? 全局运行队列是一个链表,把将要放入全局的G,链接起来. } var q gQueue q.head.set(batch[0]) q.tail.set(batch[n]) // Now put the batch on global queue. lock(\u0026amp;sched.lock) globrunqputbatch(\u0026amp;q, int32(n+1)) unlock(\u0026amp;sched.lock) return true } https://www.bing.com/search?q=WaitGroup+source+code\u0026PC=U316\u0026FORM=CHROMN\n唤醒一个空闲P,获得(创建)一个M,开始轮询调度,可能把上面已唤醒的G调度起来 # 我们继续看,wakep() // 有空闲的P,并且没有正在自旋状态的M(偷取其他线程的goroutine)\nwakep\u0026ndash;\u0026gt;startm # 当唤醒一个G,放到本地或全局了,这时如果发现有空闲的P,而且没有正在偷其他的P队列中goroutine的M,那么此时就需要唤醒一个空闲P,来工作, 一个P只能找到M,从空闲的M中找?, 然后通过m.g0找到. 生成新的M,此时,创造新的线程,因为M与线程是一一对应的,同时它也会创造一个g0. 然后开始schedule()工作. // Tries to add one more P to execute G\u0026#39;s. // Called when a G is made runnable (newproc, ready). func wakep() { // be conservative about spinning threads if !atomic.Cas(\u0026amp;sched.nmspinning, 0, 1) { return } startm(nil, true) } 这里又增加了全局的自旋线程的个数,因为我们在前面已经判断了,wakep() // 有空闲的P,并且没有正在自旋状态的M(偷取其他线程的goroutine),先再次判断下,是否还是没有自旋的线程在运行,如果没有就加一,后面准备创建一个M,startm.\n继续看下startm 如果p==nil，尝试获取一个空闲的P，如果没有空闲的P，则什么也不做。 安排一些M来运行p（必要时创建一个M）。 可以在m.p==nil的情况下运行，所以不允许写障碍。 如果设置了spinning，则调用者已经增量了nmspinning，startm将会在新启动的M中减去nmspinning或设置m.spinning。 上面我们调用的startm(nil,true) func startm(_p_ *p, spinning bool) { lock(\u0026amp;sched.lock) if _p_ == nil { _p_ = pidleget() if _p_ == nil { unlock(\u0026amp;sched.lock) if spinning { // The caller incremented nmspinning, but there are no idle Ps, // so it\u0026#39;s okay to just undo the increment and give up. if int32(atomic.Xadd(\u0026amp;sched.nmspinning, -1)) \u0026lt; 0 { throw(\u0026#34;startm: negative nmspinning\u0026#34;) } } return } } mp := mget() unlock(\u0026amp;sched.lock) // 全局空闲M队列 if mp == nil { //从全局的空闲队列没找到M var fn func() if spinning { // The caller incremented nmspinning, so set m.spinning in the new M. fn = mspinning } newm(fn, _p_) //新创建一个M,其中里面也有新g0. return // 这里如果是新创建的线程,那么直接返回,不需要继续下面的唤醒notewakeup(\u0026amp;mp.park) } if mp.spinning { throw(\u0026#34;startm: m is spinning\u0026#34;) } if mp.nextp != 0 { throw(\u0026#34;startm: m has p\u0026#34;) } if spinning \u0026amp;\u0026amp; !runqempty(_p_) { throw(\u0026#34;startm: p has runnable gs\u0026#34;) } // The caller incremented nmspinning, so set m.spinning in the new M. mp.spinning = spinning mp.nextp.set(_p_) notewakeup(\u0026amp;mp.park) // 到这一步,那么M是从全局空闲M队列获得的Ms,那么需要唤醒. } 关注三点: mget()\u0026mdash;得到全局一个M newm(fn, p),创建一个M notewakeup(\u0026amp;mp.park) mget # 这个全局的空闲M队列,从下面的函数取出,它就是一个链表,通过schedlink指针来串联所有空闲的M.\n/* //尝试从全局的(sched.midle列表中得到一个m) // Try to get an m from midle list. // Sched must be locked. // May run during STW, so write barriers are not allowed. //go:nowritebarrierrec func mget() *m { mp := sched.midle.ptr() if mp != nil { sched.midle = mp.schedlink sched.nmidle-- } return mp } */ type schedt struct { //... midle muintptr // idle m\u0026#39;s waiting for work nmidle int32 // number of idle m\u0026#39;s waiting for work //... } // muintptr类型 type muintptr uintptr //go:nosplit func (mp muintptr) ptr() *m { return (*m)(unsafe.Pointer(mp)) } //go:nosplit func (mp *muintptr) set(m *m) { *mp = muintptr(unsafe.Pointer(m)) } newm创建新的线程与M # newm(fn, _p_) //新创建一个M,其中里面也有新g0.\nfunc newm(fn func(), _p_ *p) { mp := allocm(_p_, fn) //分配一个与任何线程无关的M结构体mp,mp.g0=Gorooutine(也是从这里面分配出来的) mp.nextp.set(_p_) //把M关联上P mp.sigmask = initSigmask //初始化信号 if gp := getg(); gp != nil \u0026amp;\u0026amp; gp.m != nil \u0026amp;\u0026amp; (gp.m.lockedExt != 0 || gp.m.incgo) \u0026amp;\u0026amp; GOOS != \u0026#34;plan9\u0026#34; { // We\u0026#39;re on a locked M or a thread that may have been // started by C. The kernel state of this thread may // be strange (the user may have locked it for that // purpose). We don\u0026#39;t want to clone that into another // thread. Instead, ask a known-good thread to create // the thread for us. // // This is disabled on Plan 9. See golang.org/issue/22227. // // TODO: This may be unnecessary on Windows, which // doesn\u0026#39;t model thread creation off fork. lock(\u0026amp;newmHandoff.lock) if newmHandoff.haveTemplateThread == 0 { throw(\u0026#34;on a locked thread with no template thread\u0026#34;) } mp.schedlink = newmHandoff.newm newmHandoff.newm.set(mp) if newmHandoff.waiting { newmHandoff.waiting = false notewakeup(\u0026amp;newmHandoff.wake) } unlock(\u0026amp;newmHandoff.lock) return } newm1(mp) } newm函数主要是两件事: 创建M结构体,G结构体(用于前面m.g0)在stack上开辟空间给g0.stack. allocm() 创建一个真正的线程,与上面的M结构相关联. newm1()方式, 直接从当前线程clone出一个新的线程. newmHandoff方式 [TODO zxc:可以新开一章来讲解] 给一个一直在for循环的创建线程,然后休眠的函数来创建线程,关联上面的M结构体 allocm函数 # 因为函数需要new小对象,需要用到P.mcache,需要判断当前的M是否关联着P,如果没有就要向参数借,但是为什么没有判断如果参数_P_是nil的情况?\n释放不需要等待的全局的M\u0026rsquo;s stack资源,这里是防止新生成线程又需要stack空间,所以先释放一些?\nnew M, g0结构体,并且给g0分配stack空间.\n遗留\n为什么需要locks\u0026ndash;? // Allocate a new m unassociated with any thread. // Can use p for allocation context if needed. // fn is recorded as the new m\u0026#39;s m.mstartfn. // // This function is allowed to have write barriers even if the caller // isn\u0026#39;t because it borrows _p_. // //go:yeswritebarrierrec /* */ func allocm(_p_ *p, fn func()) *m { _g_ := getg() acquirem() // disable GC because it can be called from sysmon /* func acquirem() *m { _g_ := getg() _g_.m.locks++ return _g_.m } TODO zxc: 这里很奇怪的,只是把M结构体里面的locks字段++1; 后面的releasem又把这个locks--? func releasem(mp *m) { _g_ := getg() mp.locks-- if mp.locks == 0 \u0026amp;\u0026amp; _g_.preempt { // restore the preemption request in case we\u0026#39;ve cleared it in newstack _g_.stackguard0 = stackPreempt } } */ if _g_.m.p == 0 { //如果当前m的P不存在,acquirep(_p_)关联_p_和当前的M;只是用于小对象的分配到堆上? TODO zxc: 这里P.mcahce,就是M.mcache acquirep(_p_) // temporarily borrow p for mallocs in this function } // Release the free M list. We need to do this somewhere and // this may free up a stack we can use. if sched.freem != nil { lock(\u0026amp;sched.lock) var newList *m for freem := sched.freem; freem != nil; { if freem.freeWait != 0 { // if == 0, 安全释放g0并删除m; 这里如果不等于0,就说明需要等待,还不能立即释放,所以这个if里面就好像链表反转算法,转移到新链表. next := freem.freelink freem.freelink = newList //freem.freelink = newList(nil); so, freem.freelink == nil newList = freem // newList.freelink == nil; freem == newList freem = next // continue } stackfree(freem.g0.stack) // 说明freem.freeWait==0;可以立即g0 stack释放 freem = freem.freelink // freem等于它的next指针 } sched.freem = newList //这里就把不能释放的重新放入全局释放列表 unlock(\u0026amp;sched.lock) } mp := new(m) // new一个M结构体 mp.mstartfn = fn mcommoninit(mp) // In case of cgo or Solaris or illumos or Darwin, pthread_create will make us a stack. // Windows and Plan 9 will layout sched stack on OS stack. if iscgo || GOOS == \u0026#34;solaris\u0026#34; || GOOS == \u0026#34;illumos\u0026#34; || GOOS == \u0026#34;windows\u0026#34; || GOOS == \u0026#34;plan9\u0026#34; || GOOS == \u0026#34;darwin\u0026#34; { mp.g0 = malg(-1) } else { mp.g0 = malg(8192 * sys.StackGuardMultiplier) //约等于1024*8=8192 ==\u0026gt; 8k } mp.g0.m = mp if _p_ == _g_.m.p.ptr() { //如果是借P来进行malloc,那么需要恢复原样. releasep() } releasem(_g_.m) // TODO zxc: locks ? return mp } newm1 # func newm1(mp *m) { if iscgo { // 暂时忽略. //... } execLock.rlock() // Prevent process clone. newosproc(mp) //准备去clone. execLock.runlock() } func newosproc(mp *m) { stk := unsafe.Pointer(mp.g0.stack.hi) /* * note: strace gets confused if we use CLONE_PTRACE here. */ if false { print(\u0026#34;newosproc stk=\u0026#34;, stk, \u0026#34; m=\u0026#34;, mp, \u0026#34; g=\u0026#34;, mp.g0, \u0026#34; clone=\u0026#34;, funcPC(clone), \u0026#34; id=\u0026#34;, mp.id, \u0026#34; ostk=\u0026#34;, \u0026amp;mp, \u0026#34;\\n\u0026#34;) } // Disable signals during clone, so that the new thread starts // with signals disabled. It will enable them in minit. // 在clone,关闭信号,所以创建出来的thread信号都是关闭的,minit函数再打开. var oset sigset sigprocmask(_SIG_SETMASK, \u0026amp;sigset_all, \u0026amp;oset) ret := clone(cloneFlags, stk, unsafe.Pointer(mp), unsafe.Pointer(mp.g0), unsafe.Pointer(funcPC(mstart))) sigprocmask(_SIG_SETMASK, \u0026amp;oset, nil) if ret \u0026lt; 0 { print(\u0026#34;runtime: failed to create new OS thread (have \u0026#34;, mcount(), \u0026#34; already; errno=\u0026#34;, -ret, \u0026#34;)\\n\u0026#34;) if ret == -_EAGAIN { println(\u0026#34;runtime: may need to increase max user processes (ulimit -u)\u0026#34;) } throw(\u0026#34;newosproc\u0026#34;) } } 上面还是比较清晰,先关闭信号,防止clone时候被打断.\nclone # 这个clone函数位于src/runtime/sys_linux_amd64.s\n// int32 clone(int32 flags, void *stk, M *mp, G *gp, void (*fn)(void)); TEXT runtime·clone(SB),NOSPLIT,$0 // Linux系统调用约定，这四个参数需要分别放入rdi， rsi，rdx和r10寄存器中 MOVL\tflags+0(FP), DI MOVQ\tstk+8(FP), SI MOVQ\t$0, DX MOVQ\t$0, R10 // Copy mp, gp, fn off parent stack for use by child. // Careful: Linux system call clobbers CX and R11. // Linux系统调用会污染CX和R11; 所以我们参数不放在那里面 MOVQ\tmp+16(FP), R8 MOVQ\tgp+24(FP), R9 MOVQ\tfn+32(FP), R12 MOVL\t$SYS_clone, AX // 系统调用了; 返回值放入AX寄存器里面. SYSCALL // In parent, return. CMPQ\tAX, $0 // 如果返回值是0,证明是子进程返回 JEQ\t3(PC) MOVL\tAX, ret+40(FP) // 父进程返回,返回到父进程的AX不等于0 RET // In child, on new stack. MOVQ\tSI, SP // sp = stk+8(FP); 就是设置子线程的栈顶; // If g or m are nil, skip Go-related setup. CMPQ\tR8, $0 // m JEQ\tnog CMPQ\tR9, $0 // g JEQ\tnog // Initialize m-\u0026gt;procid to Linux tid MOVL\t$SYS_gettid, AX SYSCALL MOVQ\tAX, m_procid(R8) // 设置m.proc_id = sys_gettid() // Set FS to point at m-\u0026gt;tls. LEAQ\tm_tls(R8), DI // 不取引用,所以是DI=\u0026amp;m.tls[0];把m.tls[0]地址给DI寄存器. CALL\truntime·settls(SB) // FS寄存器里的值:就是m.tls[0]的地址. // In child, set up new stack get_tls(CX) // CX = \u0026amp;m.tls[0] MOVQ\tR8, g_m(R9) MOVQ\tR9, g(CX) // gp+24(FP) == R9; g(CX)---\u0026gt; *CX; m.tls[0]=g0; CALL\truntime·stackcheck(SB) nog: // Call fn CALL\tR12 // It shouldn\u0026#39;t return. If it does, exit that thread. MOVL\t$111, DI MOVL\t$SYS_exit, AX SYSCALL JMP\t-3(PC)\t// keep exiting gdb\nlist /tmp/kubernets/clone_test/main.go:1 list /usr/lib/golang/src/runtime/sys_linux_amd64.s:540 gdb调试自定义函数\ndefine zxc info threads info register rbp rsp pc end [New LWP 32548] [Switching to LWP 32548] Breakpoint 3, runtime.clone () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:562 562\tMOVQ\tSI, SP ---------------------------------------------------------here 这一行的确是没必要,clone完后,系统会把子线程的sp寄存器设置为传入参数stk; m.stack.hi. (gdb) zxc Id Target Id Frame * 2 LWP 32548 \u0026#34;test\u0026#34; runtime.clone () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:562 1 LWP 32547 \u0026#34;test\u0026#34; runtime.clone () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:556 rbp 0x7fffffffe3b0\t0x7fffffffe3b0 rsp 0xc000044000\t0xc000044000 pc 0x455568\t0x455568 \u0026lt;runtime.clone+56\u0026gt; (gdb) info register sp sp 0xc000044000\t0xc000044000 (gdb) step 565\tCMPQ\tR8, $0 // m (gdb) info register sp sp 0xc000044000\t0xc000044000 notewakeup(\u0026amp;mp.park) # func notewakeup(n *note) { var v uintptr for { v = atomic.Loaduintptr(\u0026amp;n.key) if atomic.Casuintptr(\u0026amp;n.key, v, locked) { break } } // Successfully set waitm to locked. // What was it before? switch { case v == 0: // 我们从上面知道 v == note.key; note.key == M结构体的指针; 如果是0,那么这个note是无效的,不需要唤醒. // Nothing was waiting. Done. case v == locked: // Two notewakeups! Not allowed. throw(\u0026#34;notewakeup - double wakeup\u0026#34;) // 不能调用这个函数两次, locked的值是1,唤醒一次,这个note.key就变成1了,下次再调用就报错. default: // Must be the waiting m. Wake it up. semawakeup((*m)(unsafe.Pointer(v))) //从这个semawakeup(mp *m)需要的参数,可以知道note.key字段是一个M的指针. } } 附录 # clone系统调用\n所以这里必须为子线程指定其使用的栈，否则父子线程会共享同一个栈从而造成混乱，从上面的newosproc函数可以看出，新线程使用的栈为m.g0.stack.lo～m.g0.stack.hi这段内存，而这段内存是newm函数在创建m结构体对象时从进程的堆上分配而来的。\n//\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\nalter table ai_course_mission_section_relation add type smallint default 0 not null;\ncomment on column ai_course_mission_section_relation.type is \u0026lsquo;任务组的类型;0:默认是任务组 1:主题组\u0026rsquo;;\ncomment on column ai_mission_section.name is \u0026lsquo;任务分组/主题组名称\u0026rsquo;;\nalter table ai_mission_section add project_resource_id bigint;\ncomment on column ai_mission_section.project_resource_id is \u0026lsquo;主题需要工程的resourceID\u0026rsquo;;\nalter table ai_mission_section add pic_resource_id bigint;\ncomment on column ai_mission_section.pic_resource_id is \u0026lsquo;主题图片resourceID\u0026rsquo;;\ncreate table ai_mission_theme_relation ( mission_id bigint not null constraint \u0026ldquo;fk_mission_theme_relation_mission_id\u0026rdquo; references ai_mission on delete cascade, ai_mission_section_id bigint not null constraint fk_mission_theme_relation_mission_section_id references ai_mission_section, update_time bigint not null, serial_number smallint default 1 not null, id bigserial not null constraint mission_theme_relation_pkey primary key );\ncomment on column ai_mission_theme_relation.mission_id is \u0026lsquo;任务ID\u0026rsquo;;\ncomment on column ai_mission_theme_relation.ai_mission_section_id is \u0026lsquo;主题(这节课自带的主题ID才可以关联)ID\u0026rsquo;;\ncomment on column ai_mission_theme_relation.update_time is \u0026lsquo;修改时间\u0026rsquo;;\ncomment on column ai_mission_theme_relation.serial_number is \u0026lsquo;主题顺序\u0026rsquo;;\nalter table ai_mission_theme_relation owner to postgres;\ncreate index idx_mission_theme_relation_mission_id on ai_mission_theme_relation (mission_id);\nTODO 删除的时候可以连着删除 1000000个数,白天只查,晚上更新\n一个无序的数组长度为1001 ，数组内的元素值在[1,1000]间，只用基本数据结构找出1001中唯一重复的元素 \u0026mdash; （不能用map）\n将IPv4（192.168.1.1）转换成longl诶行\n一个程序 接收1亿个数 最后返回前100个最大的数\n"},{"id":2,"href":"/docs/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E8%B0%83%E5%BA%A6/%E4%BA%8C-%E8%B0%83%E5%BA%A6%E7%9A%84%E6%97%B6%E6%9C%BA/4.2.3-%E5%89%A5%E5%A4%BA%E8%B0%83%E5%BA%A6/4.2.3.1-%E8%BF%90%E8%A1%8C%E7%94%A8%E6%88%B7%E4%BB%A3%E7%A0%81%E6%97%B6%E9%97%B4%E8%BF%87%E9%95%BF%E8%B0%83%E5%BA%A6/","title":"4.2.3.1 运行用户代码时间过长调度","section":"4.2.3 剥夺调度","content":" 运行时间过长被调度的情况 # 运行用户代码时间过长; 因为系统调用,导致时间过长. 系统监控 # 我们能想到有系统监控才能查看是否代码是否过长.\n开启系统监控 # sysmon永远for循环src/runtime/proc.go func main() { //... if GOARCH != \u0026#34;wasm\u0026#34; { // no threads on wasm yet, so no sysmon systemstack(func() { newm(sysmon, nil) }) } //... } 上一章我们也分析了这个newm函数;所以在进入schedule()函数之前,会先执行这个sysmon函数.\nsysmon # 这里我们只看抢占相关的,当retake返回非0,那么代表所有P不是空闲的状态,所以idle=0==\u0026gt;usleep(delay)只是休眠最少的时间,只有20us\n// Always runs without a P, so write barriers are not allowed. // //go:nowritebarrierrec func sysmon() { lock(\u0026amp;sched.lock) sched.nmsys++ //增加记录系统线程的值的个数 checkdead() unlock(\u0026amp;sched.lock) lasttrace := int64(0) idle := 0 // how many cycles in succession we had not wokeup somebody delay := uint32(0) for { if idle == 0 { // start with 20us sleep... delay = 20 } else if idle \u0026gt; 50 { // start doubling the sleep after 1ms... delay *= 2 } if delay \u0026gt; 10*1000 { // up to 10ms delay = 10 * 1000 } usleep(delay) //... // retake P\u0026#39;s blocked in syscalls // and preempt long running G\u0026#39;s // 抢占被系统调用阻塞的P和抢占长期运行的G if retake(now) != 0 { idle = 0 } else { idle++ } // check if we need to force a GC //... } } retake # 只有P是_Prunning or _Psyscall,才会进行抢占 _Prunning 连续运行超过10毫秒了，设置抢占请求. _Psyscall: 当程序没有工作需要做,且系统调用没有超过10ms就不进行系统调用抢占. 1和2说明这个程序没有工作需要做; 3说明系统调用还没超过10m func retake(now int64) uint32 { n := 0 // Prevent allp slice changes. This lock will be completely // uncontended unless we\u0026#39;re already stopping the world. lock(\u0026amp;allpLock) // We can\u0026#39;t use a range loop over allp because we may // temporarily drop the allpLock. Hence, we need to re-fetch // allp each time around the loop. for i := 0; i \u0026lt; len(allp); i++ { //遍历所有的P _p_ := allp[i] if _p_ == nil { // This can happen if procresize has grown // allp but not yet created new Ps. continue } pd := \u0026amp;_p_.sysmontick // 最后一次被sysmon观察到的tick s := _p_.status sysretake := false if s == _Prunning || s == _Psyscall { //只有当p处于 _Prunning 或 _Psyscall 状态时才会进行抢占 // Preempt G if it\u0026#39;s running for too long. t := int64(_p_.schedtick) // _p_.schedtick：每发生一次调度，调度器对该值加一 if int64(pd.schedtick) != t { // 监控线程监控到一次新的调度，所以重置跟sysmon相关的schedtick和schedwhen变量 pd.schedtick = uint32(t) pd.schedwhen = now } else if pd.schedwhen+forcePreemptNS \u0026lt;= now { // 1. 没有进第一个if语句内,说明:pd.schedtick == t; 说明(pd.schedwhen ～ now)这段时间未发生过调度; preemptone(_p_) // 2. 但是这个_P_上面的某个Goroutine被执行,一直在执行这个Goroutiine; 中间没有切换其他Goroutine,因为如果切会导致_P_.schedtick增长,导致进入第一个if语句内; // In case of syscall, preemptone() doesn\u0026#39;t // 3. 连续运行超过10毫秒了，设置抢占请求. // work, because there is no M wired to P. sysretake = true // 需要系统抢占 } } if s == _Psyscall { // P处于系统调用之中，需要检查是否需要抢占 // Retake P from syscall if it\u0026#39;s there for more than 1 sysmon tick (at least 20us). t := int64(_p_.syscalltick) // 用于记录系统调用的次数，主要由工作线程在完成系统调用之后加一 if !sysretake \u0026amp;\u0026amp; int64(pd.syscalltick) != t { // 不相等---说明已经不是上次观察到的系统调用,开始了一个新的系统调用,所以重置一下 pd.syscalltick = uint32(t) pd.syscallwhen = now continue } // On the one hand we don\u0026#39;t want to retake Ps if there is no other work to do, // but on the other hand we want to retake them eventually // because they can prevent the sysmon thread from deep sleep. // 1. _p_的本地运行队列没有Gs; runqempty(_p_)返回true // 2. 有空闲的P,或者有正在自旋状态的M(正在偷其他P队列的Gs); atomic.Load(\u0026amp;sched.nmspinning)+atomic.Load(\u0026amp;sched.npidle) \u0026gt; 0返回true // 3. 上次观测到的系统调用还没有超过10毫秒; pd.syscallwhen+10*1000*1000 \u0026gt; now返回true // - concluing: 当程序没有工作需要做,且系统调用没有超过10ms就不进行系统调用抢占. // - 1和2说明这个程序没有工作需要做; // - 3说明系统调用还没超过10ms if runqempty(_p_) \u0026amp;\u0026amp; atomic.Load(\u0026amp;sched.nmspinning)+atomic.Load(\u0026amp;sched.npidle) \u0026gt; 0 \u0026amp;\u0026amp; pd.syscallwhen+10*1000*1000 \u0026gt; now { continue } // Drop allpLock so we can take sched.lock. unlock(\u0026amp;allpLock) // Need to decrement number of idle locked M\u0026#39;s // (pretending that one more is running) before the CAS. // Otherwise the M from which we retake can exit the syscall, // increment nmidle and report deadlock. incidlelocked(-1) if atomic.Cas(\u0026amp;_p_.status, s, _Pidle) { // 需要抢占，则通过使用cas修改p的状态来获取p的使用权 if trace.enabled { // CAS: 工作线程此时此刻可能正好从系统调用返回了，也正在获取p的使用权 traceGoSysBlock(_p_) traceProcStop(_p_) } n++ _p_.syscalltick++ handoffp(_p_) // 寻找一个新的m出来接管P } incidlelocked(1) lock(\u0026amp;allpLock) } } unlock(\u0026amp;allpLock) return uint32(n) } 执行用户代码抢占 # preemptone设置了被抢占goroutine对应的g结构体中的 preempt成员为true和stackguard0成员为stackPreempt.\n// Tell the goroutine running on processor P to stop. // This function is purely best-effort. It can incorrectly fail to inform the // goroutine. It can send inform the wrong goroutine. Even if it informs the // correct goroutine, that goroutine might ignore the request if it is // simultaneously executing newstack. // No lock needs to be held. // Returns true if preemption request was issued. // The actual preemption will happen at some point in the future // and will be indicated by the gp-\u0026gt;status no longer being // Grunning func preemptone(_p_ *p) bool { mp := _p_.m.ptr() if mp == nil || mp == getg().m { return false } gp := mp.curg // gp == 被抢占的goroutine if gp == nil || gp == mp.g0 { return false } gp.preempt = true // 设置抢占信号preempt == true // Every call in a go routine checks for stack overflow by // comparing the current stack pointer to gp-\u0026gt;stackguard0. // Setting gp-\u0026gt;stackguard0 to StackPreempt folds // preemption into the normal stack overflow check. // (1\u0026lt;\u0026lt;(8*sys.PtrSize) - 1) \u0026amp; -1314 ---\u0026gt; 0xfffffffffffffade, 很大的数 gp.stackguard0 = stackPreempt //stackguard0==很大的数; 使被抢占的goroutine;在进行函数调用会去检查栈溢出;去处理抢占请求 return true } 如何读取标志,然后进行抢占 # 通过stackguard0以及preempt可以找到这个链路:morestack_noctxt()-\u0026gt;morestack()-\u0026gt;newstack().\n// TODO zxc: reference part0:汇编基础.md/go编译器加的函数头的部分.\nruntime·morestack # // morestack but not preserving ctxt. TEXT runtime·morestack_noctxt(SB),NOSPLIT,$0 MOVL\t$0, DX JMP\truntime·morestack(SB) /* * support for morestack */ // Called during function prolog when more stack is needed. // // The traceback routines see morestack on a g0 as being // the top of a stack (for example, morestack calling newstack // calling the scheduler calling newm calling gc), so we must // record an argument size. For that purpose, it has no arguments. TEXT runtime·morestack(SB),NOSPLIT,$0-0\t//开始是进行一些判断 // Cannot grow scheduler stack (m-\u0026gt;g0). //... // Cannot grow signal stack (m-\u0026gt;gsignal). //... //设置m-\u0026gt;morebuf的PC，SP，g为相对应的\u0026#39;main\u0026#39; // Called from f. // Set m-\u0026gt;morebuf to f\u0026#39;s caller. NOP\tSP\t// tell vet SP changed - stop checking offsets MOVQ\t8(SP), AX\t// f\u0026#39;s caller\u0026#39;s PC // 这里的路径比如我的： \u0026#39;main\u0026#39;---\u0026gt;\u0026#39;sub_function\u0026#39;。 // 但是抢占了，所以走下面的路径:-\u0026gt;morestack_noctxt()-\u0026gt;morestack()-\u0026gt;newstack() // 所以这里的f在我这里应该是main. // 需要注意morestack_noctxt与morestack使用的栈大小都是0，且他们的跳转没用call指令，使用的是JMP MOVQ\tAX, (m_morebuf+gobuf_pc)(BX) LEAQ\t16(SP), AX\t// f\u0026#39;s caller\u0026#39;s SP MOVQ\tAX, (m_morebuf+gobuf_sp)(BX) get_tls(CX) //... MOVQ\tg(CX), SI MOVQ\tSI, (m_morebuf+gobuf_g)(BX) //保存当前的寄存器信息到g-\u0026gt;sched中 // Set g-\u0026gt;sched to context in f. MOVQ\t0(SP), AX // f\u0026#39;s PC MOVQ\tAX, (g_sched+gobuf_pc)(SI) MOVQ\tSI, (g_sched+gobuf_g)(SI) LEAQ\t8(SP), AX // f\u0026#39;s SP MOVQ\tAX, (g_sched+gobuf_sp)(SI) // 在morestack里面就已经保存了sp的值。 MOVQ\tBP, (g_sched+gobuf_bp)(SI) MOVQ\tDX, (g_sched+gobuf_ctxt)(SI) //把g0设置为m当前运行的G; 把g0-\u0026gt;sched-\u0026gt;sp恢复到SP寄存器中; // Call newstack on m-\u0026gt;g0\u0026#39;s stack. MOVQ\tm_g0(BX), BX MOVQ\tBX, g(CX) MOVQ\t(g_sched+gobuf_sp)(BX), SP // 把g0的栈SP寄存器恢复到实际的寄存器中。所以下面就使用了g0的栈。 //调用newstack CALL\truntime·newstack(SB) CALL\truntime·abort(SB)\t// crash if newstack returns RET 总结就是： 如果它下一次被调度起来了，那么执行PC，又会重新到本函数头部执行，从上面分析也可以知道，这里的风险就是，如果执行过程没有调用其他函数，那么无法进行抢占，这个就是基于插入抢占，1.14基于信号抢占。 morestack类似于mcall 保存调用morestack函数的goroutine到它的sched成员。 将当前工作线程的g0与线程TLS关联； 将当前工作线程的g0栈恢复到CPU寄存器。 在g0栈中执行传入的参数。\u0026mdash;\u0026gt;这里是runtime·newstack(SB)函数。 newstack(SB) # func newstack() { thisg := getg() //到这里我们又是在g0栈里面。 //... gp := thisg.m.curg //这个就是原来的Goroutine. //... // NOTE: stackguard0 may change underfoot, if another thread // is about to try to preempt gp. Read it just once and use that same // value now and below. preempt := atomic.Loaduintptr(\u0026amp;gp.stackguard0) == stackPreempt //这里判断是否是抢占 打了stackguard0; // Be conservative about where we preempt. // We are interested in preempting user Go code, not runtime code. // If we\u0026#39;re holding locks, mallocing, or preemption is disabled, don\u0026#39;t // preempt. // This check is very early in newstack so that even the status change // from Grunning to Gwaiting and back doesn\u0026#39;t happen in this case. // That status change by itself can be viewed as a small preemption, // because the GC might change Gwaiting to Gscanwaiting, and then // this goroutine has to wait for the GC to finish before continuing. // If the GC is in some way dependent on this goroutine (for example, // it needs a lock held by the goroutine), that small preemption turns // into a real deadlock. if preempt { // 这里还检查了一系列的状态，如果满足就不抢占它了， 让它继续执行。 if thisg.m.locks != 0 || thisg.m.mallocing != 0 || thisg.m.preemptoff != \u0026#34;\u0026#34; || thisg.m.p.ptr().status != _Prunning { // Let the goroutine keep running for now. // gp-\u0026gt;preempt is set, so it will be preempted next time. gp.stackguard0 = gp.stack.lo + _StackGuard //还原stackguard0为正常值，表示我们已经处理过抢占请求了 gogo(\u0026amp;gp.sched) // never return } } if gp.stack.lo == 0 { throw(\u0026#34;missing stack in newstack\u0026#34;) } sp := gp.sched.sp if sys.ArchFamily == sys.AMD64 || sys.ArchFamily == sys.I386 || sys.ArchFamily == sys.WASM { // The call to morestack cost a word. sp -= sys.PtrSize } if stackDebug \u0026gt;= 1 || sp \u0026lt; gp.stack.lo { print(\u0026#34;runtime: newstack sp=\u0026#34;, hex(sp), \u0026#34; stack=[\u0026#34;, hex(gp.stack.lo), \u0026#34;, \u0026#34;, hex(gp.stack.hi), \u0026#34;]\\n\u0026#34;, \u0026#34;\\tmorebuf={pc:\u0026#34;, hex(morebuf.pc), \u0026#34; sp:\u0026#34;, hex(morebuf.sp), \u0026#34; lr:\u0026#34;, hex(morebuf.lr), \u0026#34;}\\n\u0026#34;, \u0026#34;\\tsched={pc:\u0026#34;, hex(gp.sched.pc), \u0026#34; sp:\u0026#34;, hex(gp.sched.sp), \u0026#34; lr:\u0026#34;, hex(gp.sched.lr), \u0026#34; ctxt:\u0026#34;, gp.sched.ctxt, \u0026#34;}\\n\u0026#34;) } if sp \u0026lt; gp.stack.lo { print(\u0026#34;runtime: gp=\u0026#34;, gp, \u0026#34;, goid=\u0026#34;, gp.goid, \u0026#34;, gp-\u0026gt;status=\u0026#34;, hex(readgstatus(gp)), \u0026#34;\\n \u0026#34;) print(\u0026#34;runtime: split stack overflow: \u0026#34;, hex(sp), \u0026#34; \u0026lt; \u0026#34;, hex(gp.stack.lo), \u0026#34;\\n\u0026#34;) throw(\u0026#34;runtime: split stack overflow\u0026#34;) } if preempt { if gp == thisg.m.g0 { throw(\u0026#34;runtime: preempt g0\u0026#34;) } if thisg.m.p == 0 \u0026amp;\u0026amp; thisg.m.locks == 0 { throw(\u0026#34;runtime: g is running but p is not\u0026#34;) } // Synchronize with scang. casgstatus(gp, _Grunning, _Gwaiting) // 设置gp状态变为等待状态。处理gc时把gp的状态修改成_Gwaiting if gp.preemptscan { //gc相关，暂时忽略。 for !castogscanstatus(gp, _Gwaiting, _Gscanwaiting) { // Likely to be racing with the GC as // it sees a _Gwaiting and does the // stack scan. If so, gcworkdone will // be set and gcphasework will simply // return. } if !gp.gcscandone { // gcw is safe because we\u0026#39;re on the // system stack. gcw := \u0026amp;gp.m.p.ptr().gcw scanstack(gp, gcw) gp.gcscandone = true } gp.preemptscan = false gp.preempt = false casfrom_Gscanstatus(gp, _Gscanwaiting, _Gwaiting) // This clears gcscanvalid. casgstatus(gp, _Gwaiting, _Grunning) gp.stackguard0 = gp.stack.lo + _StackGuard gogo(\u0026amp;gp.sched) // never return } // Act like goroutine called runtime.Gosched. casgstatus(gp, _Gwaiting, _Grunning) //恢复状态。 gopreempt_m(gp) // 放入全局队列，重新schedule(); never return === gopreempt_m(gp)---call---\u0026gt;goschedImpl(gp)----call--\u0026gt;globrunqput()放入全局队列/schedule() } //... } 这里就继续上面的，把替换掉的Goroutine重新放入全局队列：\ngopreempt_m(gp)---call---\u0026gt;goschedImpl(gp)----call--\u0026gt;globrunqput()放入全局队列/schedule()\n栈增长相关代码 # func newstack() { //...省略抢占的代码 // Allocate a bigger segment and move the stack. oldsize := gp.stack.hi - gp.stack.lo newsize := oldsize * 2 // 新的栈大小直接*2 if newsize \u0026gt; maxstacksize { print(\u0026#34;runtime: goroutine stack exceeds \u0026#34;, maxstacksize, \u0026#34;-byte limit\\n\u0026#34;) throw(\u0026#34;stack overflow\u0026#34;) } // The goroutine must be executing in order to call newstack, // so it must be Grunning (or Gscanrunning). casgstatus(gp, _Grunning, _Gcopystack) // The concurrent GC will not scan the stack while we are doing the copy since // the gp is in a Gcopystack status. copystack(gp, newsize, true) if stackDebug \u0026gt;= 1 { print(\u0026#34;stack grow done\\n\u0026#34;) } casgstatus(gp, _Gcopystack, _Grunning) gogo(\u0026amp;gp.sched) } func copystack(gp *g, newsize uintptr, sync bool) { //... // allocate new stack new := stackalloc(uint32(newsize)) //... } stackalloc // stackalloc allocates an n byte stack. // // stackalloc must run on the system stack because it uses per-P // resources and must not split the stack. // //go:systemstack func stackalloc(n uint32) stack { // Small stacks are allocated with a fixed-size free-list allocator. // If we need a stack of a bigger size, we fall back on allocating // a dedicated span. var v unsafe.Pointer if n \u0026lt; _FixedStack\u0026lt;\u0026lt;_NumStackOrders \u0026amp;\u0026amp; n \u0026lt; _StackCacheSize { //小堆栈用固定大小的自由列表分配器进行分配。 } else { //... if s == nil { // 如果我们需要一个更大的堆栈，我们会重新分配一个span. // Allocate a new stack from the heap. s = mheap_.allocManual(npage, \u0026amp;memstats.stacks_inuse) if s == nil { throw(\u0026#34;out of memory\u0026#34;) } osStackAlloc(s) s.elemsize = uintptr(n) } //... } //... } https://medium.com/a-journey-with-go/go-how-does-the-goroutine-stack-size-evolve-447fc02085e5#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjAzYjJkMjJjMmZlY2Y4NzNlZDE5ZTViOGNmNzA0YWZiN2UyZWQ0YmUiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MTIxNjkyMDEsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwMzk2OTgxODc3ODk3MjQyMjU0OSIsImVtYWlsIjoiZmZ6eGMuZG9AZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJ0aW0gWmhhbyIsInBpY3R1cmUiOiJodHRwczovL2xoNS5nb29nbGV1c2VyY29udGVudC5jb20vLVVrRk9jak5NWUJzL0FBQUFBQUFBQUFJL0FBQUFBQUFBQUFBL0FNWnV1Y25LajAwY21MVkhuZGpYakxGVVZWX1JHWnJ0OXcvczk2LWMvcGhvdG8uanBnIiwiZ2l2ZW5fbmFtZSI6InRpbSIsImZhbWlseV9uYW1lIjoiWmhhbyIsImlhdCI6MTYxMjE2OTUwMSwiZXhwIjoxNjEyMTczMTAxLCJqdGkiOiI5ZWZhMzYwMDUwOGNhZjg0MWMyYjQ5YmE1NDQxMWNkMGQzNmE0YzliIn0.qaSr0IXzZ3BXiIndb18-qLZpwNMDi3fEGlR-OtbQryxR8MkhONlgB-BTN5vHjYuvmWdCdGywJn26T71jPqBRVuIXMNlriZLFwPvHKdTBRrvpkPoFs8LprEpsCyZbTN7qNU5-CfDzcC1fHZji2j7992Ngo3XVd9v-6LiKCGUeolZ7CGH6KvT1e67ckiCzEN2oG5q6v7zKW32FmF7cajuOHl12p2pn6LaHxlYm3o38N3O9c96cO04meQ7WzZKn6QVoZeDIvmzq1iIKYOCbU0edZiOXgRgGHkBeBOowi-DHCz9kSJ1HkNrdzyjEC2nKUqerVGTCAc08w9BjtA8o_RmA8g\n执行系统调用抢占 # handoffp函数:判断是否需要启动工作线程来接管_p_，如果不需要则把_p_放入P的全局空闲队列.\n// Hands off P from syscall or locked M. // Always runs without a P, so write barriers are not allowed. //go:nowritebarrierrec func handoffp(_p_ *p) { // handoffp must start an M in any situation where // findrunnable would return a G to run on _p_. // if it has local work, start it straight away if !runqempty(_p_) || sched.runqsize != 0 { // 运行队列不为空，需要获得一个m来接管,而不是创建一个M结构体,和创建一个线程; startm(_p_, false) // 这个我们前面讨论过, return } // if it has GC work, start it straight away if gcBlackenEnabled != 0 \u0026amp;\u0026amp; gcMarkWorkAvailable(_p_) { // 如果有GC工作，就立即开始 startm(_p_, false) return } // no local work, check that there are no spinning/idle M\u0026#39;s, // otherwise our help is not required // 没有空闲的P,没有自旋状态的Ms;所有其它p都在运行goroutine，说明系统比较忙，需要启动m if atomic.Load(\u0026amp;sched.nmspinning)+atomic.Load(\u0026amp;sched.npidle) == 0 \u0026amp;\u0026amp; atomic.Cas(\u0026amp;sched.nmspinning, 0, 1) { // TODO: fast atomic startm(_p_, true) return } lock(\u0026amp;sched.lock) if sched.gcwaiting != 0 { _p_.status = _Pgcstop sched.stopwait-- if sched.stopwait == 0 { notewakeup(\u0026amp;sched.stopnote) } unlock(\u0026amp;sched.lock) return } if _p_.runSafePointFn != 0 \u0026amp;\u0026amp; atomic.Cas(\u0026amp;_p_.runSafePointFn, 1, 0) { sched.safePointFn(_p_) sched.safePointWait-- if sched.safePointWait == 0 { notewakeup(\u0026amp;sched.safePointNote) } } if sched.runqsize != 0 { // 全局运行队列大小不是0; 说明Goroutine需要运行,有工作要做. unlock(\u0026amp;sched.lock) startm(_p_, false) return } // If this is the last running P and nobody is polling network, // need to wakeup another M to poll network. // 所有其它P都已经处于空闲状态,只有自己一个P还在运行; // 且这时候需要监控网络连接读写事件，则需要启动新的m来poll网络连接 if sched.npidle == uint32(gomaxprocs-1) \u0026amp;\u0026amp; atomic.Load64(\u0026amp;sched.lastpoll) != 0 { unlock(\u0026amp;sched.lock) startm(_p_, false) return } pidleput(_p_) // 无事可做，把p放入全局空闲队列 unlock(\u0026amp;sched.lock) } 需要启动工作线程来接管P.\n_p_的本地运行队列或全局运行队列里面有待运行的goroutine； 需要帮助gc完成标记工作； 系统比较忙，所有其它_p_都在运行goroutine，需要帮忙； 所有其它P都已经处于空闲状态，如果需要监控网络连接读写事件，则需要启动新的m来poll网络连接。 其中startm函数我们前面介绍过. 额外的例子 # 定义程序 # main.go\npackage main import \u0026#34;fmt\u0026#34; func call_some_job() { fmt.Println(\u0026#34;complete this job\u0026#34;) } func main() { for i:=0; i\u0026lt;100000; i++{ i=i } call_some_job() } gdb调试前准备 # 编译程序 # 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n准备mcall函数断点的文件 # gdb list /usr/lib/golang/src/runtime/proc.go:267 list /tmp/kubernets/test_preempt/main.go:1 list /usr/lib/golang/src/runtime/asm_amd64.s:454 gdb调试自定义函数 # define zxc info threads info register rbp rsp pc end gdb # [root@gitlab test_preempt]# gdb ./test GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-119.el7 Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \u0026#34;show copying\u0026#34; and \u0026#34;show warranty\u0026#34; for details. This GDB was configured as \u0026#34;x86_64-redhat-linux-gnu\u0026#34;. For bug reporting instructions, please see: \u0026lt;http://www.gnu.org/software/gdb/bugs/\u0026gt;... Reading symbols from /tmp/kubernets/test_preempt/test...done. Loading Go Runtime support. (gdb) list 1\tpackage main 2 3\timport \u0026#34;fmt\u0026#34; 4 5\tfunc call_some_job() { 6 7\tfmt.Println(\u0026#34;complete this job\u0026#34;) 8\t} 9 10\tfunc main() { (gdb) 11\tcall_some_job() 12\t} (gdb) b 10 Breakpoint 1 at 0x48cf90: file /tmp/kubernets/test_preempt/main.go, line 10. (gdb) run Starting program: /tmp/kubernets/test_preempt/./test Breakpoint 1, main.main () at /tmp/kubernets/test_preempt/main.go:10 10\tfunc main() { (gdb) disas Dump of assembler code for function main.main: =\u0026gt; 0x000000000048cf90 \u0026lt;+0\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rcx --------------------------------here 0x000000000048cf99 \u0026lt;+9\u0026gt;: cmp 0x10(%rcx),%rsp --------------------------------here 0x000000000048cf9d \u0026lt;+13\u0026gt;:\tjbe 0x48cfb9 \u0026lt;main.main+41\u0026gt; 0x000000000048cf9f \u0026lt;+15\u0026gt;:\tsub $0x8,%rsp 0x000000000048cfa3 \u0026lt;+19\u0026gt;:\tmov %rbp,(%rsp) 0x000000000048cfa7 \u0026lt;+23\u0026gt;:\tlea (%rsp),%rbp 0x000000000048cfab \u0026lt;+27\u0026gt;:\tcallq 0x48cef0 \u0026lt;main.call_some_job\u0026gt; 0x000000000048cfb0 \u0026lt;+32\u0026gt;:\tmov (%rsp),%rbp 0x000000000048cfb4 \u0026lt;+36\u0026gt;:\tadd $0x8,%rsp 0x000000000048cfb8 \u0026lt;+40\u0026gt;:\tretq 0x000000000048cfb9 \u0026lt;+41\u0026gt;:\tcallq 0x4517d0 \u0026lt;runtime.morestack_noctxt\u0026gt; --------------------------------here 0x000000000048cfbe \u0026lt;+46\u0026gt;:\tjmp 0x48cf90 \u0026lt;main.main\u0026gt; End of assembler dump. 上面三个--------------------------------here,前面我们说的很清楚,就是g.stack.stackguard0与sp寄存器进行比较,如果sp小于g.stack.stackguard0 就跳转到runtime.morestack_noctxt;而我们前面设置preempt:gp.stackguard0 = stackPreempt //stackguard0==很大的数; 使被抢占的goroutine;在进行函数调用会去检查栈溢出;去处理抢占请求,它必定比sp要大,所以肯定跳转到了runtime.morestack_noctxt\nMOVQ\t0(SP), AX // f's PC,就是caller\u0026rsquo;s pc是因为它的rbp在那一步还没有保存到callee‘s stack空间.\n那继续来看如果如果调用\u0026lt;runtime.morestack_noctxt\u0026gt;,它的下一个PC就是jmp 0x48cf90 \u0026lt;main.main\u0026gt;又重新跳回来了. 看这个 "},{"id":3,"href":"/docs/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E8%B0%83%E5%BA%A6/%E4%BA%8C-%E8%B0%83%E5%BA%A6%E7%9A%84%E6%97%B6%E6%9C%BA/4.2.3-%E5%89%A5%E5%A4%BA%E8%B0%83%E5%BA%A6/4.2.3.2-%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%94%B6%E5%B0%BE%E5%A6%82%E6%9E%9C%E4%BB%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E8%BF%94%E5%9B%9E%E5%A6%82%E4%BD%95%E9%87%8D%E6%96%B0%E5%BE%97%E5%88%B0P/","title":"4.2.3.2 系统调用收尾,如从系统调用返回,如何重新得到P","section":"4.2.3 剥夺调度","content":" 定义程序 # main.go\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) var path = \u0026#34;appss.txt\u0026#34; func isError(err error) bool { if err != nil { fmt.Println(err.Error()) } return (err != nil) } func main() { var file, err = os.OpenFile(path, os.O_RDWR, 0644) if isError(err) { return } defer file.Close() } gdb调试前准备 # 编译程序 # 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n准备mcall函数断点的文件 # gdb list /usr/lib/golang/src/syscall/zsyscall_linux_amd64.go:62 list /usr/lib/golang/src/syscall/asm_linux_amd64.s:44 gdb # // func Syscall6(trap, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr) TEXT ·Syscall6(SB),NOSPLIT,$0-80 CALL\truntime·entersyscall(SB) MOVQ\ta1+8(FP), DI MOVQ\ta2+16(FP), SI MOVQ\ta3+24(FP), DX MOVQ\ta4+32(FP), R10 MOVQ\ta5+40(FP), R8 MOVQ\ta6+48(FP), R9 SYSCALL CMPQ\tAX, $0xfffffffffffff001 JLS\tok6 MOVQ\t$-1, r1+56(FP) MOVQ\t$0, r2+64(FP) NEGQ\tAX MOVQ\tAX, err+72(FP) CALL\truntime·exitsyscall(SB) RET ok6: MOVQ\tAX, r1+56(FP) MOVQ\tDX, r2+64(FP) MOVQ\t$0, err+72(FP) CALL\truntime·exitsyscall(SB) RET entersyscall # src/runtime/proc.go\nfunc entersyscall() { reentersyscall(getcallerpc(), getcallersp()) // 这个是Goroutine的pc, sp,不是g0的，因为还没有切换栈。 } reentersyscall # reentersyscall主要是做三件事: 把PC,SP保存到当前Goroutine.sched里面; 解除M与P两者之间的关系; 设置P的状态为_Psyscall /* - 把PC,SP保存到当前Goroutine.sched里面; - 解除M与P两者之间的关系; - 设置P的状态为_Psyscall */ func reentersyscall(pc, sp uintptr) { _g_ := getg() // get Goroutine的g // Disable preemption because during this function g is in Gsyscall status, // but can have inconsistent g-\u0026gt;sched, do not let GC observe it. _g_.m.locks++ // ++就能让GC不能观察到？TODO zxc: // Entersyscall must not call any function that might split/grow the stack. // (See details in comment above.) // Catch calls that might, by replacing the stack guard with something that // will trip any stack check and leaving a flag to tell newstack to die. _g_.stackguard0 = stackPreempt //进入系统调用前就设置了抢占标志。 _g_.throwsplit = true // Leave SP around for GC and traceback. save(pc, sp) //保存寄存器的值到当前Goroutine的sched结构体。 _g_.syscallsp = sp //gc使用 _g_.syscallpc = pc //gc使用 casgstatus(_g_, _Grunning, _Gsyscall) // 修改状态 if _g_.syscallsp \u0026lt; _g_.stack.lo || _g_.stack.hi \u0026lt; _g_.syscallsp { systemstack(func() { print(\u0026#34;entersyscall inconsistent \u0026#34;, hex(_g_.syscallsp), \u0026#34; [\u0026#34;, hex(_g_.stack.lo), \u0026#34;,\u0026#34;, hex(_g_.stack.hi), \u0026#34;]\\n\u0026#34;) throw(\u0026#34;entersyscall\u0026#34;) }) } if trace.enabled { systemstack(traceGoSysCall) // systemstack itself clobbers g.sched.{pc,sp} and we might // need them later when the G is genuinely blocked in a // syscall save(pc, sp) } if atomic.Load(\u0026amp;sched.sysmonwait) != 0 { systemstack(entersyscall_sysmon) save(pc, sp) } if _g_.m.p.ptr().runSafePointFn != 0 { // runSafePointFn may stack split if run on this stack systemstack(runSafePointFn) save(pc, sp) } _g_.m.syscalltick = _g_.m.p.ptr().syscalltick //把P的syscalltick,放到m中。 _g_.sysblocktraced = true _g_.m.mcache = nil pp := _g_.m.p.ptr() pp.m = 0 // 解除P与M的关系。 _g_.m.oldp.set(pp) // 把现在的P放到M中的oldp中。 _g_.m.p = 0 // 解除M与P的关系。 atomic.Store(\u0026amp;pp.status, _Psyscall) // 修改P的状态为系统调用。 if sched.gcwaiting != 0 { systemstack(entersyscall_gcwait) save(pc, sp) } _g_.m.locks-- // --解除锁定。 } 这里需要注意的是：在进入系统调用的时候，它是没有进行自增的，它是在exitsyscall()函数才开始进行自增的；\n这个就是为了判断P，在当前Goroutine进入系统调用，到返回的那一段时间，这个P有可能又被其他M关联，然后又进入_Psyscall状态，_g_.m.syscalltick = _g_.m.p.ptr().syscalltick //把P的syscalltick,放到m中。\nexitsyscall # 这个退出系统调用： 尝试重新绑定oldp,如果没有成功，从全局空闲P队列获得一个P。 如果还是失败，mcall\u0026ndash;\u0026gt;exitsyscall0()， 在这个里面再次从全局空闲P队列中尝试下，如果失败就把Goroutine放入全局空闲G队列; M放入全局空闲M队列,休眠M; schedule(). /* 这个退出系统调用： - 尝试重新绑定oldp,如果没有成功，从全局空闲P队列获得一个P。 - 如果还是失败，mcall--\u0026gt;exitsyscall0()， - 在这个里面再次从全局空闲P队列中尝试下，如果失败就把Goroutine放入全局空闲G队列; - M放入全局空闲M队列,休眠M; - schedule(). */ func exitsyscall() { _g_ := getg() _g_.m.locks++ // see comment in entersyscall 防止GC？ TODO zxc: if getcallersp() \u0026gt; _g_.syscallsp { throw(\u0026#34;exitsyscall: syscall frame is no longer valid\u0026#34;) } _g_.waitsince = 0 oldp := _g_.m.oldp.ptr() //重新取出oldp _g_.m.oldp = 0 if exitsyscallfast(oldp) { //如果返回true，那么M与P在这个里面已经重新关联了。 if _g_.m.mcache == nil { throw(\u0026#34;lost mcache\u0026#34;) } if trace.enabled { if oldp != _g_.m.p.ptr() || _g_.m.syscalltick != _g_.m.p.ptr().syscalltick { systemstack(traceGoStart) } } // There\u0026#39;s a cpu for us, so we can run. _g_.m.p.ptr().syscalltick++ //系统调用完成，syscalltick自增。 // We need to cas the status and scan before resuming... casgstatus(_g_, _Gsyscall, _Grunning) // Garbage collector isn\u0026#39;t running (since we are), // so okay to clear syscallsp. _g_.syscallsp = 0 _g_.m.locks-- if _g_.preempt { // restore the preemption request in case we\u0026#39;ve cleared it in newstack _g_.stackguard0 = stackPreempt } else { // otherwise restore the real _StackGuard, we\u0026#39;ve spoiled it in entersyscall/entersyscallblock _g_.stackguard0 = _g_.stack.lo + _StackGuard //在entersyscall里面我们设置_g_.stackguard0 = stackPreempt //进入系统调用前就设置了抢占标志。这里要恢复。 } _g_.throwsplit = false if sched.disable.user \u0026amp;\u0026amp; !schedEnabled(_g_) { // Scheduling of this goroutine is disabled. Gosched() } return } _g_.sysexitticks = 0 if trace.enabled { // Wait till traceGoSysBlock event is emitted. // This ensures consistency of the trace (the goroutine is started after it is blocked). for oldp != nil \u0026amp;\u0026amp; oldp.syscalltick == _g_.m.syscalltick { osyield() } // We can\u0026#39;t trace syscall exit right now because we don\u0026#39;t have a P. // Tracing code can invoke write barriers that cannot run without a P. // So instead we remember the syscall exit time and emit the event // in execute when we have a P. _g_.sysexitticks = cputicks() } _g_.m.locks-- // Call the scheduler. mcall(exitsyscall0) if _g_.m.mcache == nil { throw(\u0026#34;lost mcache\u0026#34;) } // Scheduler returned, so we\u0026#39;re allowed to run now. // Delete the syscallsp information that we left for // the garbage collector during the system call. // Must wait until now because until gosched returns // we don\u0026#39;t know for sure that the garbage collector // is not running. _g_.syscallsp = 0 _g_.m.p.ptr().syscalltick++ _g_.throwsplit = false } exitsyscallfast # //go:nosplit func exitsyscallfast(oldp *p) bool { _g_ := getg() // Freezetheworld sets stopwait but does not retake P\u0026#39;s. if sched.stopwait == freezeStopWait { return false } // Try to re-acquire the last P. if oldp != nil \u0026amp;\u0026amp; oldp.status == _Psyscall \u0026amp;\u0026amp; atomic.Cas(\u0026amp;oldp.status, _Psyscall, _Pidle) { /* - 查看老的P的状态是否是正处于_Psyscall; - 从reentersyscall里面的三个步骤，当它设置为_Psyscall, 它这个时候是没有与任何M相关联。 - 所以这里如果发现P又处于_psyscall，直接关联。 */ // There\u0026#39;s a cpu for us, so we can run. wirep(oldp) // 关联M和P；当前的M和这个oldp。 exitsyscallfast_reacquired() return true } // Try to get any other idle P. if sched.pidle != 0 { var ok bool systemstack(func() { ok = exitsyscallfast_pidle() if ok \u0026amp;\u0026amp; trace.enabled { if oldp != nil { // Wait till traceGoSysBlock event is emitted. // This ensures consistency of the trace (the goroutine is started after it is blocked). for oldp.syscalltick == _g_.m.syscalltick { osyield() } } traceGoSysExit(0) } }) if ok { return true } } return false } exitsyscallfast_reacquired # func exitsyscallfast_reacquired() { _g_ := getg() if _g_.m.syscalltick != _g_.m.p.ptr().syscalltick { // 如果他们两者不相等，那么说明该p被收回，然后再次进入syscall(因为_g_.m.syscalltick变了) if trace.enabled { // The p was retaken and then enter into syscall again (since _g_.m.syscalltick has changed). // traceGoSysBlock for this syscall was already emitted, // but here we effectively retake the p from the new syscall running on the same p. systemstack(func() { // Denote blocking of the new syscall. traceGoSysBlock(_g_.m.p.ptr()) // Denote completion of the current syscall. traceGoSysExit(0) }) } _g_.m.p.ptr().syscalltick++ // 这里又开始自增了---\u0026gt;因为它在进入reentersyscall()函数是不能增加这个值的。只有当退出exitsyscall()函数才会自增，所以如果 } } mcall(exitsyscall0) # // exitsyscall slow path on g0. // Failed to acquire P, enqueue gp as runnable. // //go:nowritebarrierrec func exitsyscall0(gp *g) { _g_ := getg() casgstatus(gp, _Gsyscall, _Grunnable) //从系统调用状态转变为可运行状态 dropg() //断开M与G之间的关系 lock(\u0026amp;sched.lock) //要修改全局的sched,先加锁 var _p_ *p if schedEnabled(_g_) { _p_ = pidleget() //从全局空闲P队列获取一个P } if _p_ == nil { globrunqput(gp) //如果没有获取P，那么把Goroutine放入全局空闲g队列。 } else if atomic.Load(\u0026amp;sched.sysmonwait) != 0 { atomic.Store(\u0026amp;sched.sysmonwait, 0) notewakeup(\u0026amp;sched.sysmonnote) } unlock(\u0026amp;sched.lock) if _p_ != nil { //如果有获取到P。 acquirep(_p_) // 关联P与M execute(gp, false) // Never returns. 直接执行 } if _g_.m.lockedg != 0 { // TODO zxc: 我记得是这个某个g,必须运行在某个线程上面，比如，main.main. // Wait until another thread schedules gp and so m again. stoplockedm() execute(gp, false) // Never returns. } stopm() //停止M。 schedule() // Never returns. } syscalltick # 这个syscalltick;发现不是每次系统调用一次，才增加一次。\n在我们这里，\nentersystem g.m.syscalltick = g.m.p.ptr().syscalltick existsystem exitsyscall主函数里面有一次; exitsyscallfast_reacquired函数又增加了一次. // To ensure that traceGoSysExit is emitted strictly after traceGoSysBlock, // we remember current value of syscalltick in m (g.m.syscalltick = g.m.p.ptr().syscalltick), // whoever emits traceGoSysBlock increments p.syscalltick afterwards; // and we wait for the increment before emitting traceGoSysExit. // Note that the increment is done even if tracing is not enabled, // because tracing can be enabled in the middle of syscall. We don\u0026rsquo;t want the wait to hang.\n// 为了确保traceGoSysExit严格在traceGoSysBlock之后发出。 // 我们记住m中syscalltick的当前值(g.m.syscalltick = g.m.p.ptr().syscalltick)。 // 不管是谁发出traceGoSysBlock，都会在之后增量p.syscalltick。 // 我们等待增量后再发出 traceGoSysExit。 // 注意，即使没有启用跟踪，增量也会被完成。 // 因为跟踪可以在syscall中间启用。我们不希望等待被挂起。\n在这个解释里面,发现跟踪的时候也会syscalltick\n"},{"id":4,"href":"/docs/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E8%B0%83%E5%BA%A6/%E4%BA%8C-%E8%B0%83%E5%BA%A6%E7%9A%84%E6%97%B6%E6%9C%BA/","title":"二 调度的时机","section":"第四章 调度","content":" 调度时机总结: 到这里我们的调度时机就总结完了，主要是什么时候发生调度: 主动调度 被动调度 剥夺调度 运行时间过长 系统调用被剥夺P "},{"id":5,"href":"/docs/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E8%B0%83%E5%BA%A6/%E4%B8%89-%E8%B0%83%E5%BA%A6%E5%BE%AA%E7%8E%AF/4.3.1-%E8%B0%83%E5%BA%A6%E5%BE%AA%E7%8E%AF/","title":"4.3.1 调度循环","section":"三 调度循环","content":" 当go程序初始化到运行package main里面的main函数,g0已经被初始化,g.sched和stack被赋值,当下次切换Goroutine的时候,或者说再次调度的时候, 必然要重新使用g0,那么会重新使用g.sched.PC,g.sched.SP? g0栈是否重新使用初始时候mstart1函数的栈 # main.go\npackage main import \u0026#34;fmt\u0026#34; // the function\u0026#39;s body is empty func add(x, y int64) int64 func main() { gg:=add(2, 3) fmt.Println(gg) } add_amd.s\nTEXT ·add(SB),$0-24 MOVQ x+0(FP), BX MOVQ y+8(FP), BP ADDQ BP, BX MOVQ BX, ret+16(FP) RET 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n[root@gitlab kubernets]# gdb test GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-119.el7 Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \u0026#34;show copying\u0026#34; and \u0026#34;show warranty\u0026#34; for details. This GDB was configured as \u0026#34;x86_64-redhat-linux-gnu\u0026#34;. For bug reporting instructions, please see: \u0026lt;http://www.gnu.org/software/gdb/bugs/\u0026gt;... Reading symbols from /tmp/kubernets/test...done. Loading Go Runtime support. (gdb) list /usr/lib/golang/src/runtime/asm_amd64.s:294 289\t// func mcall(fn func(*g)) 290\t// Switch to m-\u0026gt;g0\u0026#39;s stack, call fn(g). 291\t// Fn must never return. It should gogo(\u0026amp;g-\u0026gt;sched) 292\t// to keep running g. 293\tTEXT runtime·mcall(SB), NOSPLIT, $0-8 294\tMOVQ\tfn+0(FP), DI 295 296\tget_tls(CX) 297\tMOVQ\tg(CX), AX\t// save state in g-\u0026gt;sched 298\tMOVQ\t0(SP), BX\t// caller\u0026#39;s PC (gdb) 299\tMOVQ\tBX, (g_sched+gobuf_pc)(AX) 300\tLEAQ\tfn+0(FP), BX\t// caller\u0026#39;s SP 301\tMOVQ\tBX, (g_sched+gobuf_sp)(AX) 302\tMOVQ\tAX, (g_sched+gobuf_g)(AX) 303\tMOVQ\tBP, (g_sched+gobuf_bp)(AX) 304 305\t// switch to m-\u0026gt;g0 \u0026amp; its stack, call fn 306\tMOVQ\tg(CX), BX 307\tMOVQ\tg_m(BX), BX 308\tMOVQ\tm_g0(BX), SI (gdb) 309\tCMPQ\tSI, AX\t// if g == m-\u0026gt;g0 call badmcall 310\tJNE\t3(PC) 311\tMOVQ\t$runtime·badmcall(SB), AX 312\tJMP\tAX 313\tMOVQ\tSI, g(CX)\t// g = m-\u0026gt;g0 314\tMOVQ\t(g_sched+gobuf_sp)(SI), SP\t// sp = m-\u0026gt;g0-\u0026gt;sched.sp 315\tPUSHQ\tAX 316\tMOVQ\tDI, DX 317\tMOVQ\t0(DI), DI 318\tCALL\tDI (gdb) 319\tPOPQ\tAX 320\tMOVQ\t$runtime·badmcall2(SB), AX 321\tJMP\tAX 322\tRET 323 324\t// systemstack_switch is a dummy routine that systemstack leaves at the bottom 325\t// of the G stack. We need to distinguish the routine that 326\t// lives at the bottom of the G stack from the one that lives 327\t// at the top of the system stack because the one at the top of 328\t// the system stack terminates the stack walk (see topofstack()). (gdb) b 318 Breakpoint 1 at 0x44b229: file /usr/lib/golang/src/runtime/asm_amd64.s, line 318. (gdb) c The program is not being run. (gdb) run Starting program: /tmp/kubernets/test [New LWP 29827] [Switching to LWP 29827] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) info register rbp rsp pc rbp 0xc000030fa0\t0xc000030fa0 rsp 0xc000045fd0\t0xc000045fd0 pc 0x44b229\t0x44b229 \u0026lt;runtime.mcall+89\u0026gt; (gdb) disas Dump of assembler code for function runtime.mcall: 0x000000000044b1d0 \u0026lt;+0\u0026gt;:\tmov 0x8(%rsp),%rdi 0x000000000044b1d5 \u0026lt;+5\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rax 0x000000000044b1de \u0026lt;+14\u0026gt;:\tmov (%rsp),%rbx 0x000000000044b1e2 \u0026lt;+18\u0026gt;:\tmov %rbx,0x40(%rax) 0x000000000044b1e6 \u0026lt;+22\u0026gt;:\tlea 0x8(%rsp),%rbx 0x000000000044b1eb \u0026lt;+27\u0026gt;:\tmov %rbx,0x38(%rax) 0x000000000044b1ef \u0026lt;+31\u0026gt;:\tmov %rax,0x48(%rax) 0x000000000044b1f3 \u0026lt;+35\u0026gt;:\tmov %rbp,0x68(%rax) 0x000000000044b1f7 \u0026lt;+39\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rbx 0x000000000044b200 \u0026lt;+48\u0026gt;:\tmov 0x30(%rbx),%rbx 0x000000000044b204 \u0026lt;+52\u0026gt;:\tmov (%rbx),%rsi 0x000000000044b207 \u0026lt;+55\u0026gt;:\tcmp %rax,%rsi 0x000000000044b20a \u0026lt;+58\u0026gt;:\tjne 0x44b215 \u0026lt;runtime.mcall+69\u0026gt; 0x000000000044b20c \u0026lt;+60\u0026gt;:\tlea -0x23343(%rip),%rax # 0x427ed0 \u0026lt;runtime.badmcall\u0026gt; 0x000000000044b213 \u0026lt;+67\u0026gt;:\tjmpq *%rax 0x000000000044b215 \u0026lt;+69\u0026gt;:\tmov %rsi,%fs:0xfffffffffffffff8 0x000000000044b21e \u0026lt;+78\u0026gt;:\tmov 0x38(%rsi),%rsp 0x000000000044b222 \u0026lt;+82\u0026gt;:\tpush %rax 0x000000000044b223 \u0026lt;+83\u0026gt;:\tmov %rdi,%rdx 0x000000000044b226 \u0026lt;+86\u0026gt;:\tmov (%rdi),%rdi =\u0026gt; 0x000000000044b229 \u0026lt;+89\u0026gt;:\tcallq *%rdi 0x000000000044b22b \u0026lt;+91\u0026gt;:\tpop %rax // callq的下一个指令 0x000000000044b22b -------------------here 0x000000000044b22c \u0026lt;+92\u0026gt;:\tlea -0x23323(%rip),%rax # 0x427f10 \u0026lt;runtime.badmcall2\u0026gt; 0x000000000044b233 \u0026lt;+99\u0026gt;:\tjmpq *%rax 0x000000000044b235 \u0026lt;+101\u0026gt;:\tretq End of assembler dump. (gdb) step runtime.park_m (gp=0xc000000780) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { (gdb) info register rbp rsp pc rbp 0xc000030fa0\t0xc000030fa0 rsp 0xc000045fc8\t0xc000045fc8 pc 0x42d490\t0x42d490 \u0026lt;runtime.park_m\u0026gt; (gdb) x/32xb 0xc000045fc0 0xc000045fc0:\t0x80\t0x0a\t0x00\t0x00\t0xc0\t0x00\t0x00\t0x00 0xc000045fc8:\t0x2b\t0xb2\t0x44\t0x00\t0x00\t0x00\t0x00\t0x00 // 0x000000000044b22b -------------------here 0xc000045fd0:\t0x80\t0x07\t0x00\t0x00\t0xc0\t0x00\t0x00\t0x00 0xc000045fd8:\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 (gdb) disas Dump of assembler code for function runtime.park_m: =\u0026gt; 0x000000000042d490 \u0026lt;+0\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rcx 0x000000000042d499 \u0026lt;+9\u0026gt;:\tcmp 0x10(%rcx),%rsp 0x000000000042d49d \u0026lt;+13\u0026gt;:\tjbe 0x42d642 \u0026lt;runtime.park_m+434\u0026gt; //判断是否越界 0x000000000042d4a3 \u0026lt;+19\u0026gt;:\tsub $0x28,%rsp 0x000000000042d4a7 \u0026lt;+23\u0026gt;:\tmov %rbp,0x20(%rsp) 0x000000000042d4ac \u0026lt;+28\u0026gt;:\tlea 0x20(%rsp),%rbp 0x000000000042d4b1 \u0026lt;+33\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rax 0x000000000042d4ba \u0026lt;+42\u0026gt;:\tmov %rax,0x18(%rsp) 0x000000000042d4bf \u0026lt;+47\u0026gt;:\tcmpb $0x0,0xaf4ca(%rip) # 0x4dc990 \u0026lt;runtime.trace+16\u0026gt; 0x000000000042d4c6 \u0026lt;+54\u0026gt;:\tjne 0x42d61e \u0026lt;runtime.park_m+398\u0026gt; ---Type \u0026lt;return\u0026gt; to continue, or q \u0026lt;return\u0026gt; to quit---q Quit (gdb) 从\u0026mdash;-here可以看出来,当使用mcallg0\u0026rsquo;s stack\u0026gt;,只是使用了g0的栈,没有用g.sched.pc,而是用的mcall里面的callq的下一个指令 0x000000000044b22b -------------------here\n文件src/runtime/proc.go\n// park continuation on g0. func park_m(gp *g) { _g_ := getg() if trace.enabled { traceGoPark(_g_.m.waittraceev, _g_.m.waittraceskip) } casgstatus(gp, _Grunning, _Gwaiting) dropg() if fn := _g_.m.waitunlockf; fn != nil { ok := fn(gp, _g_.m.waitlock) _g_.m.waitunlockf = nil _g_.m.waitlock = nil if !ok { if trace.enabled { traceGoUnpark(gp, 2) } casgstatus(gp, _Gwaiting, _Grunnable) execute(gp, true) // Schedule it back, never returns. } } schedule() } 每个线程对应g0中g0.sched.SP会一直改变? # 在初始化后就不会改变了,下次调度,直接把g0.sched.SP重新赋值给寄存器SP.\ntype g struct { //... sched gobuf //... } type gobuf struct { sp uintptr -------------------------------here pc uintptr g guintptr ctxt unsafe.Pointer ret sys.Uintreg lr uintptr bp uintptr // for GOEXPERIMENT=framepointer } 示例: # 实践想法 # 有两个go关键字运行起来的Goroutine 当main goroutine睡眠的时候\u0026mdash;-[切换到g0栈]\u0026mdash;\u0026ndash;\u0026gt;其中一个Goroutine\u0026mdash;-[切换到g0栈]\u0026ndash;\u0026gt;另一个Goroutine或者main goroutine 所以只要看下这个切换的时候,mcall函数中(具体可看part2中:非main goroutine退出)MOVQ\t(g_sched+gobuf_sp)(SI), SP是否一直不变 // func mcall(fn func(*g)) // Switch to m-\u0026gt;g0\u0026#39;s stack, call fn(g). ------------------------here TEXT runtime·mcall(SB), NOSPLIT, $0-8 MOVQ\tfn+0(FP), DI get_tls(CX) MOVQ\tg(CX), AX\t// save state in g-\u0026gt;sched MOVQ\t0(SP), BX\t// caller\u0026#39;s PC MOVQ\tBX, (g_sched+gobuf_pc)(AX) LEAQ\tfn+0(FP), BX\t// caller\u0026#39;s SP MOVQ\tBX, (g_sched+gobuf_sp)(AX) MOVQ\tAX, (g_sched+gobuf_g)(AX) MOVQ\tBP, (g_sched+gobuf_bp)(AX) // switch to m-\u0026gt;g0 \u0026amp; its stack, call fn MOVQ\tg(CX), BX MOVQ\tg_m(BX), BX MOVQ\tm_g0(BX), SI CMPQ\tSI, AX\t// if g == m-\u0026gt;g0 call badmcall JNE\t3(PC) MOVQ\t$runtime·badmcall(SB), AX JMP\tAX MOVQ\tSI, g(CX)\t// g = m-\u0026gt;g0 MOVQ\t(g_sched+gobuf_sp)(SI), SP\t// sp = m-\u0026gt;g0-\u0026gt;sched.sp -------------------here PUSHQ\tAX MOVQ\tDI, DX MOVQ\t0(DI), DI CALL\tDI POPQ\tAX MOVQ\t$runtime·badmcall2(SB), AX JMP\tAX RET 定义程序 # main.go\npackage main import ( \u0026#34;runtime\u0026#34; \u0026#34;sync/atomic\u0026#34; ) var existFlag int32 = 2 // the function\u0026#39;s body is empty func addAssemble(x, y int64) int64 func add(a int64){ addAssemble(a, a) atomic.AddInt32(\u0026amp;existFlag, -1) } func main() { runtime.GOMAXPROCS(1) go add(1) go add(2) for { if atomic.LoadInt32(\u0026amp;existFlag) \u0026lt;=0{ break } runtime.Gosched() } } add_amd.s\nTEXT ·addAssemble(SB),$0-24 MOVQ x+0(FP), BX MOVQ y+8(FP), BP ADDQ BP, BX MOVQ BX, ret+16(FP) RET gdb调试前准备 # 编译程序 # 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n准备mcall函数断点的文件 # gdb list /usr/lib/golang/src/runtime/asm_amd64.s:300 list /tmp/kubernets/test_goroutine/add_amd.s:1 gdb调试自定义函数 # define zxc info threads info register rbp rsp pc step continue end gdb # [root@gitlab test_goroutine]# gdb ./test GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-119.el7 Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \u0026#34;show copying\u0026#34; and \u0026#34;show warranty\u0026#34; for details. This GDB was configured as \u0026#34;x86_64-redhat-linux-gnu\u0026#34;. For bug reporting instructions, please see: \u0026lt;http://www.gnu.org/software/gdb/bugs/\u0026gt;... Reading symbols from /tmp/kubernets/test_goroutine/test...done. Loading Go Runtime support. (gdb) list /usr/lib/golang/src/runtime/asm_amd64.s:300 295 296\tget_tls(CX) 297\tMOVQ\tg(CX), AX\t// save state in g-\u0026gt;sched 298\tMOVQ\t0(SP), BX\t// caller\u0026#39;s PC 299\tMOVQ\tBX, (g_sched+gobuf_pc)(AX) 300\tLEAQ\tfn+0(FP), BX\t// caller\u0026#39;s SP 301\tMOVQ\tBX, (g_sched+gobuf_sp)(AX) 302\tMOVQ\tAX, (g_sched+gobuf_g)(AX) 303\tMOVQ\tBP, (g_sched+gobuf_bp)(AX) 304 (gdb) 305\t// switch to m-\u0026gt;g0 \u0026amp; its stack, call fn 306\tMOVQ\tg(CX), BX 307\tMOVQ\tg_m(BX), BX 308\tMOVQ\tm_g0(BX), SI 309\tCMPQ\tSI, AX\t// if g == m-\u0026gt;g0 call badmcall 310\tJNE\t3(PC) 311\tMOVQ\t$runtime·badmcall(SB), AX 312\tJMP\tAX 313\tMOVQ\tSI, g(CX)\t// g = m-\u0026gt;g0 314\tMOVQ\t(g_sched+gobuf_sp)(SI), SP\t// sp = m-\u0026gt;g0-\u0026gt;sched.sp (gdb) 315\tPUSHQ\tAX 316\tMOVQ\tDI, DX 317\tMOVQ\t0(DI), DI 318\tCALL\tDI 319\tPOPQ\tAX 320\tMOVQ\t$runtime·badmcall2(SB), AX 321\tJMP\tAX 322\tRET 323 324\t// systemstack_switch is a dummy routine that systemstack leaves at the bottom (gdb) b 318 Breakpoint 1 at 0x44a2c9: file /usr/lib/golang/src/runtime/asm_amd64.s, line 318. (gdb) define zxc Type commands for definition of \u0026#34;zxc\u0026#34;. End with a line saying just \u0026#34;end\u0026#34;. \u0026gt;info threads \u0026gt;info register rbp rsp pc \u0026gt;step \u0026gt;continue \u0026gt;end (gdb) zxc No threads. The program has no registers now. (gdb) run Starting program: /tmp/kubernets/test_goroutine/./test Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) zxc Id Target Id Frame * 1 LWP 18958 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 rbp 0xc000030660\t0xc000030660 rsp 0x7fffffffe440\t0x7fffffffe440 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000300) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [New LWP 19129] [Switching to LWP 19129] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame * 2 LWP 19129 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 ------------------here 1 LWP 18958 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 rbp 0xc000031f30\t0xc000031f30 rsp 0xc00003ffd0\t0xc00003ffd0 ------------------here pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000d80) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [Switching to LWP 18958] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame 2 LWP 19129 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 * 1 LWP 18958 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 rbp 0xc000030660\t0xc000030660 rsp 0x7fffffffe440\t0x7fffffffe440 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000300) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [Switching to LWP 19129] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame * 2 LWP 19129 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 ------------------here 1 LWP 18958 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 rbp 0xc000031798\t0xc000031798 rsp 0xc00003ffd0\t0xc00003ffd0 ------------------here pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000c00) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [New LWP 18963] [Switching to LWP 18963] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame * 3 LWP 18963 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 2 LWP 19129 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 1 LWP 18958 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 rbp 0xc000030fa0\t0xc000030fa0 rsp 0xc000045fd0\t0xc000045fd0 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000780) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [Switching to LWP 18958] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame 3 LWP 18963 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 2 LWP 19129 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 * 1 LWP 18958 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 rbp 0xc000030720\t0xc000030720 rsp 0x7fffffffe440\t0x7fffffffe440 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.gosched_m (gp=0xc000000300) at /usr/lib/golang/src/runtime/proc.go:2635 2635\tfunc gosched_m(gp *g) { Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame 3 LWP 18963 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 2 LWP 19129 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 * 1 LWP 18958 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 rbp 0xc000032fc8\t0xc000032fc8 rsp 0x7fffffffe440\t0x7fffffffe440 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.goexit0 (gp=0xc000001380) at /usr/lib/golang/src/runtime/proc.go:2674 2674\tfunc goexit0(gp *g) { Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame 3 LWP 18963 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 2 LWP 19129 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 * 1 LWP 18958 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 rbp 0xc0000327c8\t0xc0000327c8 rsp 0x7fffffffe440\t0x7fffffffe440 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.goexit0 (gp=0xc000001200) at /usr/lib/golang/src/runtime/proc.go:2674 2674\tfunc goexit0(gp *g) { [LWP 19129 exited] [LWP 18963 exited] [Inferior 1 (process 18958) exited normally] (gdb) 观察------------------here,可以知道每个M有自己的g0,其中的g.sched.SP初始化以后就不会改变 都是rsp 0xc00003ffd0\t0xc00003ffd0 如果不清楚可以看下线程1* 1 LWP 18958 \u0026quot;test\u0026quot; //... Id Target Id Frame * 2 LWP 19129 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 ------------------here 1 LWP 18958 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 rbp 0xc000031f30\t0xc000031f30 rsp 0xc00003ffd0\t0xc00003ffd0 ------------------here pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000d80) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [Switching to LWP 18958] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI //... Id Target Id Frame * 2 LWP 19129 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 ------------------here 1 LWP 18958 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 rbp 0xc000031798\t0xc000031798 rsp 0xc00003ffd0\t0xc00003ffd0 ------------------here pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000c00) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [New LWP 18963] [Switching to LWP 18963] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI //... "},{"id":6,"href":"/docs/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E8%B0%83%E5%BA%A6/%E4%B8%80-%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/4.1.2-%E7%9B%97%E5%8F%96goroutine%E4%BB%8E%E5%85%B6%E4%BB%96%E9%98%9F%E5%88%97/","title":"4.1.2 盗取goroutine从其他队列","section":"一 调度策略","content":" 盗取goroutine从其他队列 # 盗取Goroutine的过程就是:尽力去各个运行队列中寻找goroutine，如果实在找不到则进入睡眠状态.\nfindrunnable函数 # // Finds a runnable goroutine to execute. // Tries to steal from other P\u0026#39;s, get g from global queue, poll network. func findrunnable() (gp *g, inheritTime bool) { _g_ := getg() // The conditions here and in handoffp must agree: if // findrunnable would return a G to run, handoffp must start // an M. top: _p_ := _g_.m.p.ptr() //... // local runq //再次看一下本地运行队列是否有需要运行的goroutine if gp, inheritTime := runqget(_p_); gp != nil { return gp, inheritTime } // global runq //再看看全局运行队列是否有需要运行的goroutine if sched.runqsize != 0 { lock(\u0026amp;sched.lock) gp := globrunqget(_p_, 0) unlock(\u0026amp;sched.lock) if gp != nil { return gp, false } } //... // Steal work from other P\u0026#39;s. // 使用所有P与空闲的P进行比较，如果除了自己，其他的P都是休眠状态。那么整个系统都没有工作需要做了。 procs := uint32(gomaxprocs) if atomic.Load(\u0026amp;sched.npidle) == procs-1 { // Either GOMAXPROCS=1 or everybody, except for us, is idle already. // New work can appear from returning syscall/cgocall, network or timers. // Neither of that submits to local run queues, so no point in stealing. goto stop //直接退出 } // If number of spinning M\u0026#39;s \u0026gt;= number of busy P\u0026#39;s, block. // This is necessary to prevent excessive CPU consumption // when GOMAXPROCS\u0026gt;\u0026gt;1 but the program parallelism is low. // 当这个M不是自旋状态，并且此时的二倍的自旋M大于当前正在工作的P; 说明此时有许多现在在寻找工作做. if !_g_.m.spinning \u0026amp;\u0026amp; 2*atomic.Load(\u0026amp;sched.nmspinning) \u0026gt;= procs-atomic.Load(\u0026amp;sched.npidle) { goto stop } if !_g_.m.spinning { //设置m的状态为spinning _g_.m.spinning = true //处于spinning状态的m数量加一 atomic.Xadd(\u0026amp;sched.nmspinning, 1) } //从其它p的本地运行队列盗取goroutine for i := 0; i \u0026lt; 4; i++ { for enum := stealOrder.start(fastrand()); !enum.done(); enum.next() { if sched.gcwaiting != 0 { goto top } stealRunNextG := i \u0026gt; 2 // first look for ready queues with more than 1 g if gp := runqsteal(_p_, allp[enum.position()], stealRunNextG); gp != nil { return gp, false } } } stop: //... // Before we drop our P, make a snapshot of the allp slice, // which can change underfoot once we no longer block // safe-points. We don\u0026#39;t need to snapshot the contents because // everything up to cap(allp) is immutable. allpSnapshot := allp // return P and block lock(\u0026amp;sched.lock) //... if sched.runqsize != 0 { gp := globrunqget(_p_, 0) unlock(\u0026amp;sched.lock) return gp, false } // 当前工作线程解除与p之间的绑定，准备去休眠 if releasep() != _p_ { throw(\u0026#34;findrunnable: wrong p\u0026#34;) } //把p放入空闲队列 pidleput(_p_) unlock(\u0026amp;sched.lock) // Delicate dance: thread transitions from spinning to non-spinning state, // potentially concurrently with submission of new goroutines. We must // drop nmspinning first and then check all per-P queues again (with // #StoreLoad memory barrier in between). If we do it the other way around, // another thread can submit a goroutine after we\u0026#39;ve checked all run queues // but before we drop nmspinning; as the result nobody will unpark a thread // to run the goroutine. // If we discover new work below, we need to restore m.spinning as a signal // for resetspinning to unpark a new worker thread (because there can be more // than one starving goroutine). However, if after discovering new work // we also observe no idle Ps, it is OK to just park the current thread: // the system is fully loaded so no spinning threads are required. // Also see \u0026#34;Worker thread parking/unparking\u0026#34; comment at the top of the file. wasSpinning := _g_.m.spinning if _g_.m.spinning { //m即将睡眠，状态不再是spinning _g_.m.spinning = false if int32(atomic.Xadd(\u0026amp;sched.nmspinning, -1)) \u0026lt; 0 { throw(\u0026#34;findrunnable: negative nmspinning\u0026#34;) } } // check all runqueues once again // 休眠之前再看一下是否有工作要做 for _, _p_ := range allpSnapshot { if !runqempty(_p_) { lock(\u0026amp;sched.lock) _p_ = pidleget() unlock(\u0026amp;sched.lock) if _p_ != nil { acquirep(_p_) if wasSpinning { _g_.m.spinning = true atomic.Xadd(\u0026amp;sched.nmspinning, 1) } goto top } break } } ...... //休眠 stopm() goto top } 随机遍历从其他的工作线程对应P上盗取G # 尽量减少M的自旋状态时间,只有在盗取的时候才把spinning标志位设为true,盗取退出后把spinning标志位重新设置为false, if !_g_.m.spinning { _g_.m.spinning = true atomic.Xadd(\u0026amp;sched.nmspinning, 1) } 随机遍历全局P中的P,如果有goroutines,就偷盗一半来运行. 工作线程进入睡眠 # src/runtime/proc.go:1910\n停止执行当前的m，直到有新的工作。带着获得的P返回。\n// Stops execution of the current m until new work is available. // Returns with acquired P. func stopm() { _g_ := getg() if _g_.m.locks != 0 { throw(\u0026#34;stopm holding locks\u0026#34;) } if _g_.m.p != 0 { throw(\u0026#34;stopm holding p\u0026#34;) } if _g_.m.spinning { throw(\u0026#34;stopm spinning\u0026#34;) } lock(\u0026amp;sched.lock) mput(_g_.m) //全局的sched.midle空闲队列 unlock(\u0026amp;sched.lock) notesleep(\u0026amp;_g_.m.park) //进入睡眠 noteclear(\u0026amp;_g_.m.park) acquirep(_g_.m.nextp.ptr()) _g_.m.nextp = 0 } note是go runtime实现的一次性睡眠和唤醒机制.\n这里的notesleep( *note)是一个抽象函数,根据不同的平台或系统有不同的实现.\ndragonfly freebsd linux[src/runtime/lock_futex.go] futex系统调用 futexsleep函数 aix darwin nacl netbsd openbsd plan9 solaris window[src/runtime/lock_sema.go] POSIX Threads js,was[src/runtime/lock_js.go] 不支持 从上面可以看出,note层增加对特定平台的支持,就可以复用上层代码. notesleep # func notesleep(n *note) { gp := getg() if gp != gp.m.g0 { throw(\u0026#34;notesleep not on g0\u0026#34;) } ns := int64(-1) if *cgo_yield != nil { // Sleep for an arbitrary-but-moderate interval to poll libc interceptors. ns = 10e6 } for atomic.Load(key32(\u0026amp;n.key)) == 0 { // --------here gp.m.blocked = true futexsleep(key32(\u0026amp;n.key), 0, ns) //进入睡眠 if *cgo_yield != nil { asmcgocall(*cgo_yield, nil) } gp.m.blocked = false } } 从这里可以看出来:如果futexsleep()退出,但是检查note.key还是为0,那么又会进入睡眠,for atomic.Load(key32(\u0026amp;n.key)) == 0 { // --------here,\n并不是其它工作线程唤醒了这个线程,所以我们知道当其他线程唤醒这个线程,需要改下这个线程对应的m结构体中note.key字段\ntype m struct { //... park note //... } "},{"id":7,"href":"/docs/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E8%B0%83%E5%BA%A6/%E4%B8%80-%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/4.1.1-%E4%BB%8E%E6%9C%AC%E5%9C%B0%E9%98%9F%E5%88%97%E6%88%96%E5%85%A8%E5%B1%80%E9%98%9F%E5%88%97%E8%8E%B7%E5%8F%96goroutine/","title":"4.1.1 从本地队列或全局队列获取goroutine","section":"一 调度策略","content":" 分析schedule函数 # src/runtime/proc.go\n// One round of scheduler: find a runnable goroutine and execute it. // Never returns. func schedule() { _g_ := getg() //... top: //... var gp *g var inheritTime bool //... if gp == nil { // Check the global runnable queue once in a while to ensure fairness. // Otherwise two goroutines can completely occupy the local runqueue // by constantly respawning each other. if _g_.m.p.ptr().schedtick%61 == 0 \u0026amp;\u0026amp; sched.runqsize \u0026gt; 0 { //schedtick就是调度次数,如果能被61整除且全局的Goroutine队列不为空就尝试获取 lock(\u0026amp;sched.lock) gp = globrunqget(_g_.m.p.ptr(), 1) //全局运行队列中获取goroutine unlock(\u0026amp;sched.lock) } } if gp == nil { gp, inheritTime = runqget(_g_.m.p.ptr()) //本地运行队列(当前线程)中获取goroutine if gp != nil \u0026amp;\u0026amp; _g_.m.spinning { throw(\u0026#34;schedule: spinning with local work\u0026#34;) } } if gp == nil { /* 则调用findrunnable函数从其它工作线程的运行队列中偷取，如果偷取不到，则当前工作线程进入睡眠, 直到获取到需要运行的goroutine之后findrunnable函数才会返回. */ gp, inheritTime = findrunnable() // blocks until work is available } // This thread is going to run a goroutine and is not spinning anymore, // so if it was marked as spinning we need to reset it now and potentially // start a new spinning M. if _g_.m.spinning { resetspinning() } //... execute(gp, inheritTime) } schedule函数按顺序分三步分别来获取可运行的Goroutine: 全局队列，本地G队列和其他P上面的G队列 第一步:从全局运行G队列中寻找Goroutine。 因为全局上面拿Goroutine是需要加锁的,尽量少从上面拿; 所以当一个P调度了61次之后且全局的Goroutine队列不为空，为了保证调度能运行全局G队列，所以尝试调度一下。 第二步:从工作线程本地运行队列[其实就是当前线程关联的P上面的运行G队列，因为一个P在同一时刻只能与一个M关联，此时就不需要加锁]中寻找Goroutine。 第三步:从P的运行队列中偷取Goroutine。如果上一步也没有，则调用findrunnable从其他工作线程的运行队列中偷取Goroutine，在偷取之前,findrunnable函数会再次尝试从全局运行G队列和当前P运行G队列中查找需要运行的Goroutine。 全局运行队列中获取goroutine # // Try get a batch of G\u0026#39;s from the global runnable queue. // Sched must be locked. func globrunqget(_p_ *p, max int32) *g { if sched.runqsize == 0 { //如果全局的G队列为空,直接返回nil return nil } /* - 如果gomaxprocs==1;sched.runqsize==1; - 导致(n==2)\u0026gt;sched.runqsize[全局的队列长度1]; - 需要判断下，取两者少的数; */ n := sched.runqsize/gomaxprocs + 1 //按照P的数量平分全局队列 if n \u0026gt; sched.runqsize { // n = sched.runqsize //取两者少的数 } if max \u0026gt; 0 \u0026amp;\u0026amp; n \u0026gt; max { n = max //取两者少的数 } if n \u0026gt; int32(len(_p_.runq))/2 { //取本地队列的一半长最多 n = int32(len(_p_.runq)) / 2 } sched.runqsize -= n gp := sched.runq.pop() //返回第一个,其他放入本地队列 n-- for ; n \u0026gt; 0; n-- { gp1 := sched.runq.pop() runqput(_p_, gp1, false) //-------------------------here /* 这里如果put G到本地满了,它又会put到全局. If the run queue is full, runnext puts g on the global queue. */ } return gp } 从上面的注释很容易看懂,只有一个需要注意,-------------------------here\n取本地队列的一半长最多 如果本身本地队列就是快满了,后面for循环继续加就会导致本地队列满了; 所以runqput函数又会把Goroutine加到全局里面去. runqput # /* - runqput尝试将g放在本地可运行队列中。 1. \u0026gt;如果 next 为 false，runqput 将 g 加到可运行队列的尾部。 2. 如果 next 为真，runqput 将 g 放在 _p_.runnext 槽中。 3. 如果运行队列满了，runnext就把g放到全局队列中。 */ func runqput(_p_ *p, gp *g, next bool) { if randomizeScheduler \u0026amp;\u0026amp; next \u0026amp;\u0026amp; fastrand()%2 == 0 { next = false } if next { //第二步所说的 retryNext: oldnext := _p_.runnext if !_p_.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) { goto retryNext } if oldnext == 0 { return } // Kick the old runnext out to the regular run queue. gp = oldnext.ptr() } retry: h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with consumers t := _p_.runqtail if t-h \u0026lt; uint32(len(_p_.runq)) { _p_.runq[t%uint32(len(_p_.runq))].set(gp) atomic.StoreRel(\u0026amp;_p_.runqtail, t+1) // store-release, makes the item available for consumption return } if runqputslow(_p_, gp, h, t) { return } // the queue is not full, now the put above must succeed goto retry } 本地运行队列(当前线程)中获取Goroutine # 本地运行队列其实分为两个部分: 一部分是由P的runq、runqhead和runqtail这三个成员组成的一个无锁循环队列，该队列最多可包含256个Goroutine; 另一部分是P的runnext成员，它是一个指向g结构体对象的指针，它最多只包含一个Goroutine. type p struct{ //... runqhead uint32 runqtail uint32 runq [256]guintptr // runnext, if non-nil, is a runnable G that was ready\u0026#39;d by // the current G and should be run next instead of what\u0026#39;s in // runq if there\u0026#39;s time remaining in the running G\u0026#39;s time // slice. It will inherit the time left in the current time // slice. If a set of goroutines is locked in a // communicate-and-wait pattern, this schedules that set as a // unit and eliminates the (potentially large) scheduling // latency that otherwise arises from adding the ready\u0026#39;d // goroutines to the end of the run queue. runnext guintptr //... } runqget # 实现是由runqget函数完成的: 首先查看runnext成员是否为空，如果不为空则返回runnext所指的Goroutine，并把runnext成员清零; 如果runnext为空，则继续从本地循环队列中查找Goroutine. // Get g from local runnable queue. // If inheritTime is true, gp should inherit the remaining time in the // current time slice. Otherwise, it should start a new time slice. // Executed only by the owner P. func runqget(_p_ *p) (gp *g, inheritTime bool) { // If there\u0026#39;s a runnext, it\u0026#39;s the next G to run. for { next := _p_.runnext if next == 0 { // runnext是空的,break for loop,然后从队列里面拿 break } if _p_.runnext.cas(next, 0) { return next.ptr(), true } } for { h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with other consumers t := _p_.runqtail if t == h { // 如果头等于尾，证明是队列是空的 return nil, false } gp := _p_.runq[h%uint32(len(_p_.runq))].ptr() if atomic.CasRel(\u0026amp;_p_.runqhead, h, h+1) { // cas-release, commits consume return gp, false } } } P的本地队列是怎么构成的 # 从上面已经知道本地运行G队列是由这三个field组成的，那么他们是怎么组织的？\ntype p struct{ //... runqhead uint32 runqtail uint32 runq [256]guintptr // runnext, if non-nil, is a runnable G that was ready\u0026#39;d by // the current G and should be run next instead of what\u0026#39;s in // runq if there\u0026#39;s time remaining in the running G\u0026#39;s time // slice. It will inherit the time left in the current time // slice. If a set of goroutines is locked in a // communicate-and-wait pattern, this schedules that set as a // unit and eliminates the (potentially large) scheduling // latency that otherwise arises from adding the ready\u0026#39;d // goroutines to the end of the run queue. runnext guintptr //... } 通过这两个函数:runqget与runqput,来看下它们是怎么运作的,我们先列出入队和出队的代码,来看一下。\n/* ----------------here: 这里是入队，把Goroutine放入本地运行队列 */ h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with consumers t := _p_.runqtail if t-h \u0026lt; uint32(len(_p_.runq)) { _p_.runq[t%uint32(len(_p_.runq))].set(gp) atomic.StoreRel(\u0026amp;_p_.runqtail, t+1) // store-release, makes the item available for consumption,runqhead不动,runqtail加一. return } //---------------------------------------------------------------------------------------------------------- for { h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with other consumers t := _p_.runqtail if t == h { // 如果头等于尾，证明是队列是空的 return nil, false } gp := _p_.runq[h%uint32(len(_p_.runq))].ptr() if atomic.CasRel(\u0026amp;_p_.runqhead, h, h+1) { // cas-release, commits consume,runqhead加一,runqtail不动. return gp, false } } 入队 runqhead不动,runqtail加一 出队 runqhead加一,runqtail不动. 为什么要用len # 它的那个gp := _p_.runq[h%uint32(len(_p_.runq))].ptr()为什么不直接用256，而需要用到len(_p_.runq),如果是为了以后的扩展，那么可以定义一个常量来替代就行了.\npackage main import ( \u0026#34;fmt\u0026#34; ) const lenghtForQueue = 256 var lenghtForQueueVar = 256 func main() { //var runq [lenghtForQueue]uintptr var runq [lenghtForQueueVar]uintptr fmt.Println(len(runq)) } 不能使用变量,只能使用常量：gdb/main.go:23:15: non-constant array bound lenghtForQueueVar.\n"},{"id":8,"href":"/docs/%E7%AC%AC%E4%B8%89%E7%AB%A0-%E9%80%80%E5%87%BAgoroutine/3.1-%E9%80%80%E5%87%BAgoroutine/","title":"3.1 goroutine退出过程","section":"第三章 退出goroutine","content":" 退出 # 非main goroutine 最后会运行goexit() main goroutine 虽然也定义了goexit(),模拟好像是是goexit函数调用的,但是这个main goroutine在src/runtime/proc.go文件里,这个函数返回到上一层. exit(0),就会退出. 如果上面没有退出,下面的for循环会再次保证\u0008程序将不会到上一层再继续执行. // The main goroutine. func main() { //... exit(0) for { var x *int32 *x = 0 } } 例子 # 我们首先来gdb调试一下这个程序\nmain.go\npackage main import \u0026#34;time\u0026#34; // the function\u0026#39;s body is empty func add(x, y int64) int64 func main() { go add(2, 3) time.Sleep(time.Minute) } add_amd.s\nTEXT ·add(SB),$0-24 MOVQ x+0(FP), BX MOVQ y+8(FP), BP ADDQ BP, BX MOVQ BX, ret+16(FP) RET 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n非main goroutine退出 # gdb断点\n//list /tmp/kubernets/main.go:3 list /tmp/kubernets/add_amd.s:3 list /usr/lib/golang/src/runtime/asm_amd64.s:1356 (gdb) disas Dump of assembler code for function main.add: 0x0000000000456b40 \u0026lt;+0\u0026gt;:\tmov 0x8(%rsp),%rbx =\u0026gt; 0x0000000000456b45 \u0026lt;+5\u0026gt;:\tmov 0x10(%rsp),%rbp 0x0000000000456b4a \u0026lt;+10\u0026gt;:\tadd %rbp,%rbx 0x0000000000456b4d \u0026lt;+13\u0026gt;:\tmov %rbx,0x18(%rsp) 0x0000000000456b52 \u0026lt;+18\u0026gt;:\tretq End of assembler dump. (gdb) step 4\tADDQ BP, BX (gdb) step 5\tMOVQ BX, ret+16(FP) (gdb) disas Dump of assembler code for function main.add: 0x0000000000456b40 \u0026lt;+0\u0026gt;:\tmov 0x8(%rsp),%rbx 0x0000000000456b45 \u0026lt;+5\u0026gt;:\tmov 0x10(%rsp),%rbp 0x0000000000456b4a \u0026lt;+10\u0026gt;:\tadd %rbp,%rbx 0x0000000000456b4d \u0026lt;+13\u0026gt;:\tmov %rbx,0x18(%rsp) =\u0026gt; 0x0000000000456b52 \u0026lt;+18\u0026gt;:\tretq End of assembler dump. (gdb) step Single stepping until exit from function main.add, which has no line number information. Breakpoint 1, runtime.goexit () at /usr/lib/golang/src/runtime/asm_amd64.s:1358 1358\tCALL\truntime·goexit1(SB)\t// does not return (gdb) list 1353 1354\t// The top-most function running on a goroutine 1355\t// returns to goexit+PCQuantum. 1356\tTEXT runtime·goexit(SB),NOSPLIT,$0-0 1357\tBYTE\t$0x90\t// NOP 1358\tCALL\truntime·goexit1(SB)\t// does not return 1359\t// traceback from goexit1 must hit code range of goexit 1360\tBYTE\t$0x90\t// NOP 1361 1362\t// This is called from .init_array and follows the platform, not Go, ABI. (gdb) (gdb) step runtime.goexit1 () at /usr/lib/golang/src/runtime/proc.go:2663 2663\tfunc goexit1() { 2670\tmcall(goexit0) // goexist1()--\u0026gt;mcall(goexit0) (gdb) step runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:294 294\tMOVQ\tfn+0(FP), DI (gdb) list 289\t// func mcall(fn func(*g)) 290\t// Switch to m-\u0026gt;g0\u0026#39;s stack, call fn(g). 291\t// Fn must never return. It should gogo(\u0026amp;g-\u0026gt;sched) 292\t// to keep running g. 293\tTEXT runtime·mcall(SB), NOSPLIT, $0-8 294\tMOVQ\tfn+0(FP), DI 295 296\tget_tls(CX) 297\tMOVQ\tg(CX), AX\t// save state in g-\u0026gt;sched 298\tMOVQ\t0(SP), BX\t// caller\u0026#39;s PC (gdb) 可以看到从add函数返回后,跳转到了\n第一步 第二步 第三步 src/runtime/asm_amd64.s src/runtime/proc.go src/runtime/asm_amd64.s runtime.goexit () goexit1() runtime.mcall() // The top-most function running on a goroutine // returns to goexit+PCQuantum. TEXT runtime·goexit(SB),NOSPLIT,$0-0 BYTE\t$0x90\t// NOP CALL\truntime·goexit1(SB)\t// does not return // traceback from goexit1 must hit code range of goexit BYTE\t$0x90\t// NOP // Finishes execution of the current goroutine. func goexit1() { if raceenabled { //忽略 racegoend() } if trace.enabled { //忽略 traceGoEnd() } mcall(goexit0) } // func mcall(fn func(*g)) // Switch to m-\u0026gt;g0\u0026#39;s stack, call fn(g). // Fn must never return. It should gogo(\u0026amp;g-\u0026gt;sched) // to keep running g. TEXT runtime·mcall(SB), NOSPLIT, $0-8 MOVQ\tfn+0(FP), DI //参数 get_tls(CX) MOVQ\tg(CX), AX // save state in gN-\u0026gt;sched MOVQ\t0(SP), BX // caller\u0026#39;s PC --\u0026gt;看下方的图 MOVQ\tBX, (g_sched+gobuf_pc)(AX) //保存caller\u0026#39;s pc到正在运行的gN.sched.pc LEAQ\tfn+0(FP), BX // caller\u0026#39;s SP MOVQ\tBX, (g_sched+gobuf_sp)(AX) //保存caller\u0026#39;s sp到正在运行的gN.sched.sp MOVQ\tAX, (g_sched+gobuf_g)(AX) //保存gN到正在运行的gN.sched.g MOVQ\tBP, (g_sched+gobuf_bp)(AX) //保存bp到正在运行的gN.sched.bp // switch to m-\u0026gt;g0 \u0026amp; its stack, call fn MOVQ\tg(CX), BX // bx=gN MOVQ\tg_m(BX), BX // bx=gN.m MOVQ\tm_g0(BX), SI // si=gN.m.g0 CMPQ\tSI, AX // if g == m-\u0026gt;g0 call badmcall; 这个gN不能等于g0, g0应该是用户调度用的. JNE\t3(PC) MOVQ\t$runtime·badmcall(SB), AX JMP\tAX MOVQ\tSI, g(CX)\t// g = m-\u0026gt;g0; 就是把m.tls[0](TLS)的值从gN的地址换为g0的地址,这样线程通过fs寄存器能找到g0继而找到m -----------here MOVQ\t(g_sched+gobuf_sp)(SI), SP // sp = m-\u0026gt;g0-\u0026gt;sched.sp,把g0的寄存器SP恢复到真实的SP ---------------here PUSHQ\tAX //gN压栈,作为后面call的参数 MOVQ\tDI, DX //dx = di(fn函数结构体) MOVQ\t0(DI), DI //所以这里是取真正的fn CALL\tDI //开始调用fn POPQ\tAX MOVQ\t$runtime·badmcall2(SB), AX JMP\tAX RET 分析mcall函数 # 我们主要来看下mcall()函数, 看下这个mcall函数的参数,函数指针\nMOVQ DI, DX //dx = di(fn函数) MOVQ 0(DI), DI //所以这里是取真正的fn 函数变量并不是一个直接指向函数代码的指针，而是一个指向funcval结构体对象的指针，funcval结构体对象的第一个成员fn才是真正指向函数代码的指针.\ntype funcval struct { fn uintptr // variable-size, fn-specific data here } 总结mcall\n保存当前g的调度信息,寄存器保存到g.sched; 把g0设置到tls中，修改CPU的rsp寄存器使其指向g0的栈; 以当前运行的g(我们这个场景是gN)为参数调用fn函数(此处为goexit0). mcall和gogo的代码非常相似，然而mcall和gogo在做切换时有个重要的区别: 都要切栈\ngogo函数在从g0切换到其它goroutine时首先切换了栈，然后通过跳转指令从runtime代码切换到了用户goroutine的代码; 而mcall函数在从其它goroutine切换回g0时只切换了栈，并未使用跳转指令跳转到runtime代码去执行. 为什么会有这个差别呢？\n原因在于在从g0切换到其它goroutine之前执行的是runtime的代码而且使用的是g0栈，所以切换时需要首先切换栈然后再从runtime代码跳转某个goroutine的代码去执行; (切换栈和跳转指令不能颠倒，因为跳转之后执行的就是用户的goroutine代码了，没有机会切换栈了); 然而从某个goroutine切换回g0时，goroutine使用的是call指令来调用mcall函数，mcall函数本身就是runtime的代码; 所以call指令其实已经完成了从goroutine代码到runtime代码的跳转，因此mcall函数自身的代码就不需要再跳转了，只需要把栈切换到g0栈即可. goexit0函数 # // goexit continuation on g0. func goexit0(gp *g) { _g_ := getg() // g0 casgstatus(gp, _Grunning, _Gdead) // 修改gN的状态 if isSystemGoroutine(gp, false) { atomic.Xadd(\u0026amp;sched.ngsys, -1) } gp.m = nil locked := gp.lockedm != 0 gp.lockedm = 0 _g_.m.lockedg = 0 gp.paniconfault = false gp._defer = nil // should be true already but just in case. gp._panic = nil // non-nil for Goexit during panic. points at stack-allocated data. gp.writebuf = nil gp.waitreason = 0 gp.param = nil gp.labels = nil gp.timer = nil //... // Note that gp\u0026#39;s stack scan is now \u0026#34;valid\u0026#34; because it has no // stack. gp.gcscanvalid = true dropg() //dropg函数解除g和m之间的关系，其实就是设置g-\u0026gt;m = nil, m-\u0026gt;currg = nil. //... gfput(_g_.m.p.ptr(), gp) //放在gfree列表中,如果本地列表太长，则将一个批次转移到全局列表中. //... schedule() } 退出流程 # goexit() goexit1() mcall() goexit0() schedule()\n"},{"id":9,"href":"/docs/%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E8%BF%9B%E5%85%A5main%E5%87%BD%E6%95%B0%E4%B9%8B%E5%89%8D%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96/2.1-%E8%BF%9B%E5%85%A5main%E5%87%BD%E6%95%B0%E4%B9%8B%E5%89%8D%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96/","title":"2.1 进入main函数之前的初始化","section":"第二章 进入main函数之前的初始化","content":" 例子 # 我们首先来gdb调试一下这个程序\nmain.go\npackage main import \u0026#34;fmt\u0026#34; // the function\u0026#39;s body is empty func add(x, y int64) int64 func main() { gg:=add(2, 3) fmt.Println(gg) } add_amd.s\nTEXT ·add(SB),$0-24 MOVQ x+0(FP), BX MOVQ y+8(FP), BP ADDQ BP, BX MOVQ BX, ret+16(FP) RET 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n程序加载到内存入口 # (gdb) info files Symbols from \u0026#34;/tmp/kubernets/test\u0026#34;. Local exec file: `/tmp/kubernets/test\u0026#39;, file type elf64-x86-64. Entry point: 0x454e00 0x0000000000401000 - 0x000000000048cfd3 is .text 0x000000000048d000 - 0x00000000004dc550 is .rodata 0x00000000004dc720 - 0x00000000004dd38c is .typelink 0x00000000004dd390 - 0x00000000004dd3e0 is .itablink 0x00000000004dd3e0 - 0x00000000004dd3e0 is .gosymtab 0x00000000004dd3e0 - 0x0000000000548adf is .gopclntab 0x0000000000549000 - 0x0000000000549020 is .go.buildinfo 0x0000000000549020 - 0x0000000000556118 is .noptrdata 0x0000000000556120 - 0x000000000055d110 is .data 0x000000000055d120 - 0x0000000000578990 is .bss 0x00000000005789a0 - 0x000000000057b108 is .noptrbss 0x0000000000400f9c - 0x0000000000401000 is .note.go.buildid (gdb) b *0x454e00 Breakpoint 1 at 0x454e00: file /usr/lib/golang/src/runtime/rt0_linux_amd64.s, line 8. //跳到这个文件来了 (gdb) list /usr/lib/golang/src/runtime/rt0_linux_amd64.s:1 1\t// Copyright 2009 The Go Authors. All rights reserved. 2\t// Use of this source code is governed by a BSD-style 3\t// license that can be found in the LICENSE file. 4 5\t#include \u0026#34;textflag.h\u0026#34; 6 7\tTEXT _rt0_amd64_linux(SB),NOSPLIT,$-8 8\tJMP\t_rt0_amd64(SB) (gdb) run Starting program: /tmp/kubernets/test Breakpoint 1, _rt0_amd64_linux () at /usr/lib/golang/src/runtime/rt0_linux_amd64.s:8 //跳到这个文件来了 8\tJMP\t_rt0_amd64(SB) (gdb) info registers bp bp 0x0\t0 (gdb) info registers sp sp 0x7fffffffe4d0\t0x7fffffffe4d0 (gdb) next 16\tLEAQ\t8(SP), SI\t// argv (gdb) list 11\t// internal linking. This is the entry point for the program from the 12\t// kernel for an ordinary -buildmode=exe program. The stack holds the 13\t// number of arguments and the C-style argv. 14\tTEXT _rt0_amd64(SB),NOSPLIT,$-8 15\tMOVQ\t0(SP), DI\t// argc 16\tLEAQ\t8(SP), SI\t// argv 17\tJMP\truntime·rt0_go(SB) (gdb) next runtime.rt0_go () at /usr/lib/golang/src/runtime/asm_amd64.s:89 //跳到这个文件来了 89\tMOVQ\tDI, AX\t// argc (gdb) list 84\tDATA _rt0_amd64_lib_argv\u0026lt;\u0026gt;(SB)/8, $0 85\tGLOBL _rt0_amd64_lib_argv\u0026lt;\u0026gt;(SB),NOPTR, $8 86 87\tTEXT runtime·rt0_go(SB),NOSPLIT,$0 88\t// copy arguments forward on an even stack 89\tMOVQ\tDI, AX\t// argc 90\tMOVQ\tSI, BX\t// argv 91\tSUBQ\t$(4*8+7), SP\t// 2args 2auto 92\tANDQ\t$~15, SP 93\tMOVQ\tAX, 16(SP) (gdb) 从上面的调试来看,最终到到达了src/runtime/asm_amd64.s:89的 runtime.rt0_go函数.\nfile /usr/lib/golang/src/runtime/rt0_linux_amd64.s, line 8. //跳到这个文件来 _rt0_amd64_linux () at /usr/lib/golang/src/runtime/rt0_linux_amd64.s:8 //跳到这个文件来 runtime.rt0_go () at /usr/lib/golang/src/runtime/asm_amd64.s:89 //跳到这个文件来 进行初始化全局变量 # 主要是rt0_go()函数\n初始化全局变量g0 # TEXT runtime·rt0_go(SB),NOSPLIT,$0 // copy arguments forward on an even stack MOVQ\tDI, AX\t// argc MOVQ\tSI, BX\t// argv SUBQ\t$(4*8+7), SP\t// 2args 2auto ANDQ\t$~15, SP MOVQ\tAX, 16(SP) MOVQ\tBX, 24(SP) //TODO zxc 调整栈顶寄存器使其按16字节对齐; argc放在SP + 16字节处; argv放在SP + 24字节处 // create istack out of the given (operating system) stack. // _cgo_init may update stackguard. MOVQ\t$runtime·g0(SB), DI //全局的变量g0的地址放入DI寄存器 LEAQ\t(-64*1024+104)(SP), BX MOVQ\tBX, g_stackguard0(DI) MOVQ\tBX, g_stackguard1(DI) MOVQ\tBX, (g_stack+stack_lo)(DI) MOVQ\tSP, (g_stack+stack_hi)(DI) // TODO zxc 始初始化全局变量g0, g0的栈大约有64K，地址范围为 SP - 64*1024 + 104 ～ SP /* ...省略一些CPU相关的汇编 */ LEAQ\truntime·m0+m_tls(SB), DI // //DI = \u0026amp;m0.tls，取m0的tls成员的地址到DI寄存器 CALL\truntime·settls(SB) // 调用settls设置线程本地存储，settls函数的参数在DI寄存器中 // store through it, to make sure it works // 可以看做是单元测试 get_tls(BX) MOVQ\t$0x123, g(BX) MOVQ\truntime·m0+m_tls(SB), AX CMPQ\tAX, $0x123 JEQ 2(PC) CALL\truntime·abort(SB) 分析下这个runtime·settls()函数,因为这个ELF需要-8,所以提前+8,那么最终执行完这个函数,它是设置在全局的m0的tls[0]里面,还是tls[1]里面呢? 观察下这个MOVQ\truntime·m0+m_tls(SB), AX,可以知道就是m0中的tls[0]的地址\n// set tls base to DI TEXT runtime·settls(SB),NOSPLIT,$32 ADDQ\t$8, DI\t// ELF wants to use -8(FS) //因为后面要-8,所以先加+8 MOVQ\tDI, SI // is SI parameter? MOVQ\t$0x1002, DI\t// ARCH_SET_FS MOVQ\t$SYS_arch_prctl, AX SYSCALL CMPQ\tAX, $0xfffffffffffff001 JLS\t2(PC) MOVL\t$0xf1, 0xf1 // crash RET 只是一个宏定义src/runtime/go_tls.h,get_tls(BX)把TLS地址放入BX寄存器.\n#ifdef GOARCH_amd64 #define\tget_tls(r)\tMOVQ TLS, r #define\tg(r)\t0(r)(TLS*1) #endif // Thread-local storage references use the TLS pseudo-register. // As a register, TLS refers to the thread-local storage base, and it // can only be loaded into another register: // // MOVQ TLS, AX // // An offset from the thread-local storage base is written off(reg)(TLS*1). // Semantically it is off(reg), but the (TLS*1) annotation marks this as // indexing from the loaded TLS base. This emits a relocation so that // if the linker needs to adjust the offset, it can. For example: // // MOVQ TLS, AX // MOVQ 0(AX)(TLS*1), CX // load g into CX // // On systems that support direct access to the TLS memory, this // pair of instructions can be reduced to a direct TLS memory reference: // // MOVQ 0(TLS), CX // load g into CX // // The 2-instruction and 1-instruction forms correspond to the two code // sequences for loading a TLS variable in the local exec model given in \u0026#34;ELF // Handling For Thread-Local Storage\u0026#34;. // When building for inclusion into a shared library, an instruction of the form // MOV off(CX)(TLS*1), AX // becomes // mov %fs:off(%rcx), %rax // which assumes that the correct TLS offset has been loaded into %rcx (today // there is only one TLS variable -- g -- so this is OK). When not building for // a shared library the instruction does not require a prefix. m0和g0绑定在一起 # 我们继续下面的runtime·rt0_go函数.\nok: // set the per-goroutine and per-mach \u0026#34;registers\u0026#34; get_tls(BX) LEAQ\truntime·g0(SB), CX //取的g0的在堆上的地址放入CX--\u0026gt;CX = g0的地址 MOVQ\tCX, g(BX) //把g0的地址保存在线程本地存储里面，也就是*TLS=\u0026amp;g0 --\u0026gt; m0.tls[0]=\u0026amp;g0 LEAQ\truntime·m0(SB), AX //AX=\u0026amp;m0 // save m-\u0026gt;g0 = g0 MOVQ\tCX, m_g0(AX) // save m0 to g0-\u0026gt;m MOVQ\tAX, g_m(CX) m0和g0绑定在一起，这样，之后在主线程中通过get_tls可以获取到g0，通过g0的m成员又可以找到m0，于是这里就实现了m0和g0与主线程之间的关联。 从这里还可以看到，保存在主线程本地存储中的值是g0的地址，也就是说工作线程的私有全局变量其实是一个指向g的指针而不是指向m的指针\n设置一些ncpu(CPU核数), M,P的个数, 初始化m0 # 我们继续下面的runtime·rt0_go函数.\nCLD\t// convention is D is always left cleared 不需要关心 CALL\truntime·check(SB) //不需要关心 MOVL\t16(SP), AX\t// copy argc; AX = argc MOVL\tAX, 0(SP) // argc放在栈顶 MOVQ\t24(SP), AX\t// copy argv; AX = argv MOVQ\tAX, 8(SP) // argv放在SP + 8的位置 CALL\truntime·args(SB) CALL\truntime·osinit(SB) //全局变量 ncpu = CPU核数 CALL\truntime·schedinit(SB) schedinit # schedinit主要是进行一些初始化工作, 必过设置M的最大个数,P的个数,还有初始化这m0,另外这个getg()也很有趣,分为our G, system G\nsr/runtime/proc.go\n// The bootstrap sequence is: // //\tcall osinit //\tcall schedinit //\tmake \u0026amp; queue new G //\tcall runtime·mstart // // The new G calls runtime·main. func schedinit() { // raceinit must be the first call to race detector. // In particular, it must be done before mallocinit below calls racemapshadow. _g_ := getg() /* getg函数在源代码中没有对应的定义，由编译器插入类似下面两行代码 get_tls(CX) // CX=TLS MOVQ g(CX), BX; BX存器里面现在放的是当前g结构体对象的地址 可以参考这个,根据https://github.com/golang/go/blob/d029058b5912312963225c40ae4bf44e3cb4be76/src/runtime/HACKING.md */ //... sched.maxmcount = 10000 // 最多启动10000个操作系统线程，也是最多10000个M //... mcommoninit(_g_.m) //初始化m0，因为从前面的代码我们知道g0-\u0026gt;m = \u0026amp;m0 //... procs := ncpu //系统中有多少核，就创建和初始化多少个p结构体对象 if n, ok := atoi32(gogetenv(\u0026#34;GOMAXPROCS\u0026#34;)); ok \u0026amp;\u0026amp; n \u0026gt; 0 { procs = n //如果指定了GOMAXPROCS大于0，则创建指定数量的p } if procresize(procs) != nil { throw(\u0026#34;unknown runnable goroutine during bootstrap\u0026#34;) } //... } mcommoninit # mcommoninit主要是判断m是否超过个数,然后放入全局m链表,防止垃圾回收\nm0.p = allp[0] allp[0].m = \u0026amp;m0 sr/runtime/proc.go\nfunc mcommoninit(mp *m) { _g_ := getg() // g0 stack won\u0026#39;t make sense for user (and is not necessary unwindable). if _g_ != _g_.m.g0 { callers(1, mp.createstack[:]) } lock(\u0026amp;sched.lock) if sched.mnext+1 \u0026lt; sched.mnext { throw(\u0026#34;runtime: thread ID overflow\u0026#34;) } mp.id = sched.mnext sched.mnext++ checkmcount() //检查m的个数是否超过sched.maxmcoun //... // Add to allm so garbage collector doesn\u0026#39;t free g-\u0026gt;m // when it is just in a register or thread-local storage. mp.alllink = allm //m放入全局链表allm, 防止垃圾回收 // NumCgoCall() iterates over allm w/o schedlock, // so we need to publish it safely. atomicstorep(unsafe.Pointer(\u0026amp;allm), unsafe.Pointer(mp)) unlock(\u0026amp;sched.lock) //... } 初始化allp, m0和allp[0]绑定 # procresize函数，我们先看下初始化时会执行的代码路径.\n考虑到初始化完成之后, 用户代码还可以通过GOMAXPROCS()函数调用它重新创建和初始化p结构体对象，我们首先只考虑初始化.\n使用make([]*p, nprocs)初始化全局变量allp，把旧的allp拷贝到新的nallp里面,然后allp = make([]*p, nprocs) 循环allp中的每个p,如果是nill的就进行创建, 然后每个都初始化,不管是新的还是旧的. 把m0和allp[0]绑定在一起，即m0.p = allp[0], allp[0].m = m0 把除了allp[0]之外的所有p放入到全局变量sched的pidle空闲队列之中 src/runtime/proc.go\nfunc procresize(nprocs int32) *p { old := gomaxprocs //系统初始化时 gomaxprocs = 0 if old \u0026lt; 0 || nprocs \u0026lt;= 0 { throw(\u0026#34;procresize: invalid arg\u0026#34;) } //... // Grow allp if necessary. if nprocs \u0026gt; int32(len(allp)) { //初始化时 len(allp) == 0 // Synchronize with retake, which could be running // concurrently since it doesn\u0026#39;t run on a P. lock(\u0026amp;allpLock) if nprocs \u0026lt;= int32(cap(allp)) { allp = allp[:nprocs] } else { //初始化时进入此分支，创建allp 切片 nallp := make([]*p, nprocs) // Copy everything up to allp\u0026#39;s cap so we // never lose old allocated Ps. copy(nallp, allp[:cap(allp)]) //把旧的allp拷贝到新的nallp里面 allp = nallp } unlock(\u0026amp;allpLock) } // initialize new P\u0026#39;s for i := old; i \u0026lt; nprocs; i++ { pp := allp[i] if pp == nil { pp = new(p) } pp.init(i) //每个p然后进行初始化 旧的也需要重新进行初始化 atomicstorep(unsafe.Pointer(\u0026amp;allp[i]), unsafe.Pointer(pp)) } //... _g_ := getg() // _g_ = g0 if _g_.m.p != 0 \u0026amp;\u0026amp; _g_.m.p.ptr().id \u0026lt; nprocs {//初始化时m0-\u0026gt;p还未初始化，所以不会执行这个分支 // continue to use the current P _g_.m.p.ptr().status = _Prunning _g_.m.p.ptr().mcache.prepareForSweep() } else {//初始化时执行这个分支 // release the current P and acquire allp[0] if _g_.m.p != 0 {//初始化时这里不执行 _g_.m.p.ptr().m = 0 } _g_.m.p = 0 _g_.m.mcache = nil p := allp[0] p.m = 0 p.status = _Pidle acquirep(p) // --\u0026gt; Associate p and the current m; 在初始化的时候就是把p和m0关联起来,其实是这两个strct的成员相互赋值 if trace.enabled { traceGoStart() } } // release resources from unused P\u0026#39;s --\u0026gt; zxc 就是释放多余的P for i := nprocs; i \u0026lt; old; i++ { p := allp[i] p.destroy() // TODO zxc can\u0026#39;t free P itself because it can be referenced by an M in syscall } //下面这个for 循环把所有空闲的p放入空闲链表 var runnablePs *p for i := nprocs - 1; i \u0026gt;= 0; i-- { p := allp[i] if _g_.m.p.ptr() == p {//不放allp[0],因为它跟m0在上面已经关联了 continue } p.status = _Pidle if runqempty(p) {//初始化时除了allp[0]其它p全部执行这个分支，放入空闲链表 pidleput(p) } else { //... } } //... return runnablePs } 创造一个将要运行runtime·mainPC的goroutine # 我们继续下面的 runtime·rt0_go函数.\n// create a new goroutine to start program MOVQ\t$runtime·mainPC(SB), AX// entry PUSHQ\tAX PUSHQ\t$0// arg size CALL\truntime·newproc(SB) POPQ\tAX newproc # src/runtime/proc.go\n// Create a new g running fn with siz bytes of arguments. // Put it on the queue of g\u0026#39;s waiting to run. // The compiler turns a go statement into a call to this. // Cannot split the stack because it assumes that the arguments // are available sequentially after \u0026amp;fn; they would not be // copied if a stack split occurred. //go:nosplit func newproc(siz int32, fn *funcval) { argp := add(unsafe.Pointer(\u0026amp;fn), sys.PtrSize) gp := getg() pc := getcallerpc() //getcallerpc:返回其调用者的程序计数器（PC）---getcallersp返回其调用者的堆栈指针（SP） systemstack(func() { // systemstack 切到g0的栈和CPU寄存器去执行, 执行完了, 再切换原来的栈和CPU寄存器 newproc1(fn, (*uint8)(argp), siz, gp, pc) /* 创建一个新的g运行的fn，参数的字节数从argp开始; siz: 参数的大小 callergp: 调用者的goroutine pointer[caller-callee] callerpc: 调用者的程序计数器 */ }) } // Create a new g running fn with narg bytes of arguments starting // at argp. callerpc is the address of the go statement that created // this. The new g is put on the queue of g\u0026#39;s waiting to run. func newproc1(fn *funcval, argp *uint8, narg int32, callergp *g, callerpc uintptr) { _g_ := getg() if fn == nil { _g_.m.throwing = -1 // do not dump full stacks throw(\u0026#34;go of nil func value\u0026#34;) } acquirem() // disable preemption because it can be holding p in a local var // TODO zxc 禁止抢占 siz := narg siz = (siz + 7) \u0026amp;^ 7 // We could allocate a larger initial stack if necessary. // Not worth it: this is almost always an error. // 4*sizeof(uintreg): extra space added below // sizeof(uintreg): caller\u0026#39;s LR (arm) or return address (x86, in gostartcall). if siz \u0026gt;= _StackMin-4*sys.RegSize-sys.RegSize { throw(\u0026#34;newproc: function arguments too large for new goroutine\u0026#34;) } _p_ := _g_.m.p.ptr() // 的到m上的p,在这里就是allp[0] newg := gfget(_p_) // 从本地或者全局队列尝试获取g if newg == nil {// 如果获取失败就构造一个g newg = malg(_StackMin) // 跳转到g0的栈,然后再堆上分配, casgstatus(newg, _Gidle, _Gdead) //_Gidle=iota //0 新创建的默认是0,所以就是idle;修改状态为Gdead,这样gc就是扫描newg上面的栈,来尝试要不要回收. allgadd(newg) // publishes with a g-\u0026gt;status of Gdead so GC scanner doesn\u0026#39;t look at uninitialized stack. 这里的把生成的g放入全局allgs []*g//保存所有的g } if newg.stack.hi == 0 { throw(\u0026#34;newproc1: newg missing stack\u0026#34;) } if readgstatus(newg) != _Gdead { //不是Gdead就panic throw(\u0026#34;newproc1: new g is not Gdead\u0026#34;) } totalSize := 4*sys.RegSize + uintptr(siz) + sys.MinFrameSize // extra space in case of reads slightly beyond frame totalSize += -totalSize \u0026amp; (sys.SpAlign - 1) // align to spAlign sp := newg.stack.hi - totalSize spArg := sp if usesLR { // caller\u0026#39;s LR *(*uintptr)(unsafe.Pointer(sp)) = 0 prepGoExitFrame(sp) spArg += sys.MinFrameSize } if narg \u0026gt; 0 { //memmove copies n bytes from \u0026#34;from\u0026#34; to \u0026#34;to\u0026#34;,把在调用者的栈上的,callee memmove(unsafe.Pointer(spArg), unsafe.Pointer(argp), uintptr(narg)) // This is a stack-to-stack copy. If write barriers // are enabled and the source stack is grey (the // destination is always black), then perform a // barrier copy. We do this *after* the memmove // because the destination stack may have garbage on // it. if writeBarrier.needed \u0026amp;\u0026amp; !_g_.m.curg.gcscandone { f := findfunc(fn.fn) stkmap := (*stackmap)(funcdata(f, _FUNCDATA_ArgsPointerMaps)) if stkmap.nbit \u0026gt; 0 { // We\u0026#39;re in the prologue, so it\u0026#39;s always stack map index 0. bv := stackmapdata(stkmap, 0) bulkBarrierBitmap(spArg, spArg, uintptr(bv.n)*sys.PtrSize, 0, bv.bytedata) } } } memclrNoHeapPointers(unsafe.Pointer(\u0026amp;newg.sched), unsafe.Sizeof(newg.sched)) //newg.sched 空间置零 newg.sched.sp = sp newg.stktopsp = sp newg.sched.pc = funcPC(goexit) + sys.PCQuantum // +PCQuantum=1 so that previous instruction is in same function newg.sched.g = guintptr(unsafe.Pointer(newg)) gostartcallfn(\u0026amp;newg.sched, fn) /* 这个函数 1. 调整newg的栈空间，把goexit函数的第二条指令的地址入栈，伪造成goexit函数调用了fn， 从而使fn执行完成后执行ret指令时返回到goexit继续执行完成最后的清理工作； 2. 重新设置newg.buf.pc 为需要执行的函数的地址，即fn，我们这个场景为runtime.main函数的地址。 // adjust Gobuf as if it executed a call to fn // and then did an immediate gosave. func gostartcallfn(gobuf *gobuf, fv *funcval) { var fn unsafe.Pointer if fv != nil { fn = unsafe.Pointer(fv.fn) } else { fn = unsafe.Pointer(funcPC(nilfunc)) } gostartcall(gobuf, fn, unsafe.Pointer(fv)) } // adjust Gobuf as if it executed a call to fn with context ctxt // and then did an immediate gosave. func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer) { sp := buf.sp if sys.RegSize \u0026gt; sys.PtrSize { sp -= sys.PtrSize *(*uintptr)(unsafe.Pointer(sp)) = 0 } sp -= sys.PtrSize *(*uintptr)(unsafe.Pointer(sp)) = buf.pc buf.sp = sp buf.pc = uintptr(fn) buf.ctxt = ctxt } */ newg.gopc = callerpc newg.ancestors = saveAncestors(callergp) newg.startpc = fn.fn //... casgstatus(newg, _Gdead, _Grunnable) //修改goroutine状态 if _p_.goidcache == _p_.goidcacheend { // Sched.goidgen is the last allocated id, // this batch must be [sched.goidgen+1, sched.goidgen+GoidCacheBatch]. // At startup sched.goidgen=0, so main goroutine receives goid=1. _p_.goidcache = atomic.Xadd64(\u0026amp;sched.goidgen, _GoidCacheBatch) _p_.goidcache -= _GoidCacheBatch - 1 _p_.goidcacheend = _p_.goidcache + _GoidCacheBatch } newg.goid = int64(_p_.goidcache) _p_.goidcache++ if raceenabled { newg.racectx = racegostart(callerpc) } if trace.enabled { traceGoCreate(newg, newg.startpc) } runqput(_p_, newg, true) //放入本地队列， 或者全局队列 if atomic.Load(\u0026amp;sched.npidle) != 0 \u0026amp;\u0026amp; atomic.Load(\u0026amp;sched.nmspinning) == 0 \u0026amp;\u0026amp; mainStarted { wakep() } releasem(_g_.m) } //end newproc1 function rbp这个寄存器的值还有用? # rbp这个寄存器的值,会保存到新创建的goroutine里的sched.bp字段?\ntype g struct { stack stack // offset known to runtime/cgo stackguard0 uintptr // offset known to liblink stackguard1 uintptr // offset known to liblink _panic *_panic // innermost panic - offset known to liblink _defer *_defer // innermost defer m *m // current m; offset known to arm liblink sched gobuf //... } type gobuf struct { sp uintptr pc uintptr g guintptr ctxt unsafe.Pointer ret sys.Uintreg lr uintptr bp uintptr // for GOEXPERIMENT=framepointer ------------------------here } 没有保存,当重新调度这个goroutine,重新观察BP寄存器这一章中我们会看到里面是空的 进入main函数里面有rpb是否会入栈 mstart # 我们继续下面的 runtime·rt0_go函数.\n// start this M CALL\truntime·mstart(SB) CALL\truntime·abort(SB)\t// mstart should never return RET // Prevent dead-code elimination of debugCallV1, which is // intended to be called by debuggers. MOVQ\t$runtime·debugCallV1(SB), AX RET // mstart is the entry-point for new Ms. // // This must not split the stack because we may not even have stack // bounds set up yet. // // May run during STW (because it doesn\u0026#39;t have a P yet), so write // barriers are not allowed. // //go:nosplit //go:nowritebarrierrec func mstart() { _g_ := getg() // g0 osStack := _g_.stack.lo == 0 /* ? linux的话， 从runtime·rt0_go我们知道，g0的stack.lo,不是为0的. // create istack out of the given (operating system) stack. // _cgo_init may update stackguard. MOVQ\t$runtime·g0(SB), DI LEAQ\t(-64*1024+104)(SP), BX MOVQ\tBX, g_stackguard0(DI) MOVQ\tBX, g_stackguard1(DI) MOVQ\tBX, (g_stack+stack_lo)(DI) MOVQ\tSP, (g_stack+stack_hi)(DI) */ if osStack { // Initialize stack bounds from system stack. // Cgo may have left stack size in stack.hi. // minit may update the stack bounds. size := _g_.stack.hi if size == 0 { size = 8192 * sys.StackGuardMultiplier } _g_.stack.hi = uintptr(noescape(unsafe.Pointer(\u0026amp;size))) _g_.stack.lo = _g_.stack.hi - size + 1024 } // Initialize stack guard so that we can start calling regular // Go code. _g_.stackguard0 = _g_.stack.lo + _StackGuard //这个stackguard0比stack.lo大，防止越界 // This is the g0, so we can also call go:systemstack // functions, which check stackguard1. _g_.stackguard1 = _g_.stackguard0 //stackguard1的作用是栈溢出检查，如果是stackguard0与stackguard1相等，那么是不能放数据到g0栈中了吗 mstart1() // Exit this thread. if GOOS == \u0026#34;windows\u0026#34; || GOOS == \u0026#34;solaris\u0026#34; || GOOS == \u0026#34;illumos\u0026#34; || GOOS == \u0026#34;plan9\u0026#34; || GOOS == \u0026#34;darwin\u0026#34; || GOOS == \u0026#34;aix\u0026#34; { // Windows, Solaris, illumos, Darwin, AIX and Plan 9 always system-allocate // the stack, but put it in _g_.stack before mstart, // so the logic above hasn\u0026#39;t set osStack yet. osStack = true } mexit(osStack) } func mstart1() { _g_ := getg() //g0 if _g_ != _g_.m.g0 { throw(\u0026#34;bad runtime·mstart\u0026#34;) } // Record the caller for use as the top of stack in mcall and // for terminating the thread. // We\u0026#39;re never coming back to mstart1 after we call schedule, // so other calls can reuse the current frame. save(getcallerpc(), getcallersp()) //就是把pc, sp保存到自己的sched里面， 但是bp寄存器没有保存，因为会重新使用这个栈？ /* TODO zxc 这里为什么不是当前的, gdb查看下. getcallerpc()返回的是mstart调用mstart1时被call指令压栈的返回地址， bp+1 ? getcallersp()函数返回的是调用mstart1函数之前mstart函数的栈顶地址 bp ? */ asminit() minit() //初始化信号 // Install signal handlers; after minit so that minit can // prepare the thread to be able to handle the signals. // 安装信号处理程序;所以minit可以准备线程以便能够处理信号, 在minit之后执行 if _g_.m == \u0026amp;m0 { mstartm0() } if fn := _g_.m.mstartfn; fn != nil { //如果m的mstartfn不是空的，先执行它 fn() } if _g_.m != \u0026amp;m0 { // 如果m不等于m0 就是m0的话,前面m0和allp[0]已经绑定了 acquirep(_g_.m.nextp.ptr()) //TODO zxc 这个acquirep函数是把m和m.nextp进行绑定,如果到这里的时候,nextp已经状态不是空闲的状态, //那么会抛出错误? _g_.m.nextp = 0 } schedule() //进入调度 } save函数保存pc,sp到g0,方便下次再度调度运行g0 # list /usr/lib/golang/src/runtime/proc.go:1167 list /usr/lib/golang/src/runtime/proc.go:1179 list /usr/lib/golang/src/runtime/proc.go:1190\n[root@gitlab kubernets]# gdb test GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-119.el7 Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \u0026#34;show copying\u0026#34; and \u0026#34;show warranty\u0026#34; for details. This GDB was configured as \u0026#34;x86_64-redhat-linux-gnu\u0026#34;. For bug reporting instructions, please see: \u0026lt;http://www.gnu.org/software/gdb/bugs/\u0026gt;... Reading symbols from /tmp/kubernets/test...done. Loading Go Runtime support. (gdb) list /usr/lib/golang/src/runtime/proc.go:1167 1162\t// Go code. 1163\t_g_.stackguard0 = _g_.stack.lo + _StackGuard 1164\t// This is the g0, so we can also call go:systemstack 1165\t// functions, which check stackguard1. 1166\t_g_.stackguard1 = _g_.stackguard0 1167\tmstart1() 1168 1169\t// Exit this thread. 1170\tif GOOS == \u0026#34;windows\u0026#34; || GOOS == \u0026#34;solaris\u0026#34; || GOOS == \u0026#34;illumos\u0026#34; || GOOS == \u0026#34;plan9\u0026#34; || GOOS == \u0026#34;darwin\u0026#34; || GOOS == \u0026#34;aix\u0026#34; { 1171\t// Windows, Solaris, illumos, Darwin, AIX and Plan 9 always system-allocate (gdb) b 1167 Breakpoint 1 at 0x42dd09: file /usr/lib/golang/src/runtime/proc.go, line 1167. (gdb) list /usr/lib/golang/src/runtime/proc.go:1179 1174\tosStack = true 1175\t} 1176\tmexit(osStack) 1177\t} 1178 1179\tfunc mstart1() { 1180\t_g_ := getg() 1181 1182\tif _g_ != _g_.m.g0 { 1183\tthrow(\u0026#34;bad runtime·mstart\u0026#34;) (gdb) b 1179 Breakpoint 2 at 0x42dd30: file /usr/lib/golang/src/runtime/proc.go, line 1179. (gdb) list /usr/lib/golang/src/runtime/proc.go:1190 1185 1186\t// Record the caller for use as the top of stack in mcall and 1187\t// for terminating the thread. 1188\t// We\u0026#39;re never coming back to mstart1 after we call schedule, 1189\t// so other calls can reuse the current frame. 1190\tsave(getcallerpc(), getcallersp()) 1191\tasminit() 1192\tminit() 1193 1194\t// Install signal handlers; after minit so that minit can (gdb) b 1190 Breakpoint 3 at 0x42dd7f: file /usr/lib/golang/src/runtime/proc.go, line 1190. (gdb) info breakpoint Num Type Disp Enb Address What 1 breakpoint keep y 0x000000000042dd09 in runtime.mstart at /usr/lib/golang/src/runtime/proc.go:1167 2 breakpoint keep y 0x000000000042dd30 in runtime.mstart1 at /usr/lib/golang/src/runtime/proc.go:1179 3 breakpoint keep y 0x000000000042dd7f in runtime.mstart1 at /usr/lib/golang/src/runtime/proc.go:1190 (gdb) run Starting program: /tmp/kubernets/test Breakpoint 1, runtime.mstart () at /usr/lib/golang/src/runtime/proc.go:1167 1167\tmstart1() (gdb) list 1162\t// Go code. 1163\t_g_.stackguard0 = _g_.stack.lo + _StackGuard 1164\t// This is the g0, so we can also call go:systemstack 1165\t// functions, which check stackguard1. 1166\t_g_.stackguard1 = _g_.stackguard0 1167\tmstart1() 1168 1169\t// Exit this thread. 1170\tif GOOS == \u0026#34;windows\u0026#34; || GOOS == \u0026#34;solaris\u0026#34; || GOOS == \u0026#34;illumos\u0026#34; || GOOS == \u0026#34;plan9\u0026#34; || GOOS == \u0026#34;darwin\u0026#34; || GOOS == \u0026#34;aix\u0026#34; { 1171\t// Windows, Solaris, illumos, Darwin, AIX and Plan 9 always system-allocate (gdb) info register rbp rsp pc rbp 0x7fffffffe490\t0x7fffffffe490 rsp 0x7fffffffe478\t0x7fffffffe478 pc 0x42dd09\t0x42dd09 \u0026lt;runtime.mstart+105\u0026gt; (gdb) continue Continuing. Breakpoint 2, runtime.mstart1 () at /usr/lib/golang/src/runtime/proc.go:1179 1179\tfunc mstart1() { (gdb) info register rbp rsp pc rbp 0x7fffffffe490\t0x7fffffffe490 rsp 0x7fffffffe470\t0x7fffffffe470 pc 0x42dd30\t0x42dd30 \u0026lt;runtime.mstart1\u0026gt; (gdb) x/8xb 0x7fffffffe470 0x7fffffffe470:\t0x0e\t0xdd\t0x42\t0x00\t0x00\t0x00\t0x00\t0x00 (gdb) info frame 0 Stack frame at 0x7fffffffe478: rip = 0x42dd30 in runtime.mstart1 (/usr/lib/golang/src/runtime/proc.go:1179); saved rip 0x42dd0e called by frame at 0x7fffffffe4a0 source language unknown. Arglist at 0x7fffffffe468, args: Locals at 0x7fffffffe468, Previous frame\u0026#39;s sp is 0x7fffffffe478 Saved registers: rip at 0x7fffffffe470 (gdb) 反汇编看下mstart1到save函数的汇编代码 # (gdb) disass Dump of assembler code for function runtime.mstart1: =\u0026gt; 0x000000000042dd30 \u0026lt;+0\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rcx // rcx = \u0026amp;g0 0x000000000042dd39 \u0026lt;+9\u0026gt;:\tcmp 0x10(%rcx),%rsp // g0.stackguard0与rsp(0x7fffffffe470)判断是否达到栈顶部 0x000000000042dd3d \u0026lt;+13\u0026gt;:\tjbe 0x42de2e \u0026lt;runtime.mstart1+254\u0026gt; // jump below equal \u0026lt;=, 就panic 0x000000000042dd43 \u0026lt;+19\u0026gt;:\tsub $0x20,%rsp // rsp=0x7fffffffe470-0x20=0x7fffffffe450 0x000000000042dd47 \u0026lt;+23\u0026gt;:\tmov %rbp,0x18(%rsp) //rsp(0x7fffffffe450)+0x18 = rbp ==\u0026gt; caller\u0026#39;s栈底(rbp)入callee的栈 /* (gdb) x/8xb 0x7fffffffe468 0x7fffffffe468:\t0x90\t0xe4\t0xff\t0xff\t0xff\t0x7f\t0x00\t0x00 从前面 info register rbp rsp pc返回可以看到,就是rbp的值 */ 0x000000000042dd4c \u0026lt;+28\u0026gt;:\tlea 0x18(%rsp),%rbp //取地址不取引用,得到rbp=0x7fffffffe468; 所以callee[mstart1函数]新的栈底rbp就指向这 /* (gdb) info register rbp rbp 0x7fffffffe468\t0x7fffffffe468 */ 0x000000000042dd51 \u0026lt;+33\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rax // rax = \u0026amp;g0 0x000000000042dd5a \u0026lt;+42\u0026gt;:\tmov 0x30(%rax),%rcx //rcx = g0.m.g0 0x000000000042dd5e \u0026lt;+46\u0026gt;:\tcmp %rax,(%rcx) // g0是否与g0.m.g0相等 0x000000000042dd61 \u0026lt;+49\u0026gt;:\tjne 0x42de14 \u0026lt;runtime.mstart1+228\u0026gt; //跳转到...如果不等于 0x000000000042dd67 \u0026lt;+55\u0026gt;:\tmov %rax,0x10(%rsp) // rsp(0x7fffffffe450)+0x10 = 0x7fffffffe460的地方保存g0的地址 0x000000000042dd6c \u0026lt;+60\u0026gt;:\tmov 0x20(%rsp),%rax //rsp(0x7fffffffe450)+0x20 = 0x7fffffffe470的地址内容放到rax //这里就是caller\u0026#39;s pc 0x000000000042dd71 \u0026lt;+65\u0026gt;:\tmov %rax,(%rsp) //所以从下面可以看出0x7fffffffe470与0x7fffffffe450内容是一样的都是caller\u0026#39;s pc 0x000000000042dd75 \u0026lt;+69\u0026gt;:\tlea 0x28(%rsp),%rax //rsp(0x7fffffffe450)+0x28 = 0x7fffffffe478这个值,不是里面的值放到rax 0x000000000042dd7a \u0026lt;+74\u0026gt;:\tmov %rax,0x8(%rsp) //rsp(0x7fffffffe450)+0x8 = 0x7fffffffe458 ==\u0026gt;所以这个go语言里面的sp是从自调用返回后,sp里的值,它不包括caller调用callee的时候,call指令临时保存的pc,所以地址是0x7fffffffe478而不是0x7fffffffe470 /* (gdb) info register rsp rsp 0x7fffffffe450\t0x7fffffffe450 (gdb) x/64xb 0x7fffffffe450 0x7fffffffe450:\t0x0e\t0xdd\t0x42\t0x00\t0x00\t0x00\t0x00\t0x00 //caller\u0026#39;s pc 0x7fffffffe458:\t0x78\t0xe4\t0xff\t0xff\t0xff\t0x7f\t0x00\t0x00 //caller\u0026#39;s sp 0x7fffffffe460:\t0xc0\t0xda\t0x55\t0x00\t0x00\t0x00\t0x00\t0x00 //局部变量_g_也就是全局变量g0的地址 0x7fffffffe468:\t0x90\t0xe4\t0xff\t0xff\t0xff\t0x7f\t0x00\t0x00 // caller\u0026#39;s rbp 0x7fffffffe470:\t0x0e\t0xdd\t0x42\t0x00\t0x00\t0x00\t0x00\t0x00 // caller\u0026#39;s pc 0x7fffffffe478:\t0x74\t0x15\t0x45\t0x00\t0x00\t0x00\t0x00\t0x00 0x7fffffffe480:\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 0x7fffffffe488:\t0x08\t0xe5\t0xfe\t0xff\t0xff\t0x7f\t0x00\t0x00 */ 0x000000000042dd7f \u0026lt;+79\u0026gt;:\tcallq 0x431bd0 \u0026lt;runtime.save\u0026gt; // save(getcallerpc(), getcallersp()) schedule # // One round of scheduler: find a runnable goroutine and execute it. // Never returns. func schedule() { _g_ := getg() // get g0 //.... top: //... var gp *g var inheritTime bool // Normal goroutines will check for need to wakeP in ready, // but GCworkers and tracereaders will not, so the check must // be done here instead. tryWakeP := false //一般的goroutines会在准备好的时候检查是否需要wakeP。gcworker, tracereaders需要现在唤醒P. if trace.enabled || trace.shutdown { gp = traceReader() if gp != nil { casgstatus(gp, _Gwaiting, _Grunnable) traceGoUnpark(gp, 0) tryWakeP = true } } if gp == nil \u0026amp;\u0026amp; gcBlackenEnabled != 0 { gp = gcController.findRunnableGCWorker(_g_.m.p.ptr()) tryWakeP = tryWakeP || gp != nil } if gp == nil { // Check the global runnable queue once in a while to ensure fairness. // Otherwise two goroutines can completely occupy the local runqueue // by constantly respawning each other. if _g_.m.p.ptr().schedtick%61 == 0 \u0026amp;\u0026amp; sched.runqsize \u0026gt; 0 { lock(\u0026amp;sched.lock) gp = globrunqget(_g_.m.p.ptr(), 1) unlock(\u0026amp;sched.lock) } } if gp == nil { gp, inheritTime = runqget(_g_.m.p.ptr()) if gp != nil \u0026amp;\u0026amp; _g_.m.spinning { throw(\u0026#34;schedule: spinning with local work\u0026#34;) } } if gp == nil { gp, inheritTime = findrunnable() // blocks until work is available } // This thread is going to run a goroutine and is not spinning anymore, // so if it was marked as spinning we need to reset it now and potentially // start a new spinning M. if _g_.m.spinning { resetspinning() } if sched.disable.user \u0026amp;\u0026amp; !schedEnabled(gp) { // Scheduling of this goroutine is disabled. Put it on // the list of pending runnable goroutines for when we // re-enable user scheduling and look again. lock(\u0026amp;sched.lock) if schedEnabled(gp) { // Something re-enabled scheduling while we // were acquiring the lock. unlock(\u0026amp;sched.lock) } else { sched.disable.runnable.pushBack(gp) sched.disable.n++ unlock(\u0026amp;sched.lock) goto top } } // If about to schedule a not-normal goroutine (a GCworker or tracereader), // wake a P if there is one. if tryWakeP { //a GCworker or tracereader,需要唤醒P if atomic.Load(\u0026amp;sched.npidle) != 0 \u0026amp;\u0026amp; atomic.Load(\u0026amp;sched.nmspinning) == 0 { wakep() } } //... execute(gp, inheritTime) // 执行G } schedule()函数主要是获取一个Goroutine.\nschedule函数通过调用globrunqget()和runqget()函数分别从全局运行队列和当前工作线程的本地运行队列中选取下一个需要运行的goroutine; 如果这两个队列都没有需要运行的goroutine则通过findrunnable()函数从其它p的运行队列中盗取goroutine. 一旦找到下一个需要运行的goroutine，则调用excute函数从g0切换到该goroutine去运行.\n否则gp, inheritTime = findrunnable() // blocks until work is available,一直阻塞在findrunnable()上面 execute函数 # // Schedules gp to run on the current M. // If inheritTime is true, gp inherits the remaining time in the // current time slice. Otherwise, it starts a new time slice. // Never returns. // // Write barriers are allowed because this is called immediately after // acquiring a P in several places. // //go:yeswritebarrierrec func execute(gp *g, inheritTime bool) { _g_ := getg() // g0 casgstatus(gp, _Grunnable, _Grunning) //修改将要运行的gp的状态. gp.waitsince = 0 gp.preempt = false //抢占标志位 gp.stackguard0 = gp.stack.lo + _StackGuard //设置栈顶的保护,比栈地址的最低位高点,防止栈越界 if !inheritTime { _g_.m.p.ptr().schedtick++ } _g_.m.curg = gp //这里就开始设置M的curg为gp,而不是g0了 gp.m = _g_.m // m.curg = gp and gp.m = m //... gogo(\u0026amp;gp.sched) } execute函数主要为将要被调度运行的gp设置状态,与M相互关联\ngogo # 寻找在那个文件里面:list /usr/lib/golang/src/runtime/proc.go:2165\ngdb查看发现又到了asm_amd64.s文件\n(gdb) step runtime.gogo () at /usr/lib/golang/src/runtime/asm_amd64.s:272 272\tTEXT runtime·gogo(SB), NOSPLIT, $16-8 (gdb) list // func gogo(buf *gobuf) // restore state from Gobuf; longjmp TEXT runtime·gogo(SB), NOSPLIT, $16-8 MOVQ\tbuf+0(FP), BX\t// gobuf MOVQ\tgobuf_g(BX), DX MOVQ\t0(DX), CX\t// make sure g != nil //防止g.gobuf.g里面的值是空的,空的取地址会panic get_tls(CX) MOVQ\tDX, g(CX) MOVQ\tgobuf_sp(BX), SP\t// restore SP MOVQ\tgobuf_ret(BX), AX MOVQ\tgobuf_ctxt(BX), DX MOVQ\tgobuf_bp(BX), BP MOVQ\t$0, gobuf_sp(BX)\t// clear to help garbage collector MOVQ\t$0, gobuf_ret(BX) MOVQ\t$0, gobuf_ctxt(BX) MOVQ\t$0, gobuf_bp(BX) MOVQ\tgobuf_pc(BX), BX JMP\tBX gogo把将要运行的main函数的goroutine从gobuf中取出到寄存器中,同时清0,有助于垃圾回收.\n重新观察BP寄存器 # 当调度这个main函数的goroutine,从g.sched中恢复的寄存器的值,其中恢复后BP寄存器的值就是0\n(gdb) info breakpoints Num Type Disp Enb Address What 1 breakpoint keep y 0x000000000042fe3e in runtime.execute at /usr/lib/golang/src/runtime/proc.go:2171 breakpoint already hit 1 time 2 breakpoint keep y 0x00000000004515ce in runtime.gogo at /usr/lib/golang/src/runtime/asm_amd64.s:281 3 breakpoint keep y 0x00000000004515d2 in runtime.gogo at /usr/lib/golang/src/runtime/asm_amd64.s:282 (gdb) run The program being debugged has been started already. Start it from the beginning? (y or n) y Starting program: /tmp/kubernets/./test Breakpoint 1, runtime.execute (gp=0xc000000300, inheritTime=true) at /usr/lib/golang/src/runtime/proc.go:2171 2171\tgogo(\u0026amp;gp.sched) (gdb) continue Continuing. Breakpoint 2, runtime.gogo () at /usr/lib/golang/src/runtime/asm_amd64.s:281 281\tMOVQ\tgobuf_bp(BX), BP //这里打了断点,观察下 (gdb) list 276\tget_tls(CX) 277\tMOVQ\tDX, g(CX) 278\tMOVQ\tgobuf_sp(BX), SP\t// restore SP 279\tMOVQ\tgobuf_ret(BX), AX 280\tMOVQ\tgobuf_ctxt(BX), DX 281\tMOVQ\tgobuf_bp(BX), BP //这里打了断点,观察下 ------------------here 282\tMOVQ\t$0, gobuf_sp(BX)\t//这里打了断点,观察下 ------------------here 283\tMOVQ\t$0, gobuf_ret(BX) 284\tMOVQ\t$0, gobuf_ctxt(BX) 285\tMOVQ\t$0, gobuf_bp(BX) (gdb) info register rbp rsp pc bp sp rbp 0x7fffffffe3c8\t0x7fffffffe3c8 rsp 0xc0000307d8\t0xc0000307d8 pc 0x4515ce\t0x4515ce \u0026lt;runtime.gogo+46\u0026gt; bp 0xe3c8\t-7224 //其值开始不是0 ------------------here sp 0xc0000307d8\t0xc0000307d8 (gdb) step Breakpoint 3, runtime.gogo () at /usr/lib/golang/src/runtime/asm_amd64.s:282 282\tMOVQ\t$0, gobuf_sp(BX)\t//这里打了断点,观察下 (gdb) info register rbp rsp pc bp sp rbp 0x0\t0x0 rsp 0xc0000307d8\t0xc0000307d8 pc 0x4515d2\t0x4515d2 \u0026lt;runtime.gogo+50\u0026gt; bp 0x0\t0//其值的确是为0 ------------------here sp 0xc0000307d8\t0xc0000307d8 (gdb) 进入main函数 # 文件里面func main() {:list /usr/lib/golang/src/runtime/proc.go:113\n我们打下断点到这个函数,然后查看它前几个汇编代码\n(gdb) list /usr/lib/golang/src/runtime/proc.go:113 108 109\t// Value to use for signal mask for newly created M\u0026#39;s. 110\tvar initSigmask sigset 111 112\t// The main goroutine. 113\tfunc main() { 114\tg := getg() 115 116\t// Racectx of m0-\u0026gt;g0 is used only as the parent of the main goroutine. 117\t// It must not be used for anything else. (gdb) b 113 Breakpoint 4 at 0x42aea0: file /usr/lib/golang/src/runtime/proc.go, line 113. (gdb) c Continuing. Breakpoint 4, runtime.main () at /usr/lib/golang/src/runtime/proc.go:113 113\tfunc main() { (gdb) list 108 109\t// Value to use for signal mask for newly created M\u0026#39;s. 110\tvar initSigmask sigset 111 112\t// The main goroutine. 113\tfunc main() { 114\tg := getg() 115 116\t// Racectx of m0-\u0026gt;g0 is used only as the parent of the main goroutine. 117\t// It must not be used for anything else. (gdb) info register rbp rsp pc bp sp rbp 0x0\t0x0 rsp 0xc0000307d8\t0xc0000307d8 pc 0x42aea0\t0x42aea0 \u0026lt;runtime.main\u0026gt; bp 0x0\t0 sp 0xc0000307d8\t0xc0000307d8 (gdb) disas Dump of assembler code for function runtime.main: =\u0026gt; 0x000000000042aea0 \u0026lt;+0\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rcx 0x000000000042aea9 \u0026lt;+9\u0026gt;:\tcmp 0x10(%rcx),%rsp 0x000000000042aead \u0026lt;+13\u0026gt;:\tjbe 0x42b1ea \u0026lt;runtime.main+842\u0026gt; 0x000000000042aeb3 \u0026lt;+19\u0026gt;:\tsub $0x78,%rsp 0x000000000042aeb7 \u0026lt;+23\u0026gt;:\tmov %rbp,0x70(%rsp) 0x000000000042aebc \u0026lt;+28\u0026gt;:\tlea 0x70(%rsp),%rbp 0x000000000042aec1 \u0026lt;+33\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rax 0x000000000042aeca \u0026lt;+42\u0026gt;:\tmov %rax,0x68(%rsp) 0x000000000042aecf \u0026lt;+47\u0026gt;:\tmov 0x30(%rax),%rcx 0x000000000042aed3 \u0026lt;+51\u0026gt;:\tmov (%rcx),%rcx 0x000000000042aed6 \u0026lt;+54\u0026gt;:\tmovq $0x0,0x130(%rcx) 0x000000000042aee1 \u0026lt;+65\u0026gt;:\tmovq $0x3b9aca00,0x11e234(%rip) # 0x549120 \u0026lt;runtime.maxstacksize\u0026gt; 0x000000000042aeec \u0026lt;+76\u0026gt;:\tmovb $0x1,0x14dabc(%rip) # 0x5789af \u0026lt;runtime.mainStarted\u0026gt; 前面三条我们前面详细解释过. Dump of assembler code for function runtime.main: =\u0026gt; 0x000000000042aea0 \u0026lt;+0\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rcx 0x000000000042aea9 \u0026lt;+9\u0026gt;:\tcmp 0x10(%rcx),%rsp 0x000000000042aead \u0026lt;+13\u0026gt;:\tjbe 0x42b1ea \u0026lt;runtime.main+842\u0026gt; 虽然这个rbp是0,是空的,但是还是保存到栈里面去了. 0x000000000042aeb3 \u0026lt;+19\u0026gt;:\tsub $0x78,%rsp 0x000000000042aeb7 \u0026lt;+23\u0026gt;:\tmov %rbp,0x70(%rsp) 0x000000000042aebc \u0026lt;+28\u0026gt;:\tlea 0x70(%rsp),%rbp TODO systemstack\nhttp://www.mit.edu/afs.new/sipb/project/golang/doc/asm.html\n// \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- need deleted \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n// \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- // 获取课堂、作业完成情况 rpc GetStudyReport (GetStudyReportRequest) returns (GetStudyReportResponse); // 获取课堂、作业完成情况Json rpc GetStudyReportJson (GetStudyReportRequest) returns (GetStudyReportResponse); // 通过分享ID, 获取课程报告情况 rpc GetStudyReportByShare (GetStudyReportByShareRequest) returns (GetStudyReportResponse); // 通过分享ID, 获取课程报告情况Json rpc GetStudyReportByShareJson (GetStudyReportByShareRequest) returns (GetStudyReportResponse);\n//获取老师点评信息 rpc GetCourseEvaluate (GetCourseEvaluateRequest) returns (GetCourseEvaluateResponse); //获取老师点评信息json rpc GetCourseEvaluateJson (GetCourseEvaluateRequest) returns (GetCourseEvaluateResponse);\nproject\n//\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n//提交AI课任务中的工程 todo 9.2 zxc rpc SubmitAIMission (SubmitMissionProjectRequest) returns (SubmitMissionProjectResponse);\n// \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n//获取工程信息 rpc GetProjectInfo (GetProjectInfoRequest) returns (GetProjectInfoResponse);\n//获取工程信息by分享ID, 不需要token校验 rpc GetTaskInClassByShare (GetTaskInClassByShareRequest) returns (GetTaskInClassByShareResponse);\n//获取工程信息by userID, serialID, courseID, missionID rpc GetProjectInfoByUnique (GetProjectInfoByUniqueRequest) returns (GetProjectInfoByUniqueResponse);\nmessage GetCourseEvaluateResponse { Project project = 1; //工程信息 repeated Comment comments = 2; //所有点评信息 string shareUUID = 3; //如果分享课程报告出去,通过这个ID来获取工程详细信息. }\n//获取工程信息 func (p *AIProjectModel) GetProject(author uint32, seriesID uint32, courseID uint32) (define.RetCode, *worker.Project) {\n// Array returns the optimal driver.Valuer and sql.Scanner for an array or // slice of any dimension. // // For example: // db.Query(SELECT * FROM t WHERE id = ANY($1), pq.Array([]int{235, 401})) // // var x []sql.NullInt64 // db.QueryRow(\u0026lsquo;SELECT ARRAY[235, 401]\u0026rsquo;).Scan(pq.Array(\u0026amp;x)) // // Scanning multi-dimensional arrays is not supported. Arrays where the lower // bound is not one (such as `[0:0]={1}\u0026rsquo;) are not supported.\naiCommentProject := define.AICommentProject{ ResourceID: resourceID, CoverURL: coverUrl, ResourceIDs: resourceIDs, } alter table ai_course_project add resource_ids integer[] default array[]::integer[]\nalter table ai_course_project add read_aloud_time int default 0 not null;\nalter table share_action_record add resource_ids integer[] default array[]::integer[]\nalter table share_action_record add read_aloud_time int default 0 not null;\n"},{"id":10,"href":"/docs/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1.1-%E6%B1%87%E7%BC%96%E5%9F%BA%E7%A1%80/","title":"1.1 底层类汇编函数","section":"第一章 基础知识","content":" go汇编 # gdb golang assamble non-split; sp bp 会更新吗?\nmain function enter BP is useful ? schedule\ndropg()函数 # dropg() //解除g和m之间的关系; func dropg() { _g_ := getg() setMNoWB(\u0026amp;_g_.m.curg.m, nil) setGNoWB(\u0026amp;_g_.m.curg, nil) } park_m函数 # 通过mcall从gN转到g0; stack转移了, jmp跳转到park_m(*g)函数, # gN\u0026ndash;\u0026gt;m \u0026mdash;\u0026gt; gN mcall怎么保证g0的m指向了m的?\n// park continuation on g0. func park_m(gp *g) { _g_ := getg() if trace.enabled { traceGoPark(_g_.m.waittraceev, _g_.m.waittraceskip) } casgstatus(gp, _Grunning, _Gwaiting) dropg() //解除g和m之间的关系 ...... schedule() } get_tls函数 # TEXT runtime·rt0_go(SB),NOSPLIT,$0 //... LEAQ\truntime·m0+m_tls(SB), DI // //DI = \u0026amp;m0.tls，取m0的tls成员的地址到DI寄存器 CALL\truntime·settls(SB) // 调用settls设置线程本地存储，settls函数的参数在DI寄存器中 // store through it, to make sure it works // 可以看做是测试;测试刚刚那个绑定是否成功。 get_tls(BX) // 把TLS地址放入BX寄存器 MOVQ\t$0x123, g(BX) MOVQ\truntime·m0+m_tls(SB), AX CMPQ\tAX, $0x123 JEQ 2(PC) CALL\truntime·abort(SB) settls # // set tls base to DI TEXT runtime·settls(SB),NOSPLIT,$32 ADDQ\t$8, DI\t// ELF wants to use -8(FS) //因为后面要-8,所以先加+8 MOVQ\tDI, SI // is SI parameter? MOVQ\t$0x1002, DI\t// ARCH_SET_FS MOVQ\t$SYS_arch_prctl, AX SYSCALL CMPQ\tAX, $0xfffffffffffff001 JLS\t2(PC) MOVL\t$0xf1, 0xf1 // crash RET arch_prctl\nint arch_prctl(int code, unsigned long addr); // arch_prctl() sets architecture-specific process or thread state. code selects a subfunction and passes argument addr to it; 其中：ARCH_SET_FS: Set the 64-bit base for the FS register to addr.\n现在知道就是把m0.tls[0]传给FS寄存器\nget_tls(BX)与g(BX) # 这个get_tls(BX)与g()其实都是宏定义：\n#ifdef GOARCH_amd64 #define\tget_tls(r)\tMOVQ TLS, r #define\tg(r)\t0(r)(TLS*1) #endif 可以得到\nMOVQ TLS, BX 0(BX)(TLS*1) 有两个疑问， 第一个是TLS是代表什么? 第二个是0(BX)(TLS*1)是能转换为什么地址？\n// Thread-local storage references use the TLS pseudo-register. // As a register, TLS refers to the thread-local storage base, and it // can only be loaded into another register: // // MOVQ TLS, AX // // An offset from the thread-local storage base is written off(reg)(TLS*1). // Semantically it is off(reg), but the (TLS*1) annotation marks this as // indexing from the loaded TLS base. This emits a relocation so that // if the linker needs to adjust the offset, it can. For example: // // MOVQ TLS, AX // MOVQ 0(AX)(TLS*1), CX // load g into CX // // On systems that support direct access to the TLS memory, this // pair of instructions can be reduced to a direct TLS memory reference: // // MOVQ 0(TLS), CX // load g into CX // // The 2-instruction and 1-instruction forms correspond to the two code // sequences for loading a TLS variable in the local exec model given in \u0026#34;ELF // Handling For Thread-Local Storage\u0026#34;. 这里总结就是:\nTLS代表的是伪寄存器，是线程本地存储的基地址。 语义上off(reg)(TLS*1)== off(reg). 而这个(TLS*1)说明是从线程本地存储的基地址上进行索引。\n参考 # https://www.zhihu.com/question/284288720 https://segmentfault.com/a/1190000038626134 https://www.altoros.com/blog/golang-internals-part-5-the-runtime-bootstrap-process/\ngid\nAddressing modes: (DI)(BX2): The location at address DI plus BX2. 64(DI)(BX2): The location at address DI plus BX2 plus 64. These modes accept only 1, 2, 4, and 8 as scale factors. https://golang.org/doc/asm#directives\nhttps://lrita.github.io/2017/12/12/golang-asm/#%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E5%A3%B0%E6%98%8E\nhttps://github.com/go-internals-cn/go-internals\n// When building for inclusion into a shared library, an instruction of the form // MOV off(CX)(TLS*1), AX // becomes // mov %fs:off(%rcx), %rax // which assumes that the correct TLS offset has been loaded into %rcx (today // there is only one TLS variable -- g -- so this is OK). When not building for // a shared library the instruction does not require a prefix. (m_morebuf+gobuf_pc)(REGISTER) # MOVQ\t8(SP), AX\t// f\u0026#39;s caller\u0026#39;s PC MOVQ\tAX, (m_morebuf+gobuf_pc)(BX) runtime.m runtime.gobuf\ntype m struct { g0 *g // goroutine with scheduling stack morebuf gobuf // gobuf arg to morestack //-----------morebuf-------------// divmod uint32 // div/mod denominator for arm - known to liblink //... } type gobuf struct { // The offsets of sp, pc, and g are known to (hard-coded in) libmach. // // ctxt is unusual with respect to GC: it may be a // heap-allocated funcval, so GC needs to track it, but it // needs to be set and cleared from assembly, where it\u0026#39;s // difficult to have write barriers. However, ctxt is really a // saved, live register, and we only ever exchange it between // the real register and the gobuf. Hence, we treat it as a // root during stack scanning, which means assembly that saves // and restores it doesn\u0026#39;t need write barriers. It\u0026#39;s still // typed as a pointer so that any other writes from Go get // write barriers. sp uintptr pc uintptr // \u0026lt;\u0026lt;\u0026lt;--- g guintptr ctxt unsafe.Pointer ret sys.Uintreg lr uintptr bp uintptr // for GOEXPERIMENT=framepointer } 我们从这个m_morebuf+gobuf_pc就知道指的是这个m结构体中的morebuf结构体字段中的pc值。\nruntime.g\n伪寄存器 # A Quick Guide to Go\u0026rsquo;s Assembler\nFP: Frame pointer: arguments and locals. PC: Program counter: jumps and branches. SB: Static base pointer: global symbols. SP: Stack pointer: top of stack. FP # FP伪寄存器是一个用于引用函数参数的虚拟帧指针。编译器维护一个虚拟帧指针，并将堆栈上的参数引用为该伪寄存器的偏移量。因此0(FP)是函数的第一个参数，8(FP)是第二个参数(在64位机器上)，以此类推。但是，当以这种方式引用一个函数参数时，有必要将名称放在开头，如first_arg+0(FP)和second_arg+8(FP)。(偏移量的含义\u0026ndash;从帧指针出发的偏移量\u0026ndash;与它在SB中的使用不同，在SB中，它是从符号出发的偏移量。) 汇编器强制执行这个约定，拒绝普通的0(FP)和8(FP)。实际的名称在语义上是不相关的，但应该用来记录参数的名称。值得强调的是，FP始终是一个伪寄存器，而不是硬件寄存器，即使在具有硬件帧指针的架构上也是如此。\nThe FP pseudo-register is a virtual frame pointer used to refer to function arguments. The compilers maintain a virtual frame pointer and refer to the arguments on the stack as offsets from that pseudo-register. Thus 0(FP) is the first argument to the function, 8(FP) is the second (on a 64-bit machine), and so on. However, when referring to a function argument this way, it is necessary to place a name at the beginning, as in first_arg+0(FP) and second_arg+8(FP). (The meaning of the offset—offset from the frame pointer—distinct from its use with SB, where it is an offset from the symbol.) The assembler enforces this convention, rejecting plain 0(FP) and 8(FP). The actual name is semantically irrelevant but should be used to document the argument\u0026rsquo;s name. It is worth stressing that FP is always a pseudo-register, not a hardware register, even on architectures with a hardware frame pointer.\n对于带有Go原型的汇编函数，go vet会检查参数名和偏移量是否匹配。在32位系统上，64位值的低位和高位32位是通过在名称中添加一个_lo或_hi后缀来区分的，如arg_lo+0(FP)或arg_hi+4(FP)。如果一个Go原型没有给它的结果命名，那么预期的汇编名是ret。\nFor assembly functions with Go prototypes, go vet will check that the argument names and offsets match. On 32-bit systems, the low and high 32 bits of a 64-bit value are distinguished by adding a _lo or _hi suffix to the name, as in arg_lo+0(FP) or arg_hi+4(FP). If a Go prototype does not name its result, the expected assembly name is ret.\nSP # SP伪寄存器是一个虚拟栈指针，用于引用帧本地变量和为函数调用准备的参数。它指向本地栈帧的顶部，所以引用时应使用负偏移量，范围为[-framesize，0)：x-8(SP)，y-4(SP)，以此类推。\nThe SP pseudo-register is a virtual stack pointer used to refer to frame-local variables and the arguments being prepared for function calls. It points to the top of the local stack frame, so references should use negative offsets in the range [−framesize, 0): x-8(SP), y-4(SP), and so on.\n在具有名为SP的硬件寄存器的架构上，名称前缀可以区分对虚拟栈指针的引用和对架构SP寄存器的引用，即x-8(SP)，y-4(SP)，以此类推。也就是说，x-8(SP)和-8(SP)是不同的内存位置：第一个是指虚拟栈指针伪寄存器，而第二个是指硬件的SP寄存器。\nOn architectures with a hardware register named SP, the name prefix distinguishes references to the virtual stack pointer from references to the architectural SP register. That is, x-8(SP) and -8(SP) are different memory locations: the first refers to the virtual stack pointer pseudo-register, while the second refers to the hardware\u0026rsquo;s SP register.\n总结下：\n如何理解伪寄存器FP和SP呢？其实伪寄存器FP和SP相当于plan9伪汇编中的一个助记符，他们是根据当前函数栈空间计算出来的一个相对于物理寄存器SP的一个偏移量坐标。 伪SP和FP的相对位置是会变的，所以不应该尝试用伪SP寄存器去找那些用FP+offset来引用的值，例如函数的入参和返回值。 官方文档中说的伪SP指向stack的top，是有问题的。其指向的局部变量位置实际上是整个栈的栈底（除caller BP 之外），所以说bottom更合适一些。 MOVQ\t0(SP), AX // f's PC,这种前面没有flags的，它相当于实际的寄存器的值，不是伪寄存器了。 下面来做下实验。\n确认FP， SP相对于real register的位置点在那里。 我们 presudo FP应该在caller\u0026rsquo;s next pc + 8byte presudo SP应该在caller\u0026rsquo;s BP main.go\npackage main func test_FP_SP(a, b int64)(first uintptr, second uintptr) func main(){ first, second := test_FP_SP(1, 2) first -= second _ = first } test_FP_SP.s\n// func test_FP_SP(a, b int64)(first uintptr, second uintptr) TEXT ·test_FP_SP(SB),$4112-16 LEAQ x-0(SP), DI // MOVQ DI, first+16(FP) // 将原伪寄存器SP偏移量存入返回值first MOVQ SP, BP // 存储物理SP偏移量到BP寄存器 ADDQ $4096, SP // 将物理SP偏移增加4K LEAQ x-0(SP), SI // 在上面中只改变了一个值就是SP这个寄存器，然后再次一模一样的把x-0(SP)给到了SI. /* 第一个 MOVQ BP, SP */ MOVQ BP, SP // 恢复物理SP，因为修改物理SP后，伪寄存器FP/SP随之改变， // 为了正确访问FP，先恢复物理SP MOVQ SI, second+24(FP) // 将偏移后的伪寄存器SP偏移量存入返回值second /* 第二个 MOVQ BP, SP */ //MOVQ BP, SP RET\t// 从输出的second-first来看，正好相差4K 编译一下源代码：go build -gcflags \u0026quot;-N -l\u0026quot; -o test . OR go build -gcflags \u0026quot;all=-N -l\u0026quot; -o test . OR GOOS=linux GOARCH=amd64 go tool compile -S .\n[root@iZf8z14idfp0rwhiicngwqZ FP_SP]# tree . . ├── main.go ├── main.o ├── test └── test_FP_SP.s 我们用到的gdb命令:\ngdb ./test list b 6 display /25i $pc-8 si si si 从上面的图中可以看出，go assemble中的x-0(SP)与first+16(FP),其实都是与SP寄存器关联的， 其中SP，伪FP，与伪SP的位置，在下图中已经标识出来了;\n+------------------------+ | | | | | second | | | |------------------------- | | | | | first | | | |------------------------| | | | | | b | | | |------------------------| | | | | | a | | | +------------------------+ \u0026lt;-------- 伪FP!!! | | | | | caller\u0026#39;s pc | | | +------------------------+ | | | | | caller\u0026#39;s BP | | | |------------------------| \u0026lt;-------- callee\u0026#39;s BP \u0026lt;===\u0026gt; 伪SP!!! | | | | | ... | | | | | +------------------------+ \u0026lt;-------- 真实寄存器SP[它等于caller\u0026#39;s SP - caller\u0026#39;s next CP(8) - callee\u0026#39;s stack size;上图已经标识了] 扩展 # 当我们把/* 第一个 MOVQ BP, SP */下面的注释掉，执行的话会panic，是因为PC寄存器读取错误，而不是注释掉的下一行导致的。\n可以实验下:我们把/* 第二个 MOVQ BP, SP */取消注释，它就正常执行，只是返回值不对而已。\ngo编译器加的函数头的部分 # =\u0026gt; 0x459240 \u0026lt;main.test_FP_SP\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rcx 0x459249 \u0026lt;main.test_FP_SP+9\u0026gt;:\tmov 0x10(%rcx),%rsi 0x45924d \u0026lt;main.test_FP_SP+13\u0026gt;:\tcmp $0xfffffffffffffade,%rsi 0x459254 \u0026lt;main.test_FP_SP+20\u0026gt;:\tje 0x4592bd \u0026lt;main.test_FP_SP+125\u0026gt; 0x459256 \u0026lt;main.test_FP_SP+22\u0026gt;:\tlea 0x370(%rsp),%rax 0x45925e \u0026lt;main.test_FP_SP+30\u0026gt;:\tsub %rsi,%rax 0x459261 \u0026lt;main.test_FP_SP+33\u0026gt;:\tcmp $0x1308,%rax 0x459267 \u0026lt;main.test_FP_SP+39\u0026gt;:\tjbe 0x4592bd \u0026lt;main.test_FP_SP+125\u0026gt; 0x459269 \u0026lt;main.test_FP_SP+41\u0026gt;:\tsub $0x1018,%rsp 0x459270 \u0026lt;main.test_FP_SP+48\u0026gt;:\tmov %rbp,0x1010(%rsp) 0x459278 \u0026lt;main.test_FP_SP+56\u0026gt;:\tlea 0x1010(%rsp),%rbp 0x459280 \u0026lt;main.test_FP_SP+64\u0026gt;:\tlea 0x1010(%rsp),%rdi 0x459288 \u0026lt;main.test_FP_SP+72\u0026gt;:\tmov %rdi,0x1030(%rsp) 0x459290 \u0026lt;main.test_FP_SP+80\u0026gt;:\tmov %rsp,%rbp 0x459293 \u0026lt;main.test_FP_SP+83\u0026gt;:\tadd $0x1000,%rsp 0x45929a \u0026lt;main.test_FP_SP+90\u0026gt;:\tlea 0x1010(%rsp),%rsi 0x4592a2 \u0026lt;main.test_FP_SP+98\u0026gt;:\tmov %rbp,%rsp 0x4592a5 \u0026lt;main.test_FP_SP+101\u0026gt;:\tmov %rsi,0x1038(%rsp) 0x4592ad \u0026lt;main.test_FP_SP+109\u0026gt;:\tmov 0x1010(%rsp),%rbp 0x4592b5 \u0026lt;main.test_FP_SP+117\u0026gt;:\tadd $0x1018,%rsp 0x4592bc \u0026lt;main.test_FP_SP+124\u0026gt;:\tretq 0x4592bd \u0026lt;main.test_FP_SP+125\u0026gt;:\tcallq 0x450c70 \u0026lt;runtime.morestack_noctxt\u0026gt; 0x4592c2 \u0026lt;main.test_FP_SP+130\u0026gt;:\tjmpq 0x459240 \u0026lt;main.test_FP_SP\u0026gt; 0x4592c7:\tadd %al,(%rax) 0x4592c9:\tadd %al,(%rax) 在头和尾部加上了其他跳转代码。 0x10(%rcx) $0xfffffffffffffade type stack struct { lo uintptr hi uintptr } type g struct { // Stack parameters. // stack describes the actual stack memory: [stack.lo, stack.hi). // stackguard0 is the stack pointer compared in the Go stack growth prologue. // It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption. // stackguard1 is the stack pointer compared in the C stack growth prologue. // It is stack.lo+StackGuard on g0 and gsignal stacks. // It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash). stack stack // offset known to runtime/cgo stackguard0 uintptr // offset known to liblink stackguard1 uintptr // offset known to liblink //... 其中rcx是g的地址，所以0x10(%rcx)是就是跳过stack得到stackguard0(它是go栈增长的序章，开始的地方,值是：stack.lo+StackGuard) var StackGuard = 928*stackGuardMultiplier() + StackSystem\n当被抢占的时候，它是: StackPreempt = -1314 // 0xfff\u0026hellip;fade\norder explainment je Jumps if equal jbe Jumps if below or equal 例子 # main goroutine得不到执行 # package main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; ) func g2() { sum := 0 for { sum++ } } func main() { runtime.GOMAXPROCS(1) go g2() for { runtime.Gosched() fmt.Println(\u0026#34;main is scheduled!\u0026#34;) } } 将不会打印[\u0026ldquo;main is scheduled!\u0026rdquo;] 一些汇编需要的常见的指令 # go build -gcflags \u0026#34;-N -l\u0026#34; -o test . info files info registers sp bp info breakpoint x/10x help x x/16xb 0x7fffffffe470 [https://visualgdb.com/gdbreference/commands/x] info register rbp rsp pc display /20i $pc https://blog.csdn.net/counsellor/article/details/100034080\n附录 # https://lrita.github.io/2017/12/12/golang-asm/#interface\n"},{"id":11,"href":"/docs/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1.2-%E5%86%85%E5%AD%98/","title":"1.2 内存大小端概念","section":"第一章 基础知识","content":" https://blog.csdn.net/fy_lei/article/details/49813137\n对于这两种存储方式，如果联系下我们常用 “异或” 操作，就能得到一个很好的记忆规则。总结来说就是： 低地址存储低字节即为小端存储；高地址存储高字节即为小端存储； 低地址存储高字节即为大端存储；高地址存储低字节即为大端存储； 我们把 “低” 、“小” 认作 “0”， 把 “高”、“大” 认作 “1”，这样就可以利用 “异或” 的结果来加强记忆了。\n内存 字节 大端/小端 低地址 低字节 小端存储 0 0 0 高地址 高字节 小端存储 1 1 0 低地址 高字节 大端存储 0 1 1 高地址 低字节 大端存储 1 0 1 "}]