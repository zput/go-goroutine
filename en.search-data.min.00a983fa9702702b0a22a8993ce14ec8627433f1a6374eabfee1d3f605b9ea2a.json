[{"id":0,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/%E8%B0%83%E5%BA%A6/%E4%BA%8C-%E8%B0%83%E5%BA%A6%E7%9A%84%E6%97%B6%E6%9C%BA/4.2.1-%E4%B8%BB%E5%8A%A8%E8%B0%83%E5%BA%A6/","title":"主动调度","section":"调度时机","content":" goroutine主动调度 # 探寻runtime.Gosched() # 我们的Goroutine主动调度(runtime.Gosched()),就是主动放弃与M的关联,放入全局空闲G队列.\nsrc/runtime/proc.go:267\n// Gosched yields the processor, allowing other goroutines to run. It does not // suspend the current goroutine, so execution resumes automatically. func Gosched() { checkTimeouts() mcall(gosched_m) } 定义程序 # main.go\npackage main import ( \u0026#34;runtime\u0026#34; \u0026#34;sync/atomic\u0026#34; ) var existFlag int32 = 2 // the function\u0026#39;s body is empty func addAssemble(x, y int64) int64 func add(a int64){ addAssemble(a, a) atomic.AddInt32(\u0026amp;existFlag, -1) } func main() { runtime.GOMAXPROCS(1) go add(1) go add(2) for { if atomic.LoadInt32(\u0026amp;existFlag) \u0026lt;=0{ break } runtime.Gosched() } } add_amd.s\nTEXT ·addAssemble(SB),$0-24 MOVQ x+0(FP), BX MOVQ y+8(FP), BP ADDQ BP, BX MOVQ BX, ret+16(FP) RET gdb调试前准备 # 编译程序 # 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n准备mcall函数断点的文件 # gdb list /usr/lib/golang/src/runtime/proc.go:267 gdb调试自定义函数 # define zxc info threads info register rbp rsp pc step continue end gdb # gdb总结 # mcall 把主动放弃调度的Goroutine的保存寄存器的值到sched的sp,bp,pc; 也包括g指针自己. 把g0\u0026rsquo;s sched.SP恢复到寄存器SP,[这里没有pc,我们前面已经很详细的讨论过这个] 根据传入进来的函数指针,执行对应函数[在这里就是gosched_m函数],从这里开始就是使用的g0的栈. gosched_m函数 # // Gosched continuation on g0. func gosched_m(gp *g) { if trace.enabled { traceGoSched() } goschedImpl(gp) } func goschedImpl(gp *g) { status := readgstatus(gp) if status\u0026amp;^_Gscan != _Grunning { dumpgstatus(gp) throw(\u0026#34;bad g status\u0026#34;) } casgstatus(gp, _Grunning, _Grunnable)//改成可运行,而不是运行中的状态了 dropg() // m.curg = nil, gp.m = nil互相不关联 lock(\u0026amp;sched.lock) //因为要操作全局队列先加锁 globrunqput(gp) unlock(\u0026amp;sched.lock) //unlock schedule() //进入调度 } 这里要注意的一点就是是把这个Goroutine放入全局队列,而不是本地队列\n附录 # golang全局Goroutine队列 # 这里扩展一下什么是全局运行队列\nGlobal runnable queue.\n这个队列里面的Goroutine的状态都是_Grunnable,可以运行,随时可运行,只要能被调度起来,调度起来就变成了运行中_Grunning.\nvar ( allglen uintptr allm *m allp []*p // len(allp) == gomaxprocs; may change at safe points, otherwise immutable allpLock mutex // Protects P-less reads of allp and all writes gomaxprocs int32 ncpu int32 sched schedt //... ) type schedt struct { //... // Global runnable queue. 全局运行队列 runq gQueue runqsize int32 //... } // A gQueue is a dequeue of Gs linked through g.schedlink. A G can only // be on one gQueue or gList at a time. type gQueue struct { head guintptr tail guintptr } 全局可运行队列全部展现出来了,那么他们是怎么关联的,有头有尾是链表类型.\n全局可运行队列关联结构 # 我们回到最上面可以的globrunqput\n// Put gp on the global runnable queue. // Sched must be locked. // May run during STW, so write barriers are not allowed. //go:nowritebarrierrec func globrunqput(gp *g) { sched.runq.pushBack(gp) sched.runqsize++ } // pushBack adds gp to the tail of q. func (q *gQueue) pushBack(gp *g) { gp.schedlink = 0 if q.tail != 0 { q.tail.ptr().schedlink.set(gp) } else { q.head.set(gp) } q.tail.set(gp) } "},{"id":1,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/%E8%B0%83%E5%BA%A6/2_init_before_enter_main_function/","title":"初始化","section":"调度","content":" 进入main函数前的初始化 # 例子 # 我们首先来gdb调试一下这个程序\nmain.go\npackage main import \u0026#34;fmt\u0026#34; // the function\u0026#39;s body is empty func add(x, y int64) int64 func main() { gg:=add(2, 3) fmt.Println(gg) } add_amd.s\nTEXT ·add(SB),$0-24 MOVQ x+0(FP), BX MOVQ y+8(FP), BP ADDQ BP, BX MOVQ BX, ret+16(FP) RET 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n程序加载到内存入口 # (gdb) info files Symbols from \u0026#34;/tmp/kubernets/test\u0026#34;. Local exec file: `/tmp/kubernets/test\u0026#39;, file type elf64-x86-64. Entry point: 0x454e00 0x0000000000401000 - 0x000000000048cfd3 is .text 0x000000000048d000 - 0x00000000004dc550 is .rodata 0x00000000004dc720 - 0x00000000004dd38c is .typelink 0x00000000004dd390 - 0x00000000004dd3e0 is .itablink 0x00000000004dd3e0 - 0x00000000004dd3e0 is .gosymtab 0x00000000004dd3e0 - 0x0000000000548adf is .gopclntab 0x0000000000549000 - 0x0000000000549020 is .go.buildinfo 0x0000000000549020 - 0x0000000000556118 is .noptrdata 0x0000000000556120 - 0x000000000055d110 is .data 0x000000000055d120 - 0x0000000000578990 is .bss 0x00000000005789a0 - 0x000000000057b108 is .noptrbss 0x0000000000400f9c - 0x0000000000401000 is .note.go.buildid (gdb) b *0x454e00 Breakpoint 1 at 0x454e00: file /usr/lib/golang/src/runtime/rt0_linux_amd64.s, line 8. //跳到这个文件来了 (gdb) list /usr/lib/golang/src/runtime/rt0_linux_amd64.s:1 1\t// Copyright 2009 The Go Authors. All rights reserved. 2\t// Use of this source code is governed by a BSD-style 3\t// license that can be found in the LICENSE file. 4 5\t#include \u0026#34;textflag.h\u0026#34; 6 7\tTEXT _rt0_amd64_linux(SB),NOSPLIT,$-8 8\tJMP\t_rt0_amd64(SB) (gdb) run Starting program: /tmp/kubernets/test Breakpoint 1, _rt0_amd64_linux () at /usr/lib/golang/src/runtime/rt0_linux_amd64.s:8 //跳到这个文件来了 8\tJMP\t_rt0_amd64(SB) (gdb) info registers bp bp 0x0\t0 (gdb) info registers sp sp 0x7fffffffe4d0\t0x7fffffffe4d0 (gdb) next 16\tLEAQ\t8(SP), SI\t// argv (gdb) list 11\t// internal linking. This is the entry point for the program from the 12\t// kernel for an ordinary -buildmode=exe program. The stack holds the 13\t// number of arguments and the C-style argv. 14\tTEXT _rt0_amd64(SB),NOSPLIT,$-8 15\tMOVQ\t0(SP), DI\t// argc 16\tLEAQ\t8(SP), SI\t// argv 17\tJMP\truntime·rt0_go(SB) (gdb) next runtime.rt0_go () at /usr/lib/golang/src/runtime/asm_amd64.s:89 //跳到这个文件来了 89\tMOVQ\tDI, AX\t// argc (gdb) list 84\tDATA _rt0_amd64_lib_argv\u0026lt;\u0026gt;(SB)/8, $0 85\tGLOBL _rt0_amd64_lib_argv\u0026lt;\u0026gt;(SB),NOPTR, $8 86 87\tTEXT runtime·rt0_go(SB),NOSPLIT,$0 88\t// copy arguments forward on an even stack 89\tMOVQ\tDI, AX\t// argc 90\tMOVQ\tSI, BX\t// argv 91\tSUBQ\t$(4*8+7), SP\t// 2args 2auto 92\tANDQ\t$~15, SP 93\tMOVQ\tAX, 16(SP) (gdb) 从上面的调试来看,最终到到达了src/runtime/asm_amd64.s:89的 runtime.rt0_go函数.\nfile /usr/lib/golang/src/runtime/rt0_linux_amd64.s, line 8. //跳到这个文件来 _rt0_amd64_linux () at /usr/lib/golang/src/runtime/rt0_linux_amd64.s:8 //跳到这个文件来 runtime.rt0_go () at /usr/lib/golang/src/runtime/asm_amd64.s:89 //跳到这个文件来 进行初始化全局变量 # 主要是rt0_go()函数\n初始化全局变量g0 # TEXT runtime·rt0_go(SB),NOSPLIT,$0 // copy arguments forward on an even stack MOVQ\tDI, AX\t// argc MOVQ\tSI, BX\t// argv SUBQ\t$(4*8+7), SP\t// 2args 2auto ANDQ\t$~15, SP MOVQ\tAX, 16(SP) MOVQ\tBX, 24(SP) //TODO zxc 调整栈顶寄存器使其按16字节对齐; argc放在SP + 16字节处; argv放在SP + 24字节处 // create istack out of the given (operating system) stack. // _cgo_init may update stackguard. MOVQ\t$runtime·g0(SB), DI //全局的变量g0的地址放入DI寄存器 LEAQ\t(-64*1024+104)(SP), BX MOVQ\tBX, g_stackguard0(DI) MOVQ\tBX, g_stackguard1(DI) MOVQ\tBX, (g_stack+stack_lo)(DI) MOVQ\tSP, (g_stack+stack_hi)(DI) // TODO zxc 始初始化全局变量g0, g0的栈大约有64K，地址范围为 SP - 64*1024 + 104 ～ SP /* ...省略一些CPU相关的汇编 */ LEAQ\truntime·m0+m_tls(SB), DI // //DI = \u0026amp;m0.tls，取m0的tls成员的地址到DI寄存器 CALL\truntime·settls(SB) // 调用settls设置线程本地存储，settls函数的参数在DI寄存器中 // store through it, to make sure it works // 可以看做是单元测试 get_tls(BX) MOVQ\t$0x123, g(BX) MOVQ\truntime·m0+m_tls(SB), AX CMPQ\tAX, $0x123 JEQ 2(PC) CALL\truntime·abort(SB) 分析下这个runtime·settls()函数,因为这个ELF需要-8,所以提前+8,那么最终执行完这个函数,它是设置在全局的m0的tls[0]里面,还是tls[1]里面呢? 观察下这个MOVQ\truntime·m0+m_tls(SB), AX,可以知道就是m0中的tls[0]的地址\n// set tls base to DI TEXT runtime·settls(SB),NOSPLIT,$32 ADDQ\t$8, DI\t// ELF wants to use -8(FS) //因为后面要-8,所以先加+8 MOVQ\tDI, SI // is SI parameter? MOVQ\t$0x1002, DI\t// ARCH_SET_FS MOVQ\t$SYS_arch_prctl, AX SYSCALL CMPQ\tAX, $0xfffffffffffff001 JLS\t2(PC) MOVL\t$0xf1, 0xf1 // crash RET 只是一个宏定义src/runtime/go_tls.h,get_tls(BX)把TLS地址放入BX寄存器.\n#ifdef GOARCH_amd64 #define\tget_tls(r)\tMOVQ TLS, r #define\tg(r)\t0(r)(TLS*1) #endif // Thread-local storage references use the TLS pseudo-register. // As a register, TLS refers to the thread-local storage base, and it // can only be loaded into another register: // // MOVQ TLS, AX // // An offset from the thread-local storage base is written off(reg)(TLS*1). // Semantically it is off(reg), but the (TLS*1) annotation marks this as // indexing from the loaded TLS base. This emits a relocation so that // if the linker needs to adjust the offset, it can. For example: // // MOVQ TLS, AX // MOVQ 0(AX)(TLS*1), CX // load g into CX // // On systems that support direct access to the TLS memory, this // pair of instructions can be reduced to a direct TLS memory reference: // // MOVQ 0(TLS), CX // load g into CX // // The 2-instruction and 1-instruction forms correspond to the two code // sequences for loading a TLS variable in the local exec model given in \u0026#34;ELF // Handling For Thread-Local Storage\u0026#34;. // When building for inclusion into a shared library, an instruction of the form // MOV off(CX)(TLS*1), AX // becomes // mov %fs:off(%rcx), %rax // which assumes that the correct TLS offset has been loaded into %rcx (today // there is only one TLS variable -- g -- so this is OK). When not building for // a shared library the instruction does not require a prefix. m0和g0绑定在一起 # 我们继续下面的runtime·rt0_go函数.\nok: // set the per-goroutine and per-mach \u0026#34;registers\u0026#34; get_tls(BX) LEAQ\truntime·g0(SB), CX //取的g0的在堆上的地址放入CX--\u0026gt;CX = g0的地址 MOVQ\tCX, g(BX) //把g0的地址保存在线程本地存储里面，也就是*TLS=\u0026amp;g0 --\u0026gt; m0.tls[0]=\u0026amp;g0 LEAQ\truntime·m0(SB), AX //AX=\u0026amp;m0 // save m-\u0026gt;g0 = g0 MOVQ\tCX, m_g0(AX) // save m0 to g0-\u0026gt;m MOVQ\tAX, g_m(CX) m0和g0绑定在一起，这样，之后在主线程中通过get_tls可以获取到g0，通过g0的m成员又可以找到m0，于是这里就实现了m0和g0与主线程之间的关联。 从这里还可以看到，保存在主线程本地存储中的值是g0的地址，也就是说工作线程的私有全局变量其实是一个指向g的指针而不是指向m的指针\n设置一些ncpu(CPU核数), M,P的个数, 初始化m0 # 我们继续下面的runtime·rt0_go函数.\nCLD\t// convention is D is always left cleared 不需要关心 CALL\truntime·check(SB) //不需要关心 MOVL\t16(SP), AX\t// copy argc; AX = argc MOVL\tAX, 0(SP) // argc放在栈顶 MOVQ\t24(SP), AX\t// copy argv; AX = argv MOVQ\tAX, 8(SP) // argv放在SP + 8的位置 CALL\truntime·args(SB) CALL\truntime·osinit(SB) //全局变量 ncpu = CPU核数 CALL\truntime·schedinit(SB) schedinit # schedinit主要是进行一些初始化工作, 必过设置M的最大个数,P的个数,还有初始化这m0,另外这个getg()也很有趣,分为our G, system G\nsr/runtime/proc.go\n// The bootstrap sequence is: // //\tcall osinit //\tcall schedinit //\tmake \u0026amp; queue new G //\tcall runtime·mstart // // The new G calls runtime·main. func schedinit() { // raceinit must be the first call to race detector. // In particular, it must be done before mallocinit below calls racemapshadow. _g_ := getg() /* getg函数在源代码中没有对应的定义，由编译器插入类似下面两行代码 get_tls(CX) // CX=TLS MOVQ g(CX), BX; BX存器里面现在放的是当前g结构体对象的地址 可以参考这个,根据https://github.com/golang/go/blob/d029058b5912312963225c40ae4bf44e3cb4be76/src/runtime/HACKING.md */ //... sched.maxmcount = 10000 // 最多启动10000个操作系统线程，也是最多10000个M //... mcommoninit(_g_.m) //初始化m0，因为从前面的代码我们知道g0-\u0026gt;m = \u0026amp;m0 //... procs := ncpu //系统中有多少核，就创建和初始化多少个p结构体对象 if n, ok := atoi32(gogetenv(\u0026#34;GOMAXPROCS\u0026#34;)); ok \u0026amp;\u0026amp; n \u0026gt; 0 { procs = n //如果指定了GOMAXPROCS大于0，则创建指定数量的p } if procresize(procs) != nil { throw(\u0026#34;unknown runnable goroutine during bootstrap\u0026#34;) } //... } mcommoninit # mcommoninit主要是判断m是否超过个数,然后放入全局m链表,防止垃圾回收\nm0.p = allp[0] allp[0].m = \u0026amp;m0 sr/runtime/proc.go\nfunc mcommoninit(mp *m) { _g_ := getg() // g0 stack won\u0026#39;t make sense for user (and is not necessary unwindable). if _g_ != _g_.m.g0 { callers(1, mp.createstack[:]) } lock(\u0026amp;sched.lock) if sched.mnext+1 \u0026lt; sched.mnext { throw(\u0026#34;runtime: thread ID overflow\u0026#34;) } mp.id = sched.mnext sched.mnext++ checkmcount() //检查m的个数是否超过sched.maxmcoun //... // Add to allm so garbage collector doesn\u0026#39;t free g-\u0026gt;m // when it is just in a register or thread-local storage. mp.alllink = allm //m放入全局链表allm, 防止垃圾回收 // NumCgoCall() iterates over allm w/o schedlock, // so we need to publish it safely. atomicstorep(unsafe.Pointer(\u0026amp;allm), unsafe.Pointer(mp)) unlock(\u0026amp;sched.lock) //... } 初始化allp, m0和allp[0]绑定 # procresize函数，我们先看下初始化时会执行的代码路径.\n考虑到初始化完成之后, 用户代码还可以通过GOMAXPROCS()函数调用它重新创建和初始化p结构体对象，我们首先只考虑初始化.\n使用make([]*p, nprocs)初始化全局变量allp，把旧的allp拷贝到新的nallp里面,然后allp = make([]*p, nprocs) 循环allp中的每个p,如果是nill的就进行创建, 然后每个都初始化,不管是新的还是旧的. 把m0和allp[0]绑定在一起，即m0.p = allp[0], allp[0].m = m0 把除了allp[0]之外的所有p放入到全局变量sched的pidle空闲队列之中 src/runtime/proc.go\nfunc procresize(nprocs int32) *p { old := gomaxprocs //系统初始化时 gomaxprocs = 0 if old \u0026lt; 0 || nprocs \u0026lt;= 0 { throw(\u0026#34;procresize: invalid arg\u0026#34;) } //... // Grow allp if necessary. if nprocs \u0026gt; int32(len(allp)) { //初始化时 len(allp) == 0 // Synchronize with retake, which could be running // concurrently since it doesn\u0026#39;t run on a P. lock(\u0026amp;allpLock) if nprocs \u0026lt;= int32(cap(allp)) { allp = allp[:nprocs] } else { //初始化时进入此分支，创建allp 切片 nallp := make([]*p, nprocs) // Copy everything up to allp\u0026#39;s cap so we // never lose old allocated Ps. copy(nallp, allp[:cap(allp)]) //把旧的allp拷贝到新的nallp里面 allp = nallp } unlock(\u0026amp;allpLock) } // initialize new P\u0026#39;s for i := old; i \u0026lt; nprocs; i++ { pp := allp[i] if pp == nil { pp = new(p) } pp.init(i) //每个p然后进行初始化 旧的也需要重新进行初始化 atomicstorep(unsafe.Pointer(\u0026amp;allp[i]), unsafe.Pointer(pp)) } //... _g_ := getg() // _g_ = g0 if _g_.m.p != 0 \u0026amp;\u0026amp; _g_.m.p.ptr().id \u0026lt; nprocs {//初始化时m0-\u0026gt;p还未初始化，所以不会执行这个分支 // continue to use the current P _g_.m.p.ptr().status = _Prunning _g_.m.p.ptr().mcache.prepareForSweep() } else {//初始化时执行这个分支 // release the current P and acquire allp[0] if _g_.m.p != 0 {//初始化时这里不执行 _g_.m.p.ptr().m = 0 } _g_.m.p = 0 _g_.m.mcache = nil p := allp[0] p.m = 0 p.status = _Pidle acquirep(p) // --\u0026gt; Associate p and the current m; 在初始化的时候就是把p和m0关联起来,其实是这两个strct的成员相互赋值 if trace.enabled { traceGoStart() } } // release resources from unused P\u0026#39;s --\u0026gt; zxc 就是释放多余的P for i := nprocs; i \u0026lt; old; i++ { p := allp[i] p.destroy() // TODO zxc can\u0026#39;t free P itself because it can be referenced by an M in syscall } //下面这个for 循环把所有空闲的p放入空闲链表 var runnablePs *p for i := nprocs - 1; i \u0026gt;= 0; i-- { p := allp[i] if _g_.m.p.ptr() == p {//不放allp[0],因为它跟m0在上面已经关联了 continue } p.status = _Pidle if runqempty(p) {//初始化时除了allp[0]其它p全部执行这个分支，放入空闲链表 pidleput(p) } else { //... } } //... return runnablePs } 创造一个将要运行runtime·mainPC的goroutine # 我们继续下面的 runtime·rt0_go函数.\n// create a new goroutine to start program MOVQ\t$runtime·mainPC(SB), AX// entry PUSHQ\tAX PUSHQ\t$0// arg size CALL\truntime·newproc(SB) POPQ\tAX newproc # src/runtime/proc.go\n// Create a new g running fn with siz bytes of arguments. // Put it on the queue of g\u0026#39;s waiting to run. // The compiler turns a go statement into a call to this. // Cannot split the stack because it assumes that the arguments // are available sequentially after \u0026amp;fn; they would not be // copied if a stack split occurred. //go:nosplit func newproc(siz int32, fn *funcval) { argp := add(unsafe.Pointer(\u0026amp;fn), sys.PtrSize) gp := getg() pc := getcallerpc() //getcallerpc:返回其调用者的程序计数器（PC）---getcallersp返回其调用者的堆栈指针（SP） systemstack(func() { // systemstack 切到g0的栈和CPU寄存器去执行, 执行完了, 再切换原来的栈和CPU寄存器 newproc1(fn, (*uint8)(argp), siz, gp, pc) /* 创建一个新的g运行的fn，参数的字节数从argp开始; siz: 参数的大小 callergp: 调用者的goroutine pointer[caller-callee] callerpc: 调用者的程序计数器 */ }) } // Create a new g running fn with narg bytes of arguments starting // at argp. callerpc is the address of the go statement that created // this. The new g is put on the queue of g\u0026#39;s waiting to run. func newproc1(fn *funcval, argp *uint8, narg int32, callergp *g, callerpc uintptr) { _g_ := getg() if fn == nil { _g_.m.throwing = -1 // do not dump full stacks throw(\u0026#34;go of nil func value\u0026#34;) } acquirem() // disable preemption because it can be holding p in a local var // TODO zxc 禁止抢占 siz := narg siz = (siz + 7) \u0026amp;^ 7 // We could allocate a larger initial stack if necessary. // Not worth it: this is almost always an error. // 4*sizeof(uintreg): extra space added below // sizeof(uintreg): caller\u0026#39;s LR (arm) or return address (x86, in gostartcall). if siz \u0026gt;= _StackMin-4*sys.RegSize-sys.RegSize { throw(\u0026#34;newproc: function arguments too large for new goroutine\u0026#34;) } _p_ := _g_.m.p.ptr() // 的到m上的p,在这里就是allp[0] newg := gfget(_p_) // 从本地或者全局队列尝试获取g if newg == nil {// 如果获取失败就构造一个g newg = malg(_StackMin) // 跳转到g0的栈,然后再堆上分配, casgstatus(newg, _Gidle, _Gdead) //_Gidle=iota //0 新创建的默认是0,所以就是idle;修改状态为Gdead,这样gc就是扫描newg上面的栈,来尝试要不要回收. allgadd(newg) // publishes with a g-\u0026gt;status of Gdead so GC scanner doesn\u0026#39;t look at uninitialized stack. 这里的把生成的g放入全局allgs []*g//保存所有的g } if newg.stack.hi == 0 { throw(\u0026#34;newproc1: newg missing stack\u0026#34;) } if readgstatus(newg) != _Gdead { //不是Gdead就panic throw(\u0026#34;newproc1: new g is not Gdead\u0026#34;) } totalSize := 4*sys.RegSize + uintptr(siz) + sys.MinFrameSize // extra space in case of reads slightly beyond frame totalSize += -totalSize \u0026amp; (sys.SpAlign - 1) // align to spAlign sp := newg.stack.hi - totalSize spArg := sp if usesLR { // caller\u0026#39;s LR *(*uintptr)(unsafe.Pointer(sp)) = 0 prepGoExitFrame(sp) spArg += sys.MinFrameSize } if narg \u0026gt; 0 { //memmove copies n bytes from \u0026#34;from\u0026#34; to \u0026#34;to\u0026#34;,把在调用者的栈上的,callee memmove(unsafe.Pointer(spArg), unsafe.Pointer(argp), uintptr(narg)) // This is a stack-to-stack copy. If write barriers // are enabled and the source stack is grey (the // destination is always black), then perform a // barrier copy. We do this *after* the memmove // because the destination stack may have garbage on // it. if writeBarrier.needed \u0026amp;\u0026amp; !_g_.m.curg.gcscandone { f := findfunc(fn.fn) stkmap := (*stackmap)(funcdata(f, _FUNCDATA_ArgsPointerMaps)) if stkmap.nbit \u0026gt; 0 { // We\u0026#39;re in the prologue, so it\u0026#39;s always stack map index 0. bv := stackmapdata(stkmap, 0) bulkBarrierBitmap(spArg, spArg, uintptr(bv.n)*sys.PtrSize, 0, bv.bytedata) } } } memclrNoHeapPointers(unsafe.Pointer(\u0026amp;newg.sched), unsafe.Sizeof(newg.sched)) //newg.sched 空间置零 newg.sched.sp = sp newg.stktopsp = sp newg.sched.pc = funcPC(goexit) + sys.PCQuantum // +PCQuantum=1 so that previous instruction is in same function newg.sched.g = guintptr(unsafe.Pointer(newg)) gostartcallfn(\u0026amp;newg.sched, fn) /* 这个函数 1. 调整newg的栈空间，把goexit函数的第二条指令的地址入栈，伪造成goexit函数调用了fn， 从而使fn执行完成后执行ret指令时返回到goexit继续执行完成最后的清理工作； 2. 重新设置newg.buf.pc 为需要执行的函数的地址，即fn，我们这个场景为runtime.main函数的地址。 // adjust Gobuf as if it executed a call to fn // and then did an immediate gosave. func gostartcallfn(gobuf *gobuf, fv *funcval) { var fn unsafe.Pointer if fv != nil { fn = unsafe.Pointer(fv.fn) } else { fn = unsafe.Pointer(funcPC(nilfunc)) } gostartcall(gobuf, fn, unsafe.Pointer(fv)) } // adjust Gobuf as if it executed a call to fn with context ctxt // and then did an immediate gosave. func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer) { sp := buf.sp if sys.RegSize \u0026gt; sys.PtrSize { sp -= sys.PtrSize *(*uintptr)(unsafe.Pointer(sp)) = 0 } sp -= sys.PtrSize *(*uintptr)(unsafe.Pointer(sp)) = buf.pc buf.sp = sp buf.pc = uintptr(fn) buf.ctxt = ctxt } */ newg.gopc = callerpc newg.ancestors = saveAncestors(callergp) newg.startpc = fn.fn //... casgstatus(newg, _Gdead, _Grunnable) //修改goroutine状态 if _p_.goidcache == _p_.goidcacheend { // Sched.goidgen is the last allocated id, // this batch must be [sched.goidgen+1, sched.goidgen+GoidCacheBatch]. // At startup sched.goidgen=0, so main goroutine receives goid=1. _p_.goidcache = atomic.Xadd64(\u0026amp;sched.goidgen, _GoidCacheBatch) _p_.goidcache -= _GoidCacheBatch - 1 _p_.goidcacheend = _p_.goidcache + _GoidCacheBatch } newg.goid = int64(_p_.goidcache) _p_.goidcache++ if raceenabled { newg.racectx = racegostart(callerpc) } if trace.enabled { traceGoCreate(newg, newg.startpc) } runqput(_p_, newg, true) //放入本地队列， 或者全局队列 if atomic.Load(\u0026amp;sched.npidle) != 0 \u0026amp;\u0026amp; atomic.Load(\u0026amp;sched.nmspinning) == 0 \u0026amp;\u0026amp; mainStarted { wakep() } releasem(_g_.m) } //end newproc1 function rbp这个寄存器的值还有用? # rbp这个寄存器的值,会保存到新创建的goroutine里的sched.bp字段?\ntype g struct { stack stack // offset known to runtime/cgo stackguard0 uintptr // offset known to liblink stackguard1 uintptr // offset known to liblink _panic *_panic // innermost panic - offset known to liblink _defer *_defer // innermost defer m *m // current m; offset known to arm liblink sched gobuf //... } type gobuf struct { sp uintptr pc uintptr g guintptr ctxt unsafe.Pointer ret sys.Uintreg lr uintptr bp uintptr // for GOEXPERIMENT=framepointer ------------------------here } 没有保存,当重新调度这个goroutine,重新观察BP寄存器这一章中我们会看到里面是空的 进入main函数里面有rpb是否会入栈 mstart # 我们继续下面的 runtime·rt0_go函数.\n// start this M CALL\truntime·mstart(SB) CALL\truntime·abort(SB)\t// mstart should never return RET // Prevent dead-code elimination of debugCallV1, which is // intended to be called by debuggers. MOVQ\t$runtime·debugCallV1(SB), AX RET // mstart is the entry-point for new Ms. // // This must not split the stack because we may not even have stack // bounds set up yet. // // May run during STW (because it doesn\u0026#39;t have a P yet), so write // barriers are not allowed. // //go:nosplit //go:nowritebarrierrec func mstart() { _g_ := getg() // g0 osStack := _g_.stack.lo == 0 /* ? linux的话， 从runtime·rt0_go我们知道，g0的stack.lo,不是为0的. // create istack out of the given (operating system) stack. // _cgo_init may update stackguard. MOVQ\t$runtime·g0(SB), DI LEAQ\t(-64*1024+104)(SP), BX MOVQ\tBX, g_stackguard0(DI) MOVQ\tBX, g_stackguard1(DI) MOVQ\tBX, (g_stack+stack_lo)(DI) MOVQ\tSP, (g_stack+stack_hi)(DI) */ if osStack { // Initialize stack bounds from system stack. // Cgo may have left stack size in stack.hi. // minit may update the stack bounds. size := _g_.stack.hi if size == 0 { size = 8192 * sys.StackGuardMultiplier } _g_.stack.hi = uintptr(noescape(unsafe.Pointer(\u0026amp;size))) _g_.stack.lo = _g_.stack.hi - size + 1024 } // Initialize stack guard so that we can start calling regular // Go code. _g_.stackguard0 = _g_.stack.lo + _StackGuard //这个stackguard0比stack.lo大，防止越界 // This is the g0, so we can also call go:systemstack // functions, which check stackguard1. _g_.stackguard1 = _g_.stackguard0 //stackguard1的作用是栈溢出检查，如果是stackguard0与stackguard1相等，那么是不能放数据到g0栈中了吗 mstart1() // Exit this thread. if GOOS == \u0026#34;windows\u0026#34; || GOOS == \u0026#34;solaris\u0026#34; || GOOS == \u0026#34;illumos\u0026#34; || GOOS == \u0026#34;plan9\u0026#34; || GOOS == \u0026#34;darwin\u0026#34; || GOOS == \u0026#34;aix\u0026#34; { // Windows, Solaris, illumos, Darwin, AIX and Plan 9 always system-allocate // the stack, but put it in _g_.stack before mstart, // so the logic above hasn\u0026#39;t set osStack yet. osStack = true } mexit(osStack) } func mstart1() { _g_ := getg() //g0 if _g_ != _g_.m.g0 { throw(\u0026#34;bad runtime·mstart\u0026#34;) } // Record the caller for use as the top of stack in mcall and // for terminating the thread. // We\u0026#39;re never coming back to mstart1 after we call schedule, // so other calls can reuse the current frame. save(getcallerpc(), getcallersp()) //就是把pc, sp保存到自己的sched里面， 但是bp寄存器没有保存，因为会重新使用这个栈？ /* TODO zxc 这里为什么不是当前的, gdb查看下. getcallerpc()返回的是mstart调用mstart1时被call指令压栈的返回地址， bp+1 ? getcallersp()函数返回的是调用mstart1函数之前mstart函数的栈顶地址 bp ? */ asminit() minit() //初始化信号 // Install signal handlers; after minit so that minit can // prepare the thread to be able to handle the signals. // 安装信号处理程序;所以minit可以准备线程以便能够处理信号, 在minit之后执行 if _g_.m == \u0026amp;m0 { mstartm0() } if fn := _g_.m.mstartfn; fn != nil { //如果m的mstartfn不是空的，先执行它 fn() } if _g_.m != \u0026amp;m0 { // 如果m不等于m0 就是m0的话,前面m0和allp[0]已经绑定了 acquirep(_g_.m.nextp.ptr()) //TODO zxc 这个acquirep函数是把m和m.nextp进行绑定,如果到这里的时候,nextp已经状态不是空闲的状态, //那么会抛出错误? _g_.m.nextp = 0 } schedule() //进入调度 } save函数保存pc,sp到g0,方便下次再度调度运行g0 # list /usr/lib/golang/src/runtime/proc.go:1167 list /usr/lib/golang/src/runtime/proc.go:1179 list /usr/lib/golang/src/runtime/proc.go:1190\n[root@gitlab kubernets]# gdb test GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-119.el7 Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \u0026#34;show copying\u0026#34; and \u0026#34;show warranty\u0026#34; for details. This GDB was configured as \u0026#34;x86_64-redhat-linux-gnu\u0026#34;. For bug reporting instructions, please see: \u0026lt;http://www.gnu.org/software/gdb/bugs/\u0026gt;... Reading symbols from /tmp/kubernets/test...done. Loading Go Runtime support. (gdb) list /usr/lib/golang/src/runtime/proc.go:1167 1162\t// Go code. 1163\t_g_.stackguard0 = _g_.stack.lo + _StackGuard 1164\t// This is the g0, so we can also call go:systemstack 1165\t// functions, which check stackguard1. 1166\t_g_.stackguard1 = _g_.stackguard0 1167\tmstart1() 1168 1169\t// Exit this thread. 1170\tif GOOS == \u0026#34;windows\u0026#34; || GOOS == \u0026#34;solaris\u0026#34; || GOOS == \u0026#34;illumos\u0026#34; || GOOS == \u0026#34;plan9\u0026#34; || GOOS == \u0026#34;darwin\u0026#34; || GOOS == \u0026#34;aix\u0026#34; { 1171\t// Windows, Solaris, illumos, Darwin, AIX and Plan 9 always system-allocate (gdb) b 1167 Breakpoint 1 at 0x42dd09: file /usr/lib/golang/src/runtime/proc.go, line 1167. (gdb) list /usr/lib/golang/src/runtime/proc.go:1179 1174\tosStack = true 1175\t} 1176\tmexit(osStack) 1177\t} 1178 1179\tfunc mstart1() { 1180\t_g_ := getg() 1181 1182\tif _g_ != _g_.m.g0 { 1183\tthrow(\u0026#34;bad runtime·mstart\u0026#34;) (gdb) b 1179 Breakpoint 2 at 0x42dd30: file /usr/lib/golang/src/runtime/proc.go, line 1179. (gdb) list /usr/lib/golang/src/runtime/proc.go:1190 1185 1186\t// Record the caller for use as the top of stack in mcall and 1187\t// for terminating the thread. 1188\t// We\u0026#39;re never coming back to mstart1 after we call schedule, 1189\t// so other calls can reuse the current frame. 1190\tsave(getcallerpc(), getcallersp()) 1191\tasminit() 1192\tminit() 1193 1194\t// Install signal handlers; after minit so that minit can (gdb) b 1190 Breakpoint 3 at 0x42dd7f: file /usr/lib/golang/src/runtime/proc.go, line 1190. (gdb) info breakpoint Num Type Disp Enb Address What 1 breakpoint keep y 0x000000000042dd09 in runtime.mstart at /usr/lib/golang/src/runtime/proc.go:1167 2 breakpoint keep y 0x000000000042dd30 in runtime.mstart1 at /usr/lib/golang/src/runtime/proc.go:1179 3 breakpoint keep y 0x000000000042dd7f in runtime.mstart1 at /usr/lib/golang/src/runtime/proc.go:1190 (gdb) run Starting program: /tmp/kubernets/test Breakpoint 1, runtime.mstart () at /usr/lib/golang/src/runtime/proc.go:1167 1167\tmstart1() (gdb) list 1162\t// Go code. 1163\t_g_.stackguard0 = _g_.stack.lo + _StackGuard 1164\t// This is the g0, so we can also call go:systemstack 1165\t// functions, which check stackguard1. 1166\t_g_.stackguard1 = _g_.stackguard0 1167\tmstart1() 1168 1169\t// Exit this thread. 1170\tif GOOS == \u0026#34;windows\u0026#34; || GOOS == \u0026#34;solaris\u0026#34; || GOOS == \u0026#34;illumos\u0026#34; || GOOS == \u0026#34;plan9\u0026#34; || GOOS == \u0026#34;darwin\u0026#34; || GOOS == \u0026#34;aix\u0026#34; { 1171\t// Windows, Solaris, illumos, Darwin, AIX and Plan 9 always system-allocate (gdb) info register rbp rsp pc rbp 0x7fffffffe490\t0x7fffffffe490 rsp 0x7fffffffe478\t0x7fffffffe478 pc 0x42dd09\t0x42dd09 \u0026lt;runtime.mstart+105\u0026gt; (gdb) continue Continuing. Breakpoint 2, runtime.mstart1 () at /usr/lib/golang/src/runtime/proc.go:1179 1179\tfunc mstart1() { (gdb) info register rbp rsp pc rbp 0x7fffffffe490\t0x7fffffffe490 rsp 0x7fffffffe470\t0x7fffffffe470 pc 0x42dd30\t0x42dd30 \u0026lt;runtime.mstart1\u0026gt; (gdb) x/8xb 0x7fffffffe470 0x7fffffffe470:\t0x0e\t0xdd\t0x42\t0x00\t0x00\t0x00\t0x00\t0x00 (gdb) info frame 0 Stack frame at 0x7fffffffe478: rip = 0x42dd30 in runtime.mstart1 (/usr/lib/golang/src/runtime/proc.go:1179); saved rip 0x42dd0e called by frame at 0x7fffffffe4a0 source language unknown. Arglist at 0x7fffffffe468, args: Locals at 0x7fffffffe468, Previous frame\u0026#39;s sp is 0x7fffffffe478 Saved registers: rip at 0x7fffffffe470 (gdb) 反汇编看下mstart1到save函数的汇编代码 # (gdb) disass Dump of assembler code for function runtime.mstart1: =\u0026gt; 0x000000000042dd30 \u0026lt;+0\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rcx // rcx = \u0026amp;g0 0x000000000042dd39 \u0026lt;+9\u0026gt;:\tcmp 0x10(%rcx),%rsp // g0.stackguard0与rsp(0x7fffffffe470)判断是否达到栈顶部 0x000000000042dd3d \u0026lt;+13\u0026gt;:\tjbe 0x42de2e \u0026lt;runtime.mstart1+254\u0026gt; // jump below equal \u0026lt;=, 就panic 0x000000000042dd43 \u0026lt;+19\u0026gt;:\tsub $0x20,%rsp // rsp=0x7fffffffe470-0x20=0x7fffffffe450 0x000000000042dd47 \u0026lt;+23\u0026gt;:\tmov %rbp,0x18(%rsp) //rsp(0x7fffffffe450)+0x18 = rbp ==\u0026gt; caller\u0026#39;s栈底(rbp)入callee的栈 /* (gdb) x/8xb 0x7fffffffe468 0x7fffffffe468:\t0x90\t0xe4\t0xff\t0xff\t0xff\t0x7f\t0x00\t0x00 从前面 info register rbp rsp pc返回可以看到,就是rbp的值 */ 0x000000000042dd4c \u0026lt;+28\u0026gt;:\tlea 0x18(%rsp),%rbp //取地址不取引用,得到rbp=0x7fffffffe468; 所以callee[mstart1函数]新的栈底rbp就指向这 /* (gdb) info register rbp rbp 0x7fffffffe468\t0x7fffffffe468 */ 0x000000000042dd51 \u0026lt;+33\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rax // rax = \u0026amp;g0 0x000000000042dd5a \u0026lt;+42\u0026gt;:\tmov 0x30(%rax),%rcx //rcx = g0.m.g0 0x000000000042dd5e \u0026lt;+46\u0026gt;:\tcmp %rax,(%rcx) // g0是否与g0.m.g0相等 0x000000000042dd61 \u0026lt;+49\u0026gt;:\tjne 0x42de14 \u0026lt;runtime.mstart1+228\u0026gt; //跳转到...如果不等于 0x000000000042dd67 \u0026lt;+55\u0026gt;:\tmov %rax,0x10(%rsp) // rsp(0x7fffffffe450)+0x10 = 0x7fffffffe460的地方保存g0的地址 0x000000000042dd6c \u0026lt;+60\u0026gt;:\tmov 0x20(%rsp),%rax //rsp(0x7fffffffe450)+0x20 = 0x7fffffffe470的地址内容放到rax //这里就是caller\u0026#39;s pc 0x000000000042dd71 \u0026lt;+65\u0026gt;:\tmov %rax,(%rsp) //所以从下面可以看出0x7fffffffe470与0x7fffffffe450内容是一样的都是caller\u0026#39;s pc 0x000000000042dd75 \u0026lt;+69\u0026gt;:\tlea 0x28(%rsp),%rax //rsp(0x7fffffffe450)+0x28 = 0x7fffffffe478这个值,不是里面的值放到rax 0x000000000042dd7a \u0026lt;+74\u0026gt;:\tmov %rax,0x8(%rsp) //rsp(0x7fffffffe450)+0x8 = 0x7fffffffe458 ==\u0026gt;所以这个go语言里面的sp是从自调用返回后,sp里的值,它不包括caller调用callee的时候,call指令临时保存的pc,所以地址是0x7fffffffe478而不是0x7fffffffe470 /* (gdb) info register rsp rsp 0x7fffffffe450\t0x7fffffffe450 (gdb) x/64xb 0x7fffffffe450 0x7fffffffe450:\t0x0e\t0xdd\t0x42\t0x00\t0x00\t0x00\t0x00\t0x00 //caller\u0026#39;s pc 0x7fffffffe458:\t0x78\t0xe4\t0xff\t0xff\t0xff\t0x7f\t0x00\t0x00 //caller\u0026#39;s sp 0x7fffffffe460:\t0xc0\t0xda\t0x55\t0x00\t0x00\t0x00\t0x00\t0x00 //局部变量_g_也就是全局变量g0的地址 0x7fffffffe468:\t0x90\t0xe4\t0xff\t0xff\t0xff\t0x7f\t0x00\t0x00 // caller\u0026#39;s rbp 0x7fffffffe470:\t0x0e\t0xdd\t0x42\t0x00\t0x00\t0x00\t0x00\t0x00 // caller\u0026#39;s pc 0x7fffffffe478:\t0x74\t0x15\t0x45\t0x00\t0x00\t0x00\t0x00\t0x00 0x7fffffffe480:\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 0x7fffffffe488:\t0x08\t0xe5\t0xfe\t0xff\t0xff\t0x7f\t0x00\t0x00 */ 0x000000000042dd7f \u0026lt;+79\u0026gt;:\tcallq 0x431bd0 \u0026lt;runtime.save\u0026gt; // save(getcallerpc(), getcallersp()) schedule # // One round of scheduler: find a runnable goroutine and execute it. // Never returns. func schedule() { _g_ := getg() // get g0 //.... top: //... var gp *g var inheritTime bool // Normal goroutines will check for need to wakeP in ready, // but GCworkers and tracereaders will not, so the check must // be done here instead. tryWakeP := false //一般的goroutines会在准备好的时候检查是否需要wakeP。gcworker, tracereaders需要现在唤醒P. if trace.enabled || trace.shutdown { gp = traceReader() if gp != nil { casgstatus(gp, _Gwaiting, _Grunnable) traceGoUnpark(gp, 0) tryWakeP = true } } if gp == nil \u0026amp;\u0026amp; gcBlackenEnabled != 0 { gp = gcController.findRunnableGCWorker(_g_.m.p.ptr()) tryWakeP = tryWakeP || gp != nil } if gp == nil { // Check the global runnable queue once in a while to ensure fairness. // Otherwise two goroutines can completely occupy the local runqueue // by constantly respawning each other. if _g_.m.p.ptr().schedtick%61 == 0 \u0026amp;\u0026amp; sched.runqsize \u0026gt; 0 { lock(\u0026amp;sched.lock) gp = globrunqget(_g_.m.p.ptr(), 1) unlock(\u0026amp;sched.lock) } } if gp == nil { gp, inheritTime = runqget(_g_.m.p.ptr()) if gp != nil \u0026amp;\u0026amp; _g_.m.spinning { throw(\u0026#34;schedule: spinning with local work\u0026#34;) } } if gp == nil { gp, inheritTime = findrunnable() // blocks until work is available } // This thread is going to run a goroutine and is not spinning anymore, // so if it was marked as spinning we need to reset it now and potentially // start a new spinning M. if _g_.m.spinning { resetspinning() } if sched.disable.user \u0026amp;\u0026amp; !schedEnabled(gp) { // Scheduling of this goroutine is disabled. Put it on // the list of pending runnable goroutines for when we // re-enable user scheduling and look again. lock(\u0026amp;sched.lock) if schedEnabled(gp) { // Something re-enabled scheduling while we // were acquiring the lock. unlock(\u0026amp;sched.lock) } else { sched.disable.runnable.pushBack(gp) sched.disable.n++ unlock(\u0026amp;sched.lock) goto top } } // If about to schedule a not-normal goroutine (a GCworker or tracereader), // wake a P if there is one. if tryWakeP { //a GCworker or tracereader,需要唤醒P if atomic.Load(\u0026amp;sched.npidle) != 0 \u0026amp;\u0026amp; atomic.Load(\u0026amp;sched.nmspinning) == 0 { wakep() } } //... execute(gp, inheritTime) // 执行G } schedule()函数主要是获取一个Goroutine.\nschedule函数通过调用globrunqget()和runqget()函数分别从全局运行队列和当前工作线程的本地运行队列中选取下一个需要运行的goroutine; 如果这两个队列都没有需要运行的goroutine则通过findrunnable()函数从其它p的运行队列中盗取goroutine. 一旦找到下一个需要运行的goroutine，则调用excute函数从g0切换到该goroutine去运行.\n否则gp, inheritTime = findrunnable() // blocks until work is available,一直阻塞在findrunnable()上面 execute函数 # // Schedules gp to run on the current M. // If inheritTime is true, gp inherits the remaining time in the // current time slice. Otherwise, it starts a new time slice. // Never returns. // // Write barriers are allowed because this is called immediately after // acquiring a P in several places. // //go:yeswritebarrierrec func execute(gp *g, inheritTime bool) { _g_ := getg() // g0 casgstatus(gp, _Grunnable, _Grunning) //修改将要运行的gp的状态. gp.waitsince = 0 gp.preempt = false //抢占标志位 gp.stackguard0 = gp.stack.lo + _StackGuard //设置栈顶的保护,比栈地址的最低位高点,防止栈越界 if !inheritTime { _g_.m.p.ptr().schedtick++ } _g_.m.curg = gp //这里就开始设置M的curg为gp,而不是g0了 gp.m = _g_.m // m.curg = gp and gp.m = m //... gogo(\u0026amp;gp.sched) } execute函数主要为将要被调度运行的gp设置状态,与M相互关联\ngogo # 寻找在那个文件里面:list /usr/lib/golang/src/runtime/proc.go:2165\ngdb查看发现又到了asm_amd64.s文件\n(gdb) step runtime.gogo () at /usr/lib/golang/src/runtime/asm_amd64.s:272 272\tTEXT runtime·gogo(SB), NOSPLIT, $16-8 (gdb) list // func gogo(buf *gobuf) // restore state from Gobuf; longjmp TEXT runtime·gogo(SB), NOSPLIT, $16-8 MOVQ\tbuf+0(FP), BX\t// gobuf MOVQ\tgobuf_g(BX), DX MOVQ\t0(DX), CX\t// make sure g != nil //防止g.gobuf.g里面的值是空的,空的取地址会panic get_tls(CX) MOVQ\tDX, g(CX) MOVQ\tgobuf_sp(BX), SP\t// restore SP MOVQ\tgobuf_ret(BX), AX MOVQ\tgobuf_ctxt(BX), DX MOVQ\tgobuf_bp(BX), BP MOVQ\t$0, gobuf_sp(BX)\t// clear to help garbage collector MOVQ\t$0, gobuf_ret(BX) MOVQ\t$0, gobuf_ctxt(BX) MOVQ\t$0, gobuf_bp(BX) MOVQ\tgobuf_pc(BX), BX JMP\tBX gogo把将要运行的main函数的goroutine从gobuf中取出到寄存器中,同时清0,有助于垃圾回收.\n重新观察BP寄存器 # 当调度这个main函数的goroutine,从g.sched中恢复的寄存器的值,其中恢复后BP寄存器的值就是0\n(gdb) info breakpoints Num Type Disp Enb Address What 1 breakpoint keep y 0x000000000042fe3e in runtime.execute at /usr/lib/golang/src/runtime/proc.go:2171 breakpoint already hit 1 time 2 breakpoint keep y 0x00000000004515ce in runtime.gogo at /usr/lib/golang/src/runtime/asm_amd64.s:281 3 breakpoint keep y 0x00000000004515d2 in runtime.gogo at /usr/lib/golang/src/runtime/asm_amd64.s:282 (gdb) run The program being debugged has been started already. Start it from the beginning? (y or n) y Starting program: /tmp/kubernets/./test Breakpoint 1, runtime.execute (gp=0xc000000300, inheritTime=true) at /usr/lib/golang/src/runtime/proc.go:2171 2171\tgogo(\u0026amp;gp.sched) (gdb) continue Continuing. Breakpoint 2, runtime.gogo () at /usr/lib/golang/src/runtime/asm_amd64.s:281 281\tMOVQ\tgobuf_bp(BX), BP //这里打了断点,观察下 (gdb) list 276\tget_tls(CX) 277\tMOVQ\tDX, g(CX) 278\tMOVQ\tgobuf_sp(BX), SP\t// restore SP 279\tMOVQ\tgobuf_ret(BX), AX 280\tMOVQ\tgobuf_ctxt(BX), DX 281\tMOVQ\tgobuf_bp(BX), BP //这里打了断点,观察下 ------------------here 282\tMOVQ\t$0, gobuf_sp(BX)\t//这里打了断点,观察下 ------------------here 283\tMOVQ\t$0, gobuf_ret(BX) 284\tMOVQ\t$0, gobuf_ctxt(BX) 285\tMOVQ\t$0, gobuf_bp(BX) (gdb) info register rbp rsp pc bp sp rbp 0x7fffffffe3c8\t0x7fffffffe3c8 rsp 0xc0000307d8\t0xc0000307d8 pc 0x4515ce\t0x4515ce \u0026lt;runtime.gogo+46\u0026gt; bp 0xe3c8\t-7224 //其值开始不是0 ------------------here sp 0xc0000307d8\t0xc0000307d8 (gdb) step Breakpoint 3, runtime.gogo () at /usr/lib/golang/src/runtime/asm_amd64.s:282 282\tMOVQ\t$0, gobuf_sp(BX)\t//这里打了断点,观察下 (gdb) info register rbp rsp pc bp sp rbp 0x0\t0x0 rsp 0xc0000307d8\t0xc0000307d8 pc 0x4515d2\t0x4515d2 \u0026lt;runtime.gogo+50\u0026gt; bp 0x0\t0//其值的确是为0 ------------------here sp 0xc0000307d8\t0xc0000307d8 (gdb) 进入main函数 # 文件里面func main() {:list /usr/lib/golang/src/runtime/proc.go:113\n我们打下断点到这个函数,然后查看它前几个汇编代码\n(gdb) list /usr/lib/golang/src/runtime/proc.go:113 108 109\t// Value to use for signal mask for newly created M\u0026#39;s. 110\tvar initSigmask sigset 111 112\t// The main goroutine. 113\tfunc main() { 114\tg := getg() 115 116\t// Racectx of m0-\u0026gt;g0 is used only as the parent of the main goroutine. 117\t// It must not be used for anything else. (gdb) b 113 Breakpoint 4 at 0x42aea0: file /usr/lib/golang/src/runtime/proc.go, line 113. (gdb) c Continuing. Breakpoint 4, runtime.main () at /usr/lib/golang/src/runtime/proc.go:113 113\tfunc main() { (gdb) list 108 109\t// Value to use for signal mask for newly created M\u0026#39;s. 110\tvar initSigmask sigset 111 112\t// The main goroutine. 113\tfunc main() { 114\tg := getg() 115 116\t// Racectx of m0-\u0026gt;g0 is used only as the parent of the main goroutine. 117\t// It must not be used for anything else. (gdb) info register rbp rsp pc bp sp rbp 0x0\t0x0 rsp 0xc0000307d8\t0xc0000307d8 pc 0x42aea0\t0x42aea0 \u0026lt;runtime.main\u0026gt; bp 0x0\t0 sp 0xc0000307d8\t0xc0000307d8 (gdb) disas Dump of assembler code for function runtime.main: =\u0026gt; 0x000000000042aea0 \u0026lt;+0\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rcx 0x000000000042aea9 \u0026lt;+9\u0026gt;:\tcmp 0x10(%rcx),%rsp 0x000000000042aead \u0026lt;+13\u0026gt;:\tjbe 0x42b1ea \u0026lt;runtime.main+842\u0026gt; 0x000000000042aeb3 \u0026lt;+19\u0026gt;:\tsub $0x78,%rsp 0x000000000042aeb7 \u0026lt;+23\u0026gt;:\tmov %rbp,0x70(%rsp) 0x000000000042aebc \u0026lt;+28\u0026gt;:\tlea 0x70(%rsp),%rbp 0x000000000042aec1 \u0026lt;+33\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rax 0x000000000042aeca \u0026lt;+42\u0026gt;:\tmov %rax,0x68(%rsp) 0x000000000042aecf \u0026lt;+47\u0026gt;:\tmov 0x30(%rax),%rcx 0x000000000042aed3 \u0026lt;+51\u0026gt;:\tmov (%rcx),%rcx 0x000000000042aed6 \u0026lt;+54\u0026gt;:\tmovq $0x0,0x130(%rcx) 0x000000000042aee1 \u0026lt;+65\u0026gt;:\tmovq $0x3b9aca00,0x11e234(%rip) # 0x549120 \u0026lt;runtime.maxstacksize\u0026gt; 0x000000000042aeec \u0026lt;+76\u0026gt;:\tmovb $0x1,0x14dabc(%rip) # 0x5789af \u0026lt;runtime.mainStarted\u0026gt; 前面三条我们前面详细解释过. Dump of assembler code for function runtime.main: =\u0026gt; 0x000000000042aea0 \u0026lt;+0\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rcx 0x000000000042aea9 \u0026lt;+9\u0026gt;:\tcmp 0x10(%rcx),%rsp 0x000000000042aead \u0026lt;+13\u0026gt;:\tjbe 0x42b1ea \u0026lt;runtime.main+842\u0026gt; 虽然这个rbp是0,是空的,但是还是保存到栈里面去了. 0x000000000042aeb3 \u0026lt;+19\u0026gt;:\tsub $0x78,%rsp 0x000000000042aeb7 \u0026lt;+23\u0026gt;:\tmov %rbp,0x70(%rsp) 0x000000000042aebc \u0026lt;+28\u0026gt;:\tlea 0x70(%rsp),%rbp TODO systemstack\nhttp://www.mit.edu/afs.new/sipb/project/golang/doc/asm.html\n// \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- need deleted \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n// \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- // 获取课堂、作业完成情况 rpc GetStudyReport (GetStudyReportRequest) returns (GetStudyReportResponse); // 获取课堂、作业完成情况Json rpc GetStudyReportJson (GetStudyReportRequest) returns (GetStudyReportResponse); // 通过分享ID, 获取课程报告情况 rpc GetStudyReportByShare (GetStudyReportByShareRequest) returns (GetStudyReportResponse); // 通过分享ID, 获取课程报告情况Json rpc GetStudyReportByShareJson (GetStudyReportByShareRequest) returns (GetStudyReportResponse);\n//获取老师点评信息 rpc GetCourseEvaluate (GetCourseEvaluateRequest) returns (GetCourseEvaluateResponse); //获取老师点评信息json rpc GetCourseEvaluateJson (GetCourseEvaluateRequest) returns (GetCourseEvaluateResponse);\nproject\n//\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n//提交AI课任务中的工程 todo 9.2 zxc rpc SubmitAIMission (SubmitMissionProjectRequest) returns (SubmitMissionProjectResponse);\n// \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n//获取工程信息 rpc GetProjectInfo (GetProjectInfoRequest) returns (GetProjectInfoResponse);\n//获取工程信息by分享ID, 不需要token校验 rpc GetTaskInClassByShare (GetTaskInClassByShareRequest) returns (GetTaskInClassByShareResponse);\n//获取工程信息by userID, serialID, courseID, missionID rpc GetProjectInfoByUnique (GetProjectInfoByUniqueRequest) returns (GetProjectInfoByUniqueResponse);\nmessage GetCourseEvaluateResponse { Project project = 1; //工程信息 repeated Comment comments = 2; //所有点评信息 string shareUUID = 3; //如果分享课程报告出去,通过这个ID来获取工程详细信息. }\n//获取工程信息 func (p *AIProjectModel) GetProject(author uint32, seriesID uint32, courseID uint32) (define.RetCode, *worker.Project) {\n// Array returns the optimal driver.Valuer and sql.Scanner for an array or // slice of any dimension. // // For example: // db.Query(SELECT * FROM t WHERE id = ANY($1), pq.Array([]int{235, 401})) // // var x []sql.NullInt64 // db.QueryRow(\u0026lsquo;SELECT ARRAY[235, 401]\u0026rsquo;).Scan(pq.Array(\u0026amp;x)) // // Scanning multi-dimensional arrays is not supported. Arrays where the lower // bound is not one (such as `[0:0]={1}\u0026rsquo;) are not supported.\naiCommentProject := define.AICommentProject{ ResourceID: resourceID, CoverURL: coverUrl, ResourceIDs: resourceIDs, } alter table ai_course_project add resource_ids integer[] default array[]::integer[]\nalter table ai_course_project add read_aloud_time int default 0 not null;\nalter table share_action_record add resource_ids integer[] default array[]::integer[]\nalter table share_action_record add read_aloud_time int default 0 not null;\n"},{"id":2,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/golang_gmp/","title":"GMP","section":"运行时","content":" golang GMP概念 # what: 是golang内部自己实现的调度器，由’‘G’’,“M”,“P\u0026quot;用来调度goruntine，被称为\u0026quot;GMP模型”。\nGMP G:为了调度方便，保存寄存器，栈地址等-\u0026gt;对应cpu切换[1.cpu寄存器的值；2.stack地址] M:与系统线程一一对应 P:一些上下文，比如局部P，防止锁，局部P的heap,也能防止加锁的导致的资源损耗。 why:\n单进程时代不需要调度器\n1.单一的执行流程，计算机只能一个任务一个任务处理。 2.进程阻塞所带来的CPU时间浪费。 多进程/线程时代有了调度器需求\n1.解决了阻塞的问题 2.CPU有很大的一部分都被浪费在进程调度 设计会变得更加复杂，要考虑很多同步竞争等问题，如锁、竞争冲突等。 协程(用户线程)来提高CPU利用率(减少CPU浪费在进程调度上)\n为什么。 线程和进程有很多相同的控制权。线程有自己的信号掩码，可以分配CPU时间，可以放入cgroups，可以查询它们使用了哪些资源。所有这些控制都增加了一些功能的开销，而这些功能对于Go程序如何使用goroutine来说是根本不需要的，而且当你的程序中有10万个线程时，它们很快就会增加。 Go调度器可以做出只在它知道内存是一致的点进行调度的决定。 如何进行调度。 一种是N:1，即在一个操作系统线程上运行几个用户空间线程。这样做的好处是上下文切换非常快，但不能利用多核系统的优势。 另一种是1:1，一个执行线程匹配一个OS线程。它可以利用机器上所有的核心，但是上下文切换很慢，因为它要通过OS进行切换。 M:N调度器。你可以得到快速的上下文切换，你可以利用系统中所有的核心。这种方法的主要缺点是它增加了调度器的复杂性。 摆脱上下文(在这里P就是上下文)？ 不行。我们使用上下文的原因是，如果正在运行的线程由于某些原因需要阻塞，我们可以将它们移交给其他线程。 以前就只有一个全局的P，也可以运行。必须要有P（上下文），是有什么值保存在里面？ why: threads get a lot of the same controls as processes. Threads have their own signal mask, can be assigned CPU affinity, can be put into cgroups and can be queried for which resources they use. All these controls add overhead for features that are simply not needed for how Go programs use goroutines and they quickly add up when you have 100,000 threads in your program. Go调度器可以做出只在它知道内存是一致的点进行调度的决定。 how: One is N:1 where several userspace threads are run on one OS thread. This has the advantage of being very quick to context switch but cannot take advantage of multi-core systems. Another is 1:1 where one thread of execution matches one OS thread. It takes advantage of all of the cores on the machine, but context switching is slow because it has to trap through the OS. M:N scheduler. You get quick context switches and you take advantage of all the cores in your system. The main disadvantage of this approach is the complexity it adds to the scheduler. get rid of contexts? Not really. The reason we have contexts is so that we can hand them off to other threads if the running thread needs to block for some reason. 这个也就说明了N-M的基础，用户线程的各自栈空间其实就是放在公共的堆（heap）上。\n每个系统线程都有一个唯一的m0, g0与之对应，想想为什么？(g0的栈空间与其他的不同，它是放在系统线程的栈空间，**应该是进程的栈空间？**TODO,线程的栈空间)\n每个线程有自己的栈空间(而这个g0就在这个栈上)。但是是与其他的线程公用的代码段，数据段，堆空间,\n所以当创建其他的goroutine的时候，把它的协裎栈在堆上，所以它可以被其他的M调用。\nhow:\nGo调度本质是把大量的goroutine分配到少量系统线程上去执行，并利用多核并行，实现更强大的并发。 通过这一点去记住，把大量goroutine分配到小量线程去尽快执行 复用 并发 防止饥饿 全局G 调度器的有两大思想： 1 压榨系统线程：协程本身就是运行在一组线程之上，不需要频繁的创建、销毁线程，而是对线程的复用。在调度器中复用线程还有2个体现： work stealing(不让它休息)，当本线程无可运行的G时，尝试从其他线程绑定的P偷取G，而不是销毁线程。 hand off(阻塞了，那就换一个压榨)，当本线程因为G进行系统调用阻塞时，线程释放绑定的P，把P转移给其他空闲的线程执行。 利用并行：GOMAXPROCS设置P的数量，当GOMAXPROCS大于1时，就最多有GOMAXPROCS个线程处于运行状态，这些线程可能分布在多个CPU核上同时运行，使得并发利用并行。另外，GOMAXPROCS也限制了并发的程度，比如GOMAXPROCS = 核数/2，则最多利用了一半的CPU核进行并行。 golang并发和并行：Rob Pike一直在强调Go是并发，不是并行，因为Go做的是在一段时间内完成几十万、甚至几百万的工作，而不是同一时间同时在做大量的工作。并发可以利用并行提高效率，调度器是有并行设计的。 调度器的两小策略： 抢占：在coroutine中要等待一个协程主动让出CPU才执行下一个协程，在Go中，一个goroutine最多占用CPU 10ms，防止其他goroutine被饿死，这就是goroutine不同于coroutine的一个地方。 全局G队列：在新的调度器中依然有全局G队列，但功能已经被弱化了，当M执行work stealing从其他P偷不到G时，它可以从全局G队列获取G。 golang GMP概念 FQA g0,m0,gM 说一说GMP的底层实现。 从程序加载到main函数执行的过程。 新的goroutine创建的过程newproc()。 调度的过程schedule()。 archive 附录 FQA # g0,m0,gM # 我们知道g0,m0是开始就创建的，那么后面创建了gM，它是新建一个M，还是使用m0？\n从图中可以看到是使用m0:https://zput.github.io/go-goroutine/part1.%E8%BF%9B%E5%85%A5main%E5%87%BD%E6%95%B0%E4%B9%8B%E5%89%8D%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96/1.%E8%BF%9B%E5%85%A5main%E5%87%BD%E6%95%B0%E4%B9%8B%E5%89%8D%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96.html#mstart 一个M对应一个系统线程，那么你想在程序开始的时候就会进行又创建一个系统线程？\n说一说GMP的底层实现。 # 从程序加载到main函数执行的过程。 # 两条主线 + GMP经典图： 那个经典的GMP在这一阶段实际是怎么表示的，(g0,m0,allp[0]), 代码加载到内存中的code segment, 从入口--\u0026gt;runtime.main--\u0026gt;main.main. 从全局变量的角度，全局变量 (g0,m0,sched); \u0026mdash;\u0026raquo;\u0026gt; sched(midle, pidle, runq(全局运行goroutine queue)). (allg,allm,allp[0,...]); 进行mcall()--\u0026gt;schedule(). 从上面就引出了创建新G，与调度问题：newproc(),与mcall()---\u0026gt;schedule()\n新的goroutine创建的过程newproc()。 # 创造一个新goroutine[go myfunc()]:\n它的最终结果一定是: 就像是goexist()函数调用这个myfunc函数一样。 创造一个新的goroutine前会在全局里面找看是否有gidle的goroutine. 最后会runput到队列中（可能是本地，可能是全局）。 调度的过程schedule()。 # TODO TEXT runtime·gogo(SB), NOSPLIT|NOFRAME, $0-8\n程序到达调度的时机; 选取下一个Goroutine; 全局：globrunqget() 从全局里面拿不是每次只拿一个，而是拿多个，多余一个的放入本地运行G队列(runqput(_p_ *p, gp *g, next bool))。 本地P：runqget() 从其他的P中偷：findrunnable() sched.gcwaiting local runq global runq netpoll steal from other P 切换Goroutine cpu执行选出的Goroutine. 抢占的分类\n抢占又分为：函数抢占，和信号抢占 archive # 谈一谈GMP 我们在C++一般的运行一个线程就是调用系统函数创建一个系统线程。 1-1 N-1 N-M(如果说1-1是) 预先创建一堆线程池，然后用户代码里面接收请求，来一个请求handle一个。 202104月总结 没有按照what，why, how顺序来说: what里面没有阐述G，M，P的实际是什么. why里面没有把握调度这个思想. 单进程里面不需要调度，但是可能进程阻塞。 多线程需要调度，解决了阻塞， 但是调度在内核态，调度花费时间长， 线程笨重，有自己的信号掩码，。。。// TODO 用户态调度，需要调度, 比如协程，goroutine. 由协程与系统线程的关联比例，可以分为[1 : N] [N : 1] [M : N]. how里面要点出主旨：把大量的goroutine高效的分配给少量的系统线程。 二个策略： 压榨系统线程： M执行P队列中的G完了，不让它销毁或者停止，从别的地方拿 当陷入系统调用中，让M-G关联，P重新拿一个M执行剩下的G. P的策略，可控制程序的并行数量，[与实际机器的CPU核数] 两个小策略： 防止饥饿； 保留一个全局P，当局部P满了，可以放入全局P中。 附录 # [1] 我是大彬: Go调度器系列\n[2] Scalable Go Scheduler Design Doc\nhttps://lessisbetter.site/2019/04/14/golang-scheduler-4-explore-source-code/\nhttps://speakerdeck.com/kavya719/the-scheduler-saga?slide=103\nhttps://www.cnblogs.com/CodeWithTxT/p/11370215.html#:~:text=%E4%B8%BA%E4%BA%86%E6%9B%B4%E5%90%88%E7%90%86%E7%9A%84%E5%88%A9%E7%94%A8,%E5%88%86%E5%9C%B0%E5%88%A9%E7%94%A8CPU%E3%80%82\n"},{"id":3,"href":"/go-goroutine/docs/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/golang_basic/","title":"基本用法","section":"基础知识","content":" Golang # Golang 数据结构 string 单引号 slice和数组的异同 slice的自动扩容后的大小是多少? 延伸 map 哈希扩容 扩容规则 struct和内存对齐 chan error.Unwrap 跟类型相关 类型系统 跟函数定义的结构相关 函数调用栈细节 闭包 方法(method) defer三阶段 before 1.13 测试题 1.13 1.14 panic AND recover 测验 跟interface相关 接口 eface iface 类型断言 reflect reflect.Type reflect.Value 运行时: 并发编程 同步原语 内存顺序保证 sync包 context(上下文) 内存管理 GC 逃逸分析 内存泄漏 附录 0.方法 1. new And make example 2. 值类型和引用类型的区别 3. 计算golang中类型的大小的方式 golang的chan archive \u0026lsquo;\u0026lsquo;里面的是chan的状态(eg: 一个零值nil通道;一个非零值但已关闭的通道)\n\u0026lsquo;空\u0026rsquo;读写阻塞-关闭恐慌; \u0026lsquo;关闭\u0026rsquo;读为0-关闭写恐慌. 如果是有缓存的chan已关闭，且现在缓存不为空,读正常得到数据 1\n数据结构:\nstring slice和数组的异同 map struct和内存对齐 error.Unwrap 跟类型相关：\n类型系统 跟函数定义的结构相关(function value) 函数调用栈细节 闭包 方法(method):method expression/method value defer三阶段 panic AND recover 跟interface相关 接口 类型断言 reflect 运行时:\n并发编程 同步原语 内存顺序保证 sync包 context(上下文) 定时器 GMP 内存管理 GC 逃逸 内存分配 内存泄漏 数据结构 # 这些数据结构的底层是怎么实现的,比如array,它就是一个指针,指向第一个元素的地址. 1.1 上面已经能想到个大概. 然后就是一个细节,如果要实现这个数据结构的一些功能,比如chan读写,map的添加删除等,它们是怎么利用数据结构里面底层的field来实现这些功能? string # string类型的底层结构.它的大小是几个字节?\n指针和字节大小\n0xxxxxxx 110xxxxx 10xxxxxx 1110xxxx 10xxxxxx 10xxxxxx package main import ( \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; \u0026#34;unicode/utf8\u0026#34; \u0026#34;unsafe\u0026#34; ) func String2Bytes(s string) []byte { sh := (*reflect.StringHeader)(unsafe.Pointer(\u0026amp;s)) bh := reflect.SliceHeader{ Data: sh.Data, Len: sh.Len, Cap: sh.Len, } return *(*[]byte)(unsafe.Pointer(\u0026amp;bh)) } func Bytes2String(b []byte) string { return *(*string)(unsafe.Pointer(\u0026amp;b)) } func main(){ var str = \u0026#34;hello 就是\u0026#34; fmt.Println(\u0026#34;---\u0026gt;\u0026#34;, len(str)) fmt.Println(\u0026#34;---\u0026gt;\u0026#34;, len([]rune(str))) // 报错cannot convert str (type string) to type []int64 //fmt.Println(\u0026#34;---\u0026gt;\u0026#34;, len([]int64(str))) //golang中的unicode/utf8包提供了用utf-8获取长度的方法 fmt.Println(\u0026#34;RuneCountInString:\u0026#34;, utf8.RuneCountInString(str)) } /* string 的底层就是uint8的数组指针+长度 预期想得到一个字符串的长度而不是字符串底层占得字节长度： 1。 utf8.RuneCountInString()函数 2。 []rune(stringxxx) or []int32(stringxxx) */ 345. 反转字符串中的元音字母 单引号 # 单引号，表示byte类型或rune类型，对应 uint8和int32类型，默认是 rune 类型。byte用来强调数据是raw data，而不是数字；而rune用来表示Unicode的code point。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; \u0026#34;unsafe\u0026#34; ) func main() { //String in double quotes x := \u0026#34;tit\\nfor\\ttat\u0026#34; fmt.Println(\u0026#34;Priting String in Double Quotes:\u0026#34;) fmt.Printf(\u0026#34;x is: %s\\n\u0026#34;, x) //String in back quotes y := `tit\\nfor\\ttat` fmt.Println(\u0026#34;\\nPriting String in Back Quotes:\u0026#34;) fmt.Printf(\u0026#34;y is: %s\\n\u0026#34;, y) //Declaring a byte with single quotes var b byte = \u0026#39;a\u0026#39; fmt.Println(\u0026#34;\\nPriting Byte:\u0026#34;) //Print Size, Type and Character fmt.Printf(\u0026#34;Size: %d\\nType: %s\\nCharacter: %c\\n\u0026#34;, unsafe.Sizeof(b), reflect.TypeOf(b), b) //Declaring a rune with single quotes r := \u0026#39;£\u0026#39; fmt.Println(\u0026#34;\\nPriting Rune:\u0026#34;) //Print Size, Type, CodePoint and Character fmt.Printf(\u0026#34;Size: %d\\nType: %s\\nUnicode CodePoint: %U\\nCharacter: %c\\n\u0026#34;, unsafe.Sizeof(r), reflect.TypeOf(r), r, r) //Below will raise a compiler error - invalid character literal (more than one character) //r = \u0026#39;ab\u0026#39; } https://golangbyexample.com/double-single-back-quotes-go/ slice和数组的异同 # Go中有两种取子切片的语法形式（假设baseContainer是一个切片或者数组）：\nbaseContainer[low : high] // 双下标形式 baseContainer[low : high : max] // 三下标形式 上面所示的双下标形式等价于下面的三下标形式：baseContainer[low : high : cap(baseContainer)]\n子切片表达式的结果切片的长度为high - low、容量为max - low。\ngolang数组是一个指向数组头地址的指针,但是如果你引用超过数组的长度,它会编译不通过.\n如果使用reflect来reflect.ValueOf(array).Index(out_of_range_index)它会在运行时panic(我猜时因为interface{}中_type的类型指明数组的大小是多少,所以会有一个判断.) type arraytype struct type arraytype struct { typ _type elem *_type slice *_type len uintptr } slice的自动扩容后的大小是多少? # 分三步来判断:\nvar newCap int if oldCap*2\u0026lt;nowNeedCap{ newCap = nowNeedCap }else{ if oldCap\u0026lt;1024 { newCap =oldCap*2 }else{ newCap =oldCap*1.25 } } realNewCap := compare(内存管理模块的细分大小, newCap*EveryElementSize) 延伸 # map # map的底层是哈希表 哈希表的得到buctet的位置一般通过取模,与运算法 hash%m hash\u0026amp;(m-1) //其中m一定要是2的整数倍,然后它减一,才会使它的后面的值都是1.否则就会导致低位buctet不会被选中的情况. 哈希冲突 开放寻址法 拉链法 跳表 建立新bucket,慢慢迁移到新bucket. 哈希扩容 # 扩容规则 # struct和内存对齐 # 懂得内存为什么要对齐,如果不对齐会怎么样?\n不对齐,可能从内存中读一个数据需要读两次,因为总线的原因 所以看一个类型的内存对齐,总的原则是不要读两次,尽量不要浪费内存.\n比如32位(4byte) 下: 小于4byte的类型以他们自己的类型长度为内存对齐. 大于等于4byte以4byte. 一个struct类型,他的内存对齐为里面field中最大的那个. 判断一个struct在内存中占用的字节数,\n先按照它的各个field以struct的开始地址作为0,开始一个个排好, 最后要符合这个struct类型的内存对齐. test type T1 struct { f1 int8 // 1 byte f2 int64 // 8 bytes f3 int32 // 4 bytes } testMe\n24 byte chan # what: 从外部看进来.\n不要通过共享内存的方式进行通信，而是应该通过通信的方式共享内存(即通信顺序进程（Communicating sequential processes，CSP）) |thread A|--------[memory]--------|thread B| |goroutine A|-----[channel]-------|goroutine B| 先进先出（FIFO：first in first out）数据队列 可以把一个通道看作是在一个程序内部的一个先进先出（FIFO：first in first out）数据队列。 一些协程可以向此通道发送数据，另外一些协程可以从此通道接收数据。 有锁管道 Channel是一个用于同步和通信的有锁队列，使用互斥锁解决程序中可能存在的线程竞争问题是很常见的，我们能很容易地实现有锁队列。 CAS实现的无锁Channel没有提供先进先出的特性，所以该提案暂时也被搁浅了. why: 内部结构细节.\n结构 type hchan struct { qcount uint // - 数组长度，即已有元素个数 dataqsiz uint // - 数组容量，即可容纳元素个数 buf unsafe.Pointer // - 数组地址 elemsize uint16 // - 元素大小 closed uint32 elemtype *_type // 元素类型 golang运行时中，内存复制、垃圾回收等机制依赖数据的类型信息，所以hchan中还要有一个指针，指向元素类型的类型元数据 sendx uint // - 下一次写下标位置 recvx uint // - 下一次读下标位置 recvq waitq // 读等待队列 sendq waitq // 写等待队列 lock mutex } type waitq struct { first *sudog last *sudog } 前面有-的就是无缓冲chan不需要的字段。 sudog\nhow: chan的实践，一般用它来干什么？\n五种操作： close(ch) ch \u0026lt;- v \u0026lt;-ch v = \u0026lt;-ch v, sentBeforeClosed = \u0026lt;-ch cap(ch) len(ch) https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-channel/#fnref:12\nhttps://docs.google.com/presentation/d/1EUOK5qZBdzKTUGbI3iMhHnhwI60pVsg8ISm3jB1D_K8/edit#slide=id.g138e72fb17_0_0\nhttps://golang.design/under-the-hood/zh-cn/part1basic/ch03lang/chan/#select- https://docs.google.com/presentation/d/18_9LcMc8u93aITZ6DqeUfRvOcHQYj2gwxhskf0XPX2U/edit#slide=id.g5ea99f63e9_0_26\nerror.Unwrap # 通过消除错误来消除错误处理 Errors are values 错误只处理一次 type errWriter struct { w io.Writer err error } func (ew *errWriter) write(buf []byte) { if ew.err != nil { return } _, ew.err = ew.w.Write(buf) } ew := \u0026amp;errWriter{w: fd} ew.write(p0[a:b]) ew.write(p1[c:d]) ew.write(p2[e:f]) // and so on if ew.err != nil { return ew.err } 将错误视为不透明的值对于生成松散耦合的软件很重要，所以如果对错误值所做的唯一事情是如下两个方面的话，则原始错误是什么类型就无关紧要了。 检查是否为 nil 打印或记录日志 但是，在某些场景，您确实需要恢复原始错误, 然后再判断的场景吗。 向错误添加上下文 fmt.Errorf 防止原始的错误类型被掩盖\nerror.Unwrap 错误发生一般是从外部，比如数据库，缓存，其他调用第三方导致的。 [一般有error code] ---来源---\u0026gt; [转换成中间值] ---去向---\u0026gt; [最后变成输出给外面的error code AND error message] 其中中间值是在自己系统中统一的，只要来源到这个系统的，都要先把错误码转为系统的中间值.\n跟类型相关 # what:都需要思考在内存中怎么表示的? 类型系统 # type mType struct{ x int64 } func (mType) Method(){ } var mObject mType 我们知道mObject内存布局代表的是一块内存,有8个字节，那么mObject怎么调用mObject.Method()? mObject.Method() \u0026mdash;\u0026gt; Method(mObject)调用方法，就是调用函数，对象自己当作第一个参数,但是这仅仅是在编译阶段，编译器可以进行转换， 但是为了支持在运行阶段的语言特性，比如反射，接口动态派发、类型断言,所以编译器会给每种类型生成对应的类型描述信息写入可执行文件，这些类型描述信息就是类型元数据,方便在运行时操作. type myslice []string func (ms myslice) Len(){ fmt.Println(len(ms)) } func (ms myslice) Cap(){ fmt.Println(cap(ms)) } type slicetype struct { typ _type elem *_type } type slicetype struct\n其中_type type _type struct 其中uncommontype type uncommontype struct rune与int32就是别名关系. // TODO 如何比较它们是否相等?\n跟函数定义的结构相关 # Function Value本质上是一个指针，却不直接指向函数指令入口，而是指向runtime.funcval结构体,然后再由这个结构体来指向函数指令入口。 为了闭包. type funcval struct { fn uintptr } 【Golang】图解Function value\n函数调用栈细节 # 为什么被调用函数, 改变不了caller的传给callee的参数? 如果一个函数调用几个函数,且他们的形参个数不一, 大小不一, 那么这个caller的栈大小,是: 自己的参数+最大(参数字节和)的callee的值. 闭包 # what:在内存中怎么表示的 闭包有自由变量(函数外定义,函数内被引用). go语言里面返回一个闭包函数其实就是:一个funcval结构体加上捕获的变量. 只是如果这个变量在赋给初值以后没有再改变,与后面又被改变两种情况. why: how: 通过有捕获列表的Function Value实现. 方法(method) # // TODO\ndefer三阶段 # what: defer在内存中怎么表示的? 调用defer B(a)[a是int64]\u0026mdash;\u0026gt;func deferproc(size int32, fn *funcval)\u0026mdash;\u0026gt;deferproc(8, B's funcval Addr),就变成了调用deferproc函数. func deferproc(siz int32, fn *funcval) why: how: 关键字return不是一个原子操作，实际上return只代理汇编指令 ret，即将跳转程序执行。比如语句return i，实际上分两步进行，即将i值存入栈中作为返回值，然后执行跳转 只要懂得defer函数其实会被编译器编译成deferproc function。\nbefore 1.13 # deferproc函数填充一个 _defer结构体,参数注册时候会拷贝到堆上,执行时候又拷贝到栈上.\n生成的_defer struct放到g._defer链表,且每次都放链表头,这样串联起来.\ng._defer\n测试题 # 1.13 # 1.13如何解决慢的问题? why:为什么慢?\nhow:\n不让_defer结构体参数在栈和堆之间进行拷贝,直接在栈上定义_defer结构体,和参数. func deferprocStack(d *_defer) package main import \u0026#34;testing\u0026#34; func BenchmarkDefer(b *testing.B){ for i:=0; i\u0026lt; b.N; i++{ Defer(i) } } func Defer(i int)(r int){ defer func(){ r -= 1 r |= r\u0026gt;\u0026gt;1 r |= r\u0026gt;\u0026gt;2 r |= r\u0026gt;\u0026gt;4 r |= r\u0026gt;\u0026gt;8 r |= r\u0026gt;\u0026gt;16 r |= r\u0026gt;\u0026gt;32 r += 1 }() r = i*i return } 1.14 # 1.14如何解决慢的问题? why:为什么慢? how: 直接省去上面两个步骤: 省去构造_defer链表项,并注册到g._defer链表的过程. 把defer后面函数需要的参数定义为局部变量,然后在函数返回前直接调用defer后面的函数. 如果需要到执行阶段才能确定是否需要被调用执行, go使用标识变量df来解决这个问题.var df bytedf中的每一位标识一个defer函数是否需要被执行\t. 思考为什么不支持循环defer? panic AND recover # first: panic执行defer函数的方式:先标记后释放,为了终止之前发生的panic. second: 异常信息的输出方式:所有还在panic链表上的项都会被输出.\nwhat: panic()函数执行的时候也是填充一个 type _panic struct结构体,放入goroutine's _panic链表中,然后不再执行panic后面的代码,返回,就开始检查goroutine's _defer链表,如果发现_defer\u0026rsquo;s SP是这个panic所在函数的,就先置started为true,_panic指针指向导致这个defer运行的panic(这是为了当defer里面又有其他panic).当defer执行完后再执行g._panic从链表尾开始输出. 填充一个_panic结构体 不执行panic函数后面的代码, 执行_defer链表。 why: how: panic\u0026mdash;\u0026gt;defer\u0026ndash;\u0026gt;panic recover what: recover只把当前的_panic.recoved设置为true. 然后panic流程会在每个defer执行完毕后,检查次panic是否已经恢复,如果恢复就把它从g._panic链表中移除. why: how: 当_panic被移除后,我们需要跳出panic流程,我们就恢复到defer执行流程(利用它里面的SP, PC),最后ret==1,跳转到 func deferreturn(arg0 uintptr) func deferreturn(arg0 uintptr) { //... sp := getcallersp() if d.sp != sp { // 如果g._defer链表中头的SP不相等(说明不是这个函数定义的defer)就跳出,不再继续执行. return } 补充一下，上面可能很难理解，看下图，它有两个runtime.deferreturn(),如果函数是正常结束，那么会执行第一个runtime.deferreturn()。如果因为中间panic, 填充一个_panic结构体; 不执行panic函数后面的代码, 执行_defer链表。 当_defer链表中某一个_defer有recover函数 recover只把当前的_panic.recoved设置为true. 然后panic流程会在每个defer执行完毕后,检查次panic是否已经恢复,如果恢复就把它从g._panic链表中移除. 此时，panic被移除了，\u0026mdash;\u0026gt; 需要跳出这个panic流程 恢复_defer链表中的sp,它就到了下图中的注册~~~~~\u0026gt;这个地方。 返回1，所以跳到第二个runtime.deferreturn(). // TODO 如何恢复？ https://goplay.tools/snippet/qoxyANNV481\n测验 # 它的输出结果是什么?\npackage main import ( \u0026#34;fmt\u0026#34; ) func deferA() { fmt.Println(\u0026#34;deferA\u0026#34;) panic(\u0026#34;panicA\u0026#34;) } func main() { defer deferA() fmt.Println(\u0026#34;Hello World\u0026#34;) panic(\u0026#34;main\u0026#34;) } https://goplay.tools/snippet/G5zFPvuiEdJ\npackage main import ( \u0026#34;fmt\u0026#34; ) func A() { defer A1() defer A2() panic(\u0026#34;panicA\u0026#34;) } func A1() { fmt.Println(\u0026#34;A1\u0026#34;) } func A2() { defer B1() panic(\u0026#34;panicA2\u0026#34;) } func B1() { p := recover() fmt.Println(\u0026#34;B1---\u0026gt;\u0026#34;, p) } func main() { A() } https://goplay.tools/snippet/phXUi8KDi61\n跟interface相关 # 接口 # eface iface interfacetype eface # 其中_type type _type struct\niface # type interfacetype struct {\ntype interfacetype struct { typ _type pkgpath name mhdr []imethod } 如果接口类型与动态类型确定了,那么itab的就固定了,此时它是可以复用的. go会把itab缓存起来. 类型断言 # reflect # reflect.Type # what: TypeOf // TypeOf returns the reflection Type that represents the dynamic type of i. // If i is a nil interface value, TypeOf returns nil. func TypeOf(i interface{}) Type { eface := *(*emptyInterface)(unsafe.Pointer(\u0026amp;i)) return toType(eface.typ) }} why: how: TypeOf它的输入参数是eface,返回参数是iface类型(因为 type Type interface是一个非空接口类型.) 所以需要转换一下,下图有说明. confliction:\n空接口类型的参数只能接收地址的需求.\nTOOD:感觉这个地方与reflect不能Set成员函数也有关系.\n因为reflect.Type也是一个非空接口类型, 它有许多方法.所以使用上节所用的非空接口类型。 reflect.Value # type Value struct { // typ holds the type of the value represented by a Value. typ *rtype ptr unsafe.Pointer // The lowest bits are flag bits: //\t- flagStickyRO: obtained via unexported not embedded field, so read-only //\t- flagEmbedRO: obtained via unexported embedded field, so read-only //\t- flagIndir: val holds a pointer to the data //\t- flagAddr: v.CanAddr is true (implies flagIndir) //\t- flagMethod: v is a method value. flag } flagIndir的解释\n注意点：rtype这个结构体与 ../runtime/type.go:/^type._type是要保持一致的(一一对应)。\n继续来看下这个unpackEface\n// unpackEface converts the empty interface i to a Value. func unpackEface(i interface{}) Value { e := (*emptyInterface)(unsafe.Pointer(\u0026amp;i)) // NOTE: don\u0026#39;t read e.word until we know whether it is really a pointer or not. t := e.typ if t == nil { return Value{} } // 下面的这些主要是构建一个Value结构体中的一个字段flag f := flag(t.Kind())// 这里t.Kind()就是rtype或者说_type中的kind字段 if ifaceIndir(t) { f |= flagIndir } return Value{t, e.word, f} } func (t *rtype) Kind() Kind { return Kind(t.kind \u0026amp; kindMask) } const kindMask = (1 \u0026lt;\u0026lt; 5) - 1 //取最后的五个bits。 //最终再强制转为flag类型---》 rtype使用Kind方法可以得到实际的Kind值 //我们看下flag得到Kind值 func (f flag) kind() Kind { return Kind(f \u0026amp; flagKindMask) } const flagKindMask flag = 1\u0026lt;\u0026lt;flagKindWidth - //是一模一样的。 // -------------------------- // -------------------------- // -------------------------- 运行时: # 并发编程 # 同步原语 # what: 并发同步是指如何控制若干并发计算（在Go中，即协程），从而 避免在它们之间产生数据竞争的现象； 避免在它们无所事事的时候消耗CPU资源。 并发同步有时候也称为数据同步。 why: 避免在它们之间产生数据竞争的现象； 避免在它们无所事事的时候消耗CPU资源。 how: 通道用例大全\n如何优雅地关闭通道\nsync标准库包中提供的并发同步技术\nsync.WaitGroup（等待组）类型: ：Add(delta int)、Done()和Wait() sync.Once类型: Do(f func())方法 一个sync.Once值被用来确保一段代码在一个并发程序中被执行且仅被执行一次 sync.Mutex（互斥锁）和sync.RWMutex（读写锁）类型: 实现了sync.Locker接口类型。 所以这两个类型都有两个方法：Lock()和Unlock()， 用来保护一份数据不会被多个使用者同时读取和修改。 *sync.RWMutex类型还有两个另外的方法：RLock()和RUnlock()， 用来支持多个读取者并发读取一份数据但防止此份数据被某个数据写入者和其它数据访问者（包括读取者和写入者）同时使用 sync.Cond类型:提供了一种有效的方式来实现多个协程间的通知。 成员： sync.Locker类型的名为L的字段 成员函数： Wait()、Signal()和Broadcast()。 每个Cond值维护着一个先进先出等待协程队列。 对于一个可寻址的Cond值c， c.Wait()必须在c.L字段值的锁被成功获取的时候调用；否则，c.Wait()调用将造成一个恐慌。 一个c.Wait()调用将 首先将当前协程入到c所维护的等待协程队列； 然后调用c.L.Unlock()释放c.L的锁； 然后使当前协程进入阻塞状态；（当前协程将被另一个协程通过c.Signal()或c.Broadcast()调用唤醒而重新进入运行状态。） 一旦当前协程重新进入运行状态，c.L.Lock()将被调用以试图重新获取c.L字段值的锁。 此c.Wait()调用将在此试图成功之后退出。 一个c.Signal()调用将唤醒并移除c所维护的等待协程队列中的第一个协程（如果此队列不为空的话）。 一个c.Broadcast()调用将唤醒并移除c所维护的等待协程队列中的所有协程（如果此队列不为空的话）。 sync/atomic标准库包中提供的原子操作技术\nT\u0026mdash;可以换成\u0026mdash;\u0026gt;内置int32、int64、uint32、uint64和uintptr类型 func AddT(addr *T, delta T)(new T) func LoadT(addr *T) (val T) func StoreT(addr *T, val T) func SwapT(addr *T, new T) (old T) func CompareAndSwapT(addr *T, old, new T) (swapped bool) 我们也可以利用网络和文件读写来做并发同步，但是这样的并发同步方法使用在一个程序进程内部时效率相对比较低。 一般来说，这样的方法多适用于多个进程之间或多个主机之间的并发同步。《Go语言101》中不介绍这样的并发同步方法。\n内存顺序保证 # Go中的内存顺序保证 内存顺序 what: 调整指令执行顺序，从而使得指令执行顺序和代码中指定的顺序不太一致。 指令顺序也称为内存顺序。 why: how: 通道操作相关的顺序保证: 一个通道上的第n次成功发送操作的开始发生在此通道上的第n次成功接收操作完成之前，无论此通道是缓冲的还是非缓冲的。 如果是缓冲，一边是发送，数据进入到通道中，当另一边接收的时候，肯定在前一边之后。 一个容量为m通道上的第n次成功接收操作的开始发生在此通道上的第n+m次发送操作完成之前。 特别地，如果此通道是非缓冲的（m == 0），则此通道上的第n次成功接收操作的开始发生在此通道上的第n次发送操作完成之前。 一个通道的关闭操作发生在任何因为此通道被关闭而从此通道接收到了零值的操作完成之前。 事实上， 对一个非缓冲通道来说，其上的第n次成功发送的完成和其上的第n次成功接收的完成应被视为同一事件。 好像他们就是保证当阻塞发送接收的时候，他们可以当成一个事件，同时发生的。 example: sync包 # sync.Map\nsync.Map的核心实现 - 两个map，一个用于写，另一个用于读，这样的设计思想可以类比缓存与数据库 sync.Map的局限性 - 如果写远高于读，dirty-\u0026gt;readOnly 这个类似于 刷数据 的频率就比较高，不如直接用 mutex + map 的组合 sync.Map的设计思想 - 保证高频读的无锁结构、空间换时间 sync.Cond\nsync.Cond的核心实现 - 通过一个锁，封装了notify 通知的实现，包括了单个通知与广播这两种方式 sync.Cond与channel的异同 - channel应用于一收一发的场景，sync.Cond应用于多收一发的场景 sync.Cond的使用探索 - 多从专业社区收集意见 https://github.com/golang/go/issues/21165 sync.Pool\nsync.Pool的核心作用 - 读源码，缓存稍后会频繁使用的对象+减轻GC压力 sync.Pool的Put与Get - Put的顺序为local private-\u0026gt; local shared，Get的顺序为 local private -\u0026gt; local shared -\u0026gt; remote shared 思考sync.Pool应用的核心场景 - 高频使用且生命周期短的对象，且初始化始终一致，如fmt 探索Go1.13引入victim的作用 - 了解victim cache的机制 context(上下文) # 1 // TODO\n四个方法 type Context interface { Deadline() (deadline time.Time, ok bool) Done() \u0026lt;-chan struct{} //返回一个channel. Err() error Value(key interface{}) interface{} } 内部Ctx cancelCtx WithCancel: 把父Context装进子Context\u0026rsquo;s Context. func WithCancel(parent Context) (ctx Context, cancel CancelFunc) { if parent == nil { panic(\u0026#34;cannot create context from nil parent\u0026#34;) } c := newCancelCtx(parent) // 常见一个新的子context； propagateCancel(parent, \u0026amp;c) // 然后准备把新生成的子context加入到parent中； return \u0026amp;c, func() { c.cancel(true, Canceled) } } 目前 context 有个非常重要点, 在 Go 只有阻塞式调用没有非阻塞调用，所以必须要有 context cancel (or Done() or Err()) 相关操作，否则操作一旦陷进去没响应 (本质是 goroutines 调度控制)，goroutine 就无法继续了. func main() { ctx,cancel := context.WithCancel(context.Background()) go Speak(ctx) time.Sleep(10*time.Second) cancel() time.Sleep(1*time.Second) } func Speak(ctx context.Context) { for range time.Tick(time.Second){ select { case \u0026lt;- ctx.Done(): fmt.Println(\u0026#34;我要闭嘴了\u0026#34;) return default: fmt.Println(\u0026#34;balabalabalabala\u0026#34;) } } } https://gist.github.com/zput/27591a68b2384b63e14c92546394b6e7 https://gist.github.com/zput/d798e8e43498944e3e77111198cdefde valueCtx WithValue: emptyCtx 这里需要说明的是type interface可以放进struct里面：\n内存管理 # GC # 逃逸分析 # what:\n指针（或者引用）的逃逸(Escape): 当变量（或者对象）在方法中分配后，其指针有可能被返回或者被全局引用，这样就会被其他过程或者线程所引用. 逃逸分析: 是一种确定指针动态范围的方法，可以分析在程序的哪些地方可以访问到指针。 必然发生逃逸: 在某个函数中new或字面量创建出的变量，将其指针作为函数返回值，则该变量一定发生逃逸（构造函数返回的指针变量一定逃逸）； 被已经逃逸的变量引用的指针，一定发生逃逸； type User struct { Username string Password string Age int } func main() { a := \u0026#34;aaa\u0026#34; u := \u0026amp;User{a, \u0026#34;123\u0026#34;, 12} Call1(u) } func Call1(u *User) { fmt.Printf(\u0026#34;%v\u0026#34;,u) } /* func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error) { p := newPrinter() p.doPrintf(format, a) ... } func (p *pp) doPrintf(format string, a []interface{}) { ... p.printArg(a[argNum], rune(c)) ... } func (p *pp) printArg(arg interface{}, verb rune) { p.arg = arg p.value = reflect.Value{} ... } */ //-------------上面逃逸，下面没有------------------// func main() { a := \u0026#34;aaa\u0026#34; u := \u0026amp;User{a, \u0026#34;123\u0026#34;, 12} Call1(u) } func Call1(u *User) int { return Call2(u) } func Call2(u *User) int { return Call3(u) } func Call3(u *User) int { u.Username = \u0026#34;bbb\u0026#34; return u.Age * 20 } /* go run -gcflags \u0026#34;-m -l\u0026#34; main.go # command-line-arguments ./main.go:23:12: Call3 u does not escape ./main.go:19:12: Call2 u does not escape ./main.go:15:12: Call1 u does not escape ./main.go:11:23: main \u0026amp;User literal does not escape */ 被指针类型的slice、map和chan引用的指针，一定发生逃逸； func main() { a := make([]*int,1) b := 12 a[0] = \u0026amp;b c := make(map[string]*int) d := 14 c[\u0026#34;aaa\u0026#34;]=\u0026amp;d e := make(chan *int,1) f := 15 e \u0026lt;- \u0026amp;f } /* go run -gcflags \u0026#34;-m -l\u0026#34; main.go # command-line-arguments ./main.go:7:2: moved to heap: b ./main.go:11:2: moved to heap: d ./main.go:15:2: moved to heap: f ./main.go:6:11: main make([]*int, 1) does not escape ./main.go:10:11: main make(map[string]*int) does not escape */ 必然不会逃逸的情况： 指针被未发生逃逸的变量引用； 仅仅在函数内对变量做取址操作，而未将指针传出； 可能发生逃逸，也可能不会发生逃逸： 将指针作为入参传给别的函数；这里还是要看指针在被传入的函数中的处理过程，如果发生了上边的三种情况，则会逃逸；否则不会逃逸； why:\ngc压力 //我：“golang函数传参是不是应该跟c一样，尽量不要直接传结构体，而要传结构体指针？“ //leader：“不对，咱们项目很多都是直接传结构体的。“ //我：“那样不会造成不必要的内存copy开销吗？” //leader：“确实会有，但这样可以减小gc压力，因为传值会在栈上分配，而一旦传指针，结构体就会逃逸到堆上。“ 不逃逸的对象放栈上，可能逃逸的放堆上: gc的压力: 最大的好处应该是减少gc的压力，不逃逸的对象分配在栈上，当函数返回时就回收了资源，不需要gc标记清除。 因为逃逸分析完后可以确定哪些变量可以分配在栈上，栈的分配比堆快，性能好 同步消除，如果你定义的对象的方法上有同步锁，但在运行时，却只有一个线程在访问，此时逃逸分析后的机器码，会去掉同步锁运行。 how:\n-gcflags '-m -l' 从内部函数开始分析，它的好处的是，如果底部的函数中都没有逃逸，那么最上层的变量就直接放入栈里面就行了，不需要放堆里。 这里的objS没有逃逸，而m却逃逸了，这是因为go的逃逸分析不知道objS和m的关系，逃逸分析不知道参数M是struct S的一个成员，所以只能把它分配给堆。 Golang逃逸分析 GoLang-逃逸分析\nhttps://zhuanlan.zhihu.com/p/91559562\n内存泄漏 # what:\n当使用一门支持自动垃圾回收的语言编程时，一般来说我们不需要关心内存泄露问题，因为程序的运行时会负责回收不再使用的内存。 但是，我们确实也需要知道一些特殊的可能会造成暂时性或永久性内存泄露的情形。 why:\n临时性内存泄露 底层共用同一个内存空间引起的： 子字符串造成的暂时性内存泄露 var s0 []int func g(s1 []int) { // 假设s1的长度远大于30。 s0 = s1[len(s1)-30:] } 子切片造成的暂时性内存泄露 因为未重置丢失的切片元素中的指针而造成的临时性内存泄露 func h() []*int { s := []*int{new(int), new(int), new(int), new(int)} // 使用此s切片 ... //s[0], s[len(s)-1] = nil, nil // 添加这一行:重置首尾元素指针 return s[1:3:3] } 延迟调用函数导致的临时性内存泄露 func writeManyFiles(files []File) error { for _, file := range files { f, err := os.Open(file.path) if err != nil { return err } defer f.Close() _, err = f.WriteString(file.content) if err != nil { return err } err = f.Sync() if err != nil { return err } } return nil } 永久性内存泄露 因为协程被永久阻塞而造成的永久性内存泄露 泄露的场景不仅限于以下两类，但因channel相关的泄露是最多的。 channel的读或者写： \u0026lsquo;空\u0026rsquo;读写阻塞-关闭恐慌; \u0026lt;\u0026mdash;\u0026gt; \u0026lsquo;关闭\u0026rsquo;读为0-关闭写恐慌.如果是有缓存的chan已关闭，且现在缓存不为空,读正常得到数. 写： 无缓冲channel的阻塞通常是写操作因为没有读而阻塞 有缓冲的channel因为缓冲区满了，写操作阻塞 读：期待从channel读数据，结果没有goroutine写 select操作，select里也是channel操作，如果所有case上的操作阻塞，goroutine也无法继续执行。 default 因为没有停止不再使用的time.Ticker值而造成的永久性内存泄露 因为不正确地使用终结器（finalizer）而造成的永久性内存泄露 how: 怎么发现内存泄露,\n在Go中发现内存泄露有2种方法: 一个是通用的监控工具; 另一个是go pprof。 当连接在服务器终端上的时候，是没有浏览器可以使用的，Go提供了命令行的方式，能够获取以上5类信息，这种方式用起来更方便。\n使用命令go tool pprof url可以获取指定的profile文件，此命令会发起http请求，然后下载数据到本地，之后进入交互式模式(只要进入到交互模式说明已经下载完成，与将要调查的程序无关了).\n# 下载cpu profile，默认从当前开始收集30s的cpu使用情况，需要等待30s go tool pprof http://localhost:6060/debug/pprof/profile # 30-second CPU profile go tool pprof http://localhost:6060/debug/pprof/profile?seconds=120 # wait 120s # 下载heap profile go tool pprof http://localhost:6060/debug/pprof/heap # heap profile # 下载goroutine profile go tool pprof http://localhost:6060/debug/pprof/goroutine # goroutine profile # 下载block profile go tool pprof http://localhost:6060/debug/pprof/block # goroutine blocking profile # 下载mutex profile go tool pprof http://localhost:6060/debug/pprof/mutex top 按前面下载的指标是什么，按照它们的大小列出前10个函数，比如heap是按heap占用多少。 list function_name 查看某个函数的代码，以及该函数每行代码的指标信息，如果函数名不明确，会进行模糊匹配，比如list main会列出main.main和runtime.main。 traces 打印所有调用栈，以及调用栈的指标信息。 // 隔一段时间分别运行，抓取它的heap信息. go tool pprof http://localhost:6060/debug/pprof/heap // 使用-base Options来以001作为基准，与003来比较。 go tool pprof -base pprof.main.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz pprof.main.alloc_objects.alloc_space.inuse_objects.inuse_space.003.pb.gz // top top // list 上面展示的函数，查看具体代码行。 很像gdb list xxxxx https://segmentfault.com/a/1190000019222661 https://gfw.go101.org/article/memory-leaking.html\n附录 # [1] go101\n[2] code_reading\n[3] Go 语言设计与实现\n0.方法 # 为指针类型属主隐式声明的方法\n每个方法对应着一个隐式声明的函数 类型T的方法集总是类型*T的方法集的子集。 如何决定一个方法声明使用值类型属主还是指针类型属主？\npackage main import \u0026#34;fmt\u0026#34; type Book struct { pages int } type Books []Book func (books *Books) Modify() { *books = append(*books, Book{789}) (*books)[0].pages = 500 } func main() { var books = Books{{123}, {456}} books.Modify() fmt.Println(books) // [{500} {456} {789}] } (\u0026amp;book).SetPages(123)一行为什么可以被简化为book.SetPages(123)呢？\n毕竟，类型Book并不拥有一个SetPages方法。 这可以看作是Go中为了让代码看上去更简洁而特别设计的语法糖。 此语法糖只对可寻址的值类型的属主有效。 编译器会自动将book.SetPages(123)改写为(\u0026amp;book).SetPages(123)。 但另一方面，我们应该总是认为aBookExpression.SetPages是一个合法的选择器（从语法层面讲），即使表达式aBookExpression被估值为一个不可寻址的Book值（在这种情况下，aBookExpression.SetPages是一个无效但合法的选择器）。 package main import \u0026#34;fmt\u0026#34; type Book struct { pages int } func (b Book) Pages() int { return b.pages } func (b *Book) SetPages(pages int) { b.pages = pages } func main() { var book Book fmt.Printf(\u0026#34;%T \\n\u0026#34;, book.Pages) // func() int fmt.Printf(\u0026#34;%T \\n\u0026#34;, (\u0026amp;book).SetPages) // func(int) // \u0026amp;book值有一个隐式方法Pages。 fmt.Printf(\u0026#34;%T \\n\u0026#34;, (\u0026amp;book).Pages) // func() int // 调用这三个方法。 (\u0026amp;book).SetPages(123) book.SetPages(123) // 等价于上一行 fmt.Println(book.Pages()) // 123 fmt.Println((\u0026amp;book).Pages()) // 123 } 方法\n1. new And make # what: allocates AND initializes\nnew (allocates memory). make (allocates and initializes an object of type slice, map, or chan (only)). why:\nhow:\n区别从函数定义就可以看出来:func new(Type) *Type AND func make(t Type, size ...IntegerType) Type 入参值 make 只能用来分配及初始化类型为 slice、map、chan 的数据(slice, map, or chan (only))。new 可以分配任意类型的数据； 输出值 new 分配返回的是指针，即类型 *Type。make 返回引用，即 Type； new 分配的空间被清零。make 分配空间后，会进行初始化； 外部参考\nexample # package main import ( \u0026#34;fmt\u0026#34; ) func main() { var i *int *i=10 fmt.Println(*i) } 这个例子会打印出什么？0还是10?。以上全错，运行的时候会painc，原因如下：\npanic: runtime error: invalid memory address or nil pointer dereference 从这个提示中可以看出，对于引用类型的变量，我们不光要声明它，还要为它分配内容空间，否则我们的值放在哪里去呢？这就是上面错误提示的原因。\n对于值类型的声明不需要，是因为已经默认帮我们分配好了。\n要分配内存，就引出来今天的new和make。\n2. 值类型和引用类型的区别 # 2.1. go语言中没有引用类型。\nthe concept \u0026ldquo;reference type\u0026rdquo; has been totally removed from Go spec since Apr 3rd, 2013 2.2. 引用关系是通过指针和各种内置类型的某些可能的内部结构建立的\n内部结构的实现定义 // 映射类型 type _map *hashtableImpl // 目前，官方标准编译器是使用哈希表来实现映射的。 // 通道类型 type _channel *channelImpl // 函数类型 type _function *functionImpl type _slice struct { elements unsafe.Pointer // 引用着底层的元素 len int // 当前的元素个数 cap int // 切片的容量 } type _string struct { elements *byte // 引用着底层的byte元素 len int // 字符串的长度 } type _interface struct { dynamicType *_type // 引用着接口值的动态类型 dynamicValue unsafe.Pointer // 引用着接口值的动态值 } type _interface struct { dynamicTypeInfo *struct { dynamicType *_type // 引用着接口值的动态类型 methods []*_function // 引用着动态类型的对应方法列表 } dynamicValue unsafe.Pointer // 引用着动态值 } Question\n但是为什么使用%p来打印的时候这些常规认为的“引用类型”,可以打印，答应我们认为的值类型的时候，出错。 从下面的gdb例子中可以看出，我们能打印这些常规认为的“引用类型”,是因为Slice: %p\taddress of 0th element in base 16 notation, with leading 0x, 它实际打印的是第0个元素的地址。而\u0026amp;test是打印的它实际的struct的地址。 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) func testS(test []int) { // test:0xc000078150 fmt.Printf(\u0026#34;---test:%p\\n\u0026#34;, test) // \u0026amp;test:0xc000098028 fmt.Printf(\u0026#34;\u0026amp;test:%p\\n\u0026#34;, \u0026amp;test) fmt.Println() } func main() { s := []int{1, 2} fmt.Println(unsafe.Sizeof(s)) fmt.Printf(\u0026#34;s---:%p\\n\u0026#34;, s) fmt.Printf(\u0026#34;\u0026amp;s[0]---:%p\\n\u0026#34;, \u0026amp;s[0]) fmt.Printf(\u0026#34;\u0026amp;s:%p\\n\u0026#34;, \u0026amp;s) testS(s) } Package fmt There Are No Reference Types in Go About the terminology \u0026ldquo;reference type\u0026rdquo; in Go\n3. 计算golang中类型的大小的方式 # struct会不会有填充的（padding）的bit? 会的，就像下方的例子中，T.B就是一个bit,但是它后面填充了7个bit. 类似sizeof的函数？ unsafe.Sizeof() 如果是分配在heap中，还可以使用什么方式来评估？ runtime.ReadMemStats(\u0026amp;m1)// 好像一个snapshot,可以评估当前heap的空间. t := T{} runtime.ReadMemStats(\u0026amp;m2) package main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;unsafe\u0026#34; ) // Unlike C, there is no _#pragma pack_ in Go, the real memory allocation // depends on its implementation. type T struct { B uint8 // is a byte I int // it is int32 on my x86 32 bit PC P *int // it is int32 on my x86 32 bit PC S string SS []string } var p = fmt.Println // In this case, the `t := T{}` can not measured by this method. func memUsage(m1, m2 *runtime.MemStats) { p(\u0026#34;Alloc:\u0026#34;, m2.Alloc-m1.Alloc, \u0026#34;TotalAlloc:\u0026#34;, m2.TotalAlloc-m1.TotalAlloc, \u0026#34;HeapAlloc:\u0026#34;, m2.HeapAlloc-m1.HeapAlloc) } func main() { // Here is a tricky to get pointer size const PtrSize = 32 \u0026lt;\u0026lt; uintptr(^uintptr(0)\u0026gt;\u0026gt;63) p(\u0026#34;PtrSize=\u0026#34;, PtrSize) p(\u0026#34;IntSize=\u0026#34;, strconv.IntSize) var m1, m2, m3, m4, m5, m6 runtime.MemStats runtime.ReadMemStats(\u0026amp;m1) t := T{} runtime.ReadMemStats(\u0026amp;m2) p(\u0026#34;sizeof(uint8)\u0026#34;, unsafe.Sizeof(t.B), \u0026#34;offset=\u0026#34;, unsafe.Offsetof(t.B)) p(\u0026#34;sizeof(int)\u0026#34;, unsafe.Sizeof(t.I), \u0026#34;offset=\u0026#34;, unsafe.Offsetof(t.I)) p(\u0026#34;sizeof(*int)\u0026#34;, unsafe.Sizeof(t.P), \u0026#34;offset=\u0026#34;, unsafe.Offsetof(t.P)) p(\u0026#34;sizeof(string)\u0026#34;, unsafe.Sizeof(t.S), \u0026#34;offset=\u0026#34;, unsafe.Offsetof(t.S)) // Slice is a structure of Pointer, Len and Cap. // Detail [here](http://blog.golang.org/go-slices-usage-and-internals) p(\u0026#34;sizeof([]string)\u0026#34;, unsafe.Sizeof(t.SS), \u0026#34;offset=\u0026#34;, unsafe.Offsetof(t.SS)) // We can see the this structure is 4 + 4 + 4 + 8 + 12 = 32 bytes // There are 3 padding bytes of first t.B expanded to 4 bytes. p(\u0026#34;sizeof(T)\u0026#34;, unsafe.Sizeof(t)) // We will see 0 bytes, because it is on stack, so sizeof is the // proper method to tell how much memory allocated. memUsage(\u0026amp;m1, \u0026amp;m2) // Even string assignment is in stack. also is zero runtime.ReadMemStats(\u0026amp;m3) t2 := \u0026#34;abc\u0026#34; runtime.ReadMemStats(\u0026amp;m4) memUsage(\u0026amp;m3, \u0026amp;m4) // map will alloc memory in heap runtime.ReadMemStats(\u0026amp;m5) t3 := map[int]string{1: \u0026#34;x\u0026#34;} runtime.ReadMemStats(\u0026amp;m6) memUsage(\u0026amp;m5, \u0026amp;m6) fmt.Println(t2, t3) // prevent compiler error } golang的chan # \u0026lsquo;\u0026lsquo;里面的是chan的状态(eg: 一个零值nil通道;一个非零值但已关闭的通道)\n\u0026lsquo;空\u0026rsquo;读写阻塞-关闭恐慌; \u0026lsquo;关闭\u0026rsquo;读为0-关闭写恐慌. 如果是有缓存的chan已关闭，且现在缓存不为空,读正常得到数据\nwhat:\n通道的主要作用是用来实现并发同步 可以把一个通道看作是在一个程序内部的一个先进先出（FIFO：first in first out）数据队列。 一些协程可以向此通道发送数据，另外一些协程可以从此通道接收数据。 channel类型与值 字面形式chan T表示一个元素类型为T的双向通道类型。 编译器允许从此类型的值中接收和向此类型的值中发送数据。 字面形式chan\u0026lt;- T表示一个元素类型为T的单向发送通道类型。 编译器不允许从此类型的值中接收数据。 字面形式\u0026lt;-chan T表示一个元素类型为T的单向接收通道类型。 编译器不允许向此类型的值中发送数据。 类型T总是放在最后面。 我们了解到一个通道值可能含有底层部分。 当一个通道值被赋给另一个通道值后，这两个通道值将共享相同的底层部分。 why:\n一个通道内部维护了三个队列（均可被视为先进先出队列）： 接收数据协程队列（可以看做是先进先出队列但其实并不完全是，见下面解释）。此队列是一个没有长度限制的链表。 此队列中的协程均处于阻塞状态，它们正等待着从此通道接收数据。 发送数据协程队列（可以看做是先进先出队列但其实并不完全是，见下面解释）。此队列也是一个没有长度限制的链表。 此队列中的协程亦均处于阻塞状态，它们正等待着向此通道发送数据。 此队列中的每个协程将要发送的值（或者此值的指针，取决于具体编译器实现）和此协程一起存储在此队列中。 数据缓冲队列。这是一个循环队列（绝对先进先出），它的长度为此通道的容量。此队列中存放的值的类型都为此通道的元素类型。 如果此队列中当前存放的值的个数已经达到此通道的容量，则我们说此通道已经处于满槽状态。 如果此队列中当前存放的值的个数为零，则我们说此通道处于空槽状态。 对于一个非缓冲通道（容量为零），它总是同时处于满槽状态和空槽状态。 操作 一个零值nil通道 一个非零值但已关闭的通道 一个非零值且尚未关闭的通道 关闭 产生恐慌 产生恐慌 成功关闭(C) 发送数据 永久阻塞(I) 产生恐慌 阻塞或者成功发送(B) 接收数据 永久阻塞(II) 永不阻塞(D) 阻塞或者成功接收(A) 看下I: 直接看下发送数据的源码 walkexpr \u0026ndash;\u0026gt; chansend1 \u0026ndash;\u0026gt; chansend\nfunc chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { if c == nil { if !block { return false } // 这里面直接进入睡眠，永远阻塞 gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(\u0026#34;unreachable\u0026#34;) } //... how: 五种操作： close(ch) ch \u0026lt;- v \u0026lt;-ch v = \u0026lt;-ch v, sentBeforeClosed = \u0026lt;-ch cap(ch) len(ch) archive # 数据结构\n常用关键字\nfor 和 range select defer panic 和 recover make 和 new 并发编程\n内存\n"},{"id":4,"href":"/go-goroutine/docs/%E5%81%A5%E5%A3%AE%E4%B8%8E%E6%80%A7%E8%83%BD/golang_test/","title":"测试相关函数","section":"健壮与性能","content":" golang测试相关函数 # test函数的种类 # func TestXxx(t *testing.T) { ... }\n注意下这个Xxx需要大写 func BenchmarkXxx(b *testing.B) { ... }\n注意下这个Xxx需要大写 ExampleXxx\nprints output to os.Stdout \u0026ldquo;Output:\u0026rdquo; \u0026ldquo;Unordered output:\u0026rdquo; go help testfunc\nThe \u0026#39;go test\u0026#39; command expects to find test, benchmark, and example functions in the \u0026#34;*_test.go\u0026#34; files corresponding to the package under test. A test function is one named TestXxx (where Xxx does not start with a lower case letter) and should have the signature, func TestXxx(t *testing.T) { ... } A benchmark function is one named BenchmarkXxx and should have the signature, func BenchmarkXxx(b *testing.B) { ... } An example function is similar to a test function but, instead of using *testing.T to report success or failure, prints output to os.Stdout. If the last comment in the function starts with \u0026#34;Output:\u0026#34; then the output is compared exactly against the comment (see examples below). If the last comment begins with \u0026#34;Unordered output:\u0026#34; then the output is compared to the comment, however the order of the lines is ignored. An example with no such comment is compiled but not executed. An example with no text after \u0026#34;Output:\u0026#34; is compiled, executed, and expected to produce no output. Godoc displays the body of ExampleXxx to demonstrate the use of the function, constant, or variable Xxx. An example of a method M with receiver type T or *T is named ExampleT_M. There may be multiple examples for a given function, constant, or variable, distinguished by a trailing _xxx, where xxx is a suffix not beginning with an upper case letter. Here is an example of an example: func ExamplePrintln() { Println(\u0026#34;The output of\\nthis example.\u0026#34;) // Output: The output of // this example. } Here is another example where the ordering of the output is ignored: func ExamplePerm() { for _, value := range Perm(4) { fmt.Println(value) } // Unordered output: 4 // 2 // 1 // 3 // 0 } The entire test file is presented as the example when it contains a single example function, at least one other function, type, variable, or constant declaration, and no test or benchmark functions. See the documentation of the testing package for more information. go test工具 # 测试结果输出结果：test status ('ok' or 'FAIL') package name elapsed time\ngo test [build/test flags] [packages] [build/test flags \u0026amp; test binary flags] 当前目录模式: 如果当前没有go.mod，那么会自动生成 一个临时_path:_/tmp/hello go test go test -v 包列表模式 go test \u0026lt;module name\u0026gt; go test . go test ./... 打印更详细的信息 go test -v . go test -bench . [root@iZf8z14idfp0rwhiicngwqZ hello]# tree . ├── main.go └── main_test.go 0 directories, 2 files [root@iZf8z14idfp0rwhiicngwqZ hello]# go test PASS ok _/tmp/hello\t1.001s // --------------------------------------------------------------------------- // [root@iZf8z14idfp0rwhiicngwqZ hello]# tree . ├── go.mod ├── main.go └── main_test.go 0 directories, 3 files [root@iZf8z14idfp0rwhiicngwqZ hello]# cat go.mod module example.com/hello go 1.13 // --------------------------------------------------------------------------- // [root@iZf8z14idfp0rwhiicngwqZ hello]# go test example.com/hello/hello can\u0026#39;t load package: package example.com/hello/hello: module example.com/hello/hello: Get https://proxy.golang.org/example.com/hello/hello/@v/list: dial tcp 172.217.160.113:443: i/o timeout [root@iZf8z14idfp0rwhiicngwqZ hello]# go test example.com/hello ok example.com/hello\t1.002s [root@iZf8z14idfp0rwhiicngwqZ hello]# go test hello can\u0026#39;t load package: package hello: malformed module path \u0026#34;hello\u0026#34;: missing dot in first path element [root@iZf8z14idfp0rwhiicngwqZ hello]# go test . ok example.com/hello\t(cached) [root@iZf8z14idfp0rwhiicngwqZ hello]# go test ./... ok example.com/hello\t(cached) // --------------------------------------------------------------------------- // [root@iZf8z14idfp0rwhiicngwqZ hello]# go test -v . === RUN TestHello --- PASS: TestHello (1.00s) PASS ok example.com/hello\t1.001s [root@iZf8z14idfp0rwhiicngwqZ hello]# go test -bench . PASS ok example.com/hello\t1.001s go help test usage: go test [build/test flags] [packages] [build/test flags \u0026amp; test binary flags] \u0026#39;Go test\u0026#39; automates testing the packages named by the import paths. It prints a summary of the test results in the format: ok archive/tar 0.011s FAIL archive/zip 0.022s ok compress/gzip 0.033s ... followed by detailed output for each failed package. \u0026#39;Go test\u0026#39; recompiles each package along with any files with names matching the file pattern \u0026#34;*_test.go\u0026#34;. These additional files can contain test functions, benchmark functions, and example functions. See \u0026#39;go help testfunc\u0026#39; for more. Each listed package causes the execution of a separate test binary. Files whose names begin with \u0026#34;_\u0026#34; (including \u0026#34;_test.go\u0026#34;) or \u0026#34;.\u0026#34; are ignored. Test files that declare a package with the suffix \u0026#34;_test\u0026#34; will be compiled as a separate package, and then linked and run with the main test binary. The go tool will ignore a directory named \u0026#34;testdata\u0026#34;, making it available to hold ancillary data needed by the tests. As part of building a test binary, go test runs go vet on the package and its test source files to identify significant problems. If go vet finds any problems, go test reports those and does not run the test binary. Only a high-confidence subset of the default go vet checks are used. That subset is: \u0026#39;atomic\u0026#39;, \u0026#39;bool\u0026#39;, \u0026#39;buildtags\u0026#39;, \u0026#39;nilfunc\u0026#39;, and \u0026#39;printf\u0026#39;. You can see the documentation for these and other vet tests via \u0026#34;go doc cmd/vet\u0026#34;. To disable the running of go vet, use the -vet=off flag. All test output and summary lines are printed to the go command\u0026#39;s standard output, even if the test printed them to its own standard error. (The go command\u0026#39;s standard error is reserved for printing errors building the tests.) Go test runs in two different modes: The first, called local directory mode, occurs when go test is invoked with no package arguments (for example, \u0026#39;go test\u0026#39; or \u0026#39;go test -v\u0026#39;). In this mode, go test compiles the package sources and tests found in the current directory and then runs the resulting test binary. In this mode, caching (discussed below) is disabled. After the package test finishes, go test prints a summary line showing the test status (\u0026#39;ok\u0026#39; or \u0026#39;FAIL\u0026#39;), package name, and elapsed time. The second, called package list mode, occurs when go test is invoked with explicit package arguments (for example \u0026#39;go test math\u0026#39;, \u0026#39;go test ./...\u0026#39;, and even \u0026#39;go test .\u0026#39;). In this mode, go test compiles and tests each of the packages listed on the command line. If a package test passes, go test prints only the final \u0026#39;ok\u0026#39; summary line. If a package test fails, go test prints the full test output. If invoked with the -bench or -v flag, go test prints the full output even for passing package tests, in order to display the requested benchmark results or verbose logging. After the package tests for all of the listed packages finish, and their output is printed, go test prints a final \u0026#39;FAIL\u0026#39; status if any package test has failed. In package list mode only, go test caches successful package test results to avoid unnecessary repeated running of tests. When the result of a test can be recovered from the cache, go test will redisplay the previous output instead of running the test binary again. When this happens, go test prints \u0026#39;(cached)\u0026#39; in place of the elapsed time in the summary line. The rule for a match in the cache is that the run involves the same test binary and the flags on the command line come entirely from a restricted set of \u0026#39;cacheable\u0026#39; test flags, defined as -cpu, -list, -parallel, -run, -short, and -v. If a run of go test has any test or non-test flags outside this set, the result is not cached. To disable test caching, use any test flag or argument other than the cacheable flags. The idiomatic way to disable test caching explicitly is to use -count=1. Tests that open files within the package\u0026#39;s source root (usually $GOPATH) or that consult environment variables only match future runs in which the files and environment variables are unchanged. A cached test result is treated as executing in no time at all, so a successful package test result will be cached and reused regardless of -timeout setting. In addition to the build flags, the flags handled by \u0026#39;go test\u0026#39; itself are: -args Pass the remainder of the command line (everything after -args) to the test binary, uninterpreted and unchanged. Because this flag consumes the remainder of the command line, the package list (if present) must appear before this flag. -c Compile the test binary to pkg.test but do not run it (where pkg is the last element of the package\u0026#39;s import path). The file name can be changed with the -o flag. -exec xprog Run the test binary using xprog. The behavior is the same as in \u0026#39;go run\u0026#39;. See \u0026#39;go help run\u0026#39; for details. -i Install packages that are dependencies of the test. Do not run the test. -json Convert test output to JSON suitable for automated processing. See \u0026#39;go doc test2json\u0026#39; for the encoding details. -o file Compile the test binary to the named file. The test still runs (unless -c or -i is specified). The test binary also accepts flags that control execution of the test; these flags are also accessible by \u0026#39;go test\u0026#39;. See \u0026#39;go help testflag\u0026#39; for details. For more about build flags, see \u0026#39;go help build\u0026#39;. For more about specifying packages, see \u0026#39;go help packages\u0026#39;. See also: go build, go vet. 运行文件的单个测试单元 # 运行某个test文件的某个test例子。 go test -v -run=\u0026quot;Regrex$\u0026quot; main_test.go // tree . ├── go.mod ├── main.go └── main_test.go 0 directories, 3 files // cat main_test.go package hello import ( \u0026#34;testing\u0026#34; \u0026#34;time\u0026#34; ) func TestHello(t *testing.T) { want := \u0026#34;Hello, world.\u0026#34; if got := Hello(); got != want { t.Errorf(\u0026#34;Hello() = %q, want %q\u0026#34;, got, want) } time.Sleep(time.Second) } func TestRegrex(t *testing.T) { want := \u0026#34;Hello, world. --- regrex\u0026#34; if got := Hello(); got != want { t.Errorf(\u0026#34;Hello() = %q, want %q\u0026#34;, got, want) } time.Sleep(time.Second) } // cat go.mod module example.com/hello go 1.13 [root@iZf8z14idfp0rwhiicngwqZ hello]# go test -v -run=\u0026#34;Regrex$\u0026#34; main_test.go # command-line-arguments [command-line-arguments.test] ./main_test.go:10:15: undefined: Hello ./main_test.go:19:15: undefined: Hello FAIL\tcommand-line-arguments [build failed] FAIL [root@iZf8z14idfp0rwhiicngwqZ hello]# go test -v -run=\u0026#34;Regrex$\u0026#34; main_test.go main.go === RUN TestRegrex --- FAIL: TestRegrex (1.00s) main_test.go:20: Hello() = \u0026#34;Hello, world.\u0026#34;, want \u0026#34;Hello, world. --- regrex\u0026#34; FAIL FAIL\tcommand-line-arguments\t1.002s FAIL [root@iZf8z14idfp0rwhiicngwqZ hello]# go test -v -run=\u0026#34;Regrex$\u0026#34; . === RUN TestRegrex --- FAIL: TestRegrex (1.00s) main_test.go:20: Hello() = \u0026#34;Hello, world.\u0026#34;, want \u0026#34;Hello, world. --- regrex\u0026#34; FAIL FAIL\texample.com/hello\t1.001s FAIL [root@iZf8z14idfp0rwhiicngwqZ hello]# go test -v -run=\u0026#34;Regrex$\u0026#34; example.com/hello === RUN TestRegrex --- FAIL: TestRegrex (1.00s) main_test.go:20: Hello() = \u0026#34;Hello, world.\u0026#34;, want \u0026#34;Hello, world. --- regrex\u0026#34; FAIL FAIL\texample.com/hello\t1.001s FAIL test文件生成二进制文件 # go test [build/test flags] [packages] [build/test flags \u0026amp; test binary flags] test binary flags // ls ------------------------------------------------------------ bench.test go.mod main.go main_test.go // cat go.mod ----------------------module is bench--------------- module bench go 1.13 // cat main.go -------------------------------------------------- package main func fib(n int) int { if n == 0 || n == 1 { return n } return fib(n-2) + fib(n-1) } // cat main_test.go ----------------------------------------------- package main import \u0026#34;testing\u0026#34; func BenchmarkFib(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { fib(30) // run fib(30) b.N times } } go test bench -c -o test_binary -bench . go test -bench=\u0026#34;Fib$\u0026#34; . goos: darwin goarch: amd64 pkg: bench BenchmarkFib-4 224 5329575 ns/op PASS ok bench 1.739s ./bench.test -test.bench=\u0026#34;Fib$\u0026#34; goos: darwin goarch: amd64 pkg: bench BenchmarkFib-4 223 5320568 ns/op PASS benchmark # go test 命令默认不运行 benchmark 用例的，如果我们想运行 benchmark 用例，则需要加上 -bench 参数。\n-bench参数支持传入一个正则表达式，匹配到的用例才会得到执行。 -cpu\nGOMAXPROCS，默认等于 CPU 核数。可以通过 -cpu 参数改变 GOMAXPROCS，-cpu 支持传入一个列表作为参数 提升测试准确度的一个重要手段就是增加测试的次数。\n我们可以使用-benchtime 和 -count两个参数达到这个目的。 -benchtime的默认时间是 1s，那么我们可以使用-benchtime指定为 5s。 还可以是具体的次数。```-benchtime=30x`(执行30次) -count 参数可以用来设置 benchmark 的轮数。例如，进行 3 轮 benchmark。 -benchmem参数可以度量内存分配的次数。\n基准测试的测试次数 # b *testing.B，有个属性 b.N 表示这个用例需要运行的次数。b.N 对于每个用例都是不一样的。\n那这个值是如何决定的呢？b.N 从 1 开始，如果该用例能够在 1s 内完成，b.N 的值便会增加，再次执行。b.N 的值大概以 1, 2, 3, 5, 10, 20, 30, 50, 100 这样的序列递增，越到后面，增加得越快。\n精确 # ResetTimer func BenchmarkFib(b *testing.B) { time.Sleep(time.Second * 3) // 模拟耗时准备任务 b.ResetTimer() // 重置定时器 for n := 0; n \u0026lt; b.N; n++ { fib(30) // run fib(30) b.N times } } StopTimer \u0026amp; StartTimer // sort_test.go package main import ( \u0026#34;math/rand\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;time\u0026#34; ) func generateWithCap(n int) []int { rand.Seed(time.Now().UnixNano()) nums := make([]int, 0, n) for i := 0; i \u0026lt; n; i++ { nums = append(nums, rand.Int()) } return nums } func bubbleSort(nums []int) { for i := 0; i \u0026lt; len(nums); i++ { for j := 1; j \u0026lt; len(nums)-i; j++ { if nums[j] \u0026lt; nums[j-1] { nums[j], nums[j-1] = nums[j-1], nums[j] } } } } func BenchmarkBubbleSort(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { b.StopTimer() nums := generateWithCap(10000) b.StartTimer() bubbleSort(nums) } } 附录 # "},{"id":5,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/%E8%B0%83%E5%BA%A6/%E4%BA%8C-%E8%B0%83%E5%BA%A6%E7%9A%84%E6%97%B6%E6%9C%BA/4.2.2-%E8%A2%AB%E5%8A%A8%E8%B0%83%E5%BA%A6/","title":"goroutine被动调度","section":"调度时机","content":" goroutine被动调度 # 什么是被动调度: goroutine执行某个操作因条件不满足需要等待而发生的调度； goroutine进入睡眠[比如是Goroutine N发送数据到无缓冲chan上面,当没有其他Goroutine从chan上面读数据的时候,Goroutine N阻塞在chan上面. 此刻睡眠含义:进入chan的缓存读取队列(goroutine链表)]; 重新运行schedule() 唤醒睡眠中的goroutine; 唤醒空闲的P和唤醒创建工作线程; goroutine(被创建出来后/创建运行了一段时间后)如何放入运行队列[P中]; 探寻被动调度,如何进入睡眠 # goroutine因某个条件而阻塞 chan waitGroup 等这些都会发生阻塞 定义程序 # 先来看一个例子,\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;sync/atomic\u0026#34; ) func main() { var n int32 var wg sync.WaitGroup runtime.GOMAXPROCS(2) wg.Add(1) go func() { wg.Done() for{ atomic.AddInt32(\u0026amp;n, 1) } }() wg.Wait() fmt.Println(atomic.LoadInt32(\u0026amp;n)) // 1 } gdb调试前准备 # 编译程序 # 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n准备mcall函数断点的文件 # 进入wg.Wait()\n睡眠 # 我们都知道main函数里面的wg.Wait()这个只有当wg的计数器到0后才会继续执行, 我们来看看当wg还没有到0的时候,这里会发生什么呢?\nsrc/sync/waitgroup.go\n// Wait blocks until the WaitGroup counter is zero. func (wg *WaitGroup) Wait() { statep, semap := wg.state() //....省去race代码 for { state := atomic.LoadUint64(statep) v := int32(state \u0026gt;\u0026gt; 32) w := uint32(state) if v == 0 { // Counter is 0, no need to wait. //....省去race代码 return } // Increment waiters count. if atomic.CompareAndSwapUint64(statep, state, state+1) { //....省去race代码 runtime_Semacquire(semap) // PV操作 ---------------------here if *statep != 0 { panic(\u0026#34;sync: WaitGroup is reused before previous Wait has returned\u0026#34;) } if race.Enabled { race.Enable() race.Acquire(unsafe.Pointer(wg)) } return } } } runtime_Semacquire # src/sync/runtime.go\n// Semacquire waits until *s \u0026gt; 0 and then atomically decrements it. // It is intended as a simple sleep primitive for use by the synchronization // library and should not be used directly. func runtime_Semacquire(s *uint32) 这里类似PV操作, 但是: P:等待直到(*s)大于0, 然后才自动减. [话句话如果是等于0, 那么就会休眠]; v:首先自动增加(*s),唤醒被等待的goroutine. src/sync/runtime.go\n// Semacquire waits until *s \u0026gt; 0 and then atomically decrements it. // It is intended as a simple sleep primitive for use by the synchronization // library and should not be used directly. func runtime_Semacquire(s *uint32) 继续下去发现找不到定义了, 我们gdb查找一下:\nlist /usr/lib/golang/src/sync/waitgroup.go:130 b 130 list /usr/lib/golang/src/sync/runtime.go:1 b 130 (gdb) info break Num Type Disp Enb Address What 1 breakpoint keep y 0x0000000000466bfb in sync.(*WaitGroup).Wait at /usr/lib/golang/src/sync/waitgroup.go:130 2 breakpoint keep y 0x00000000004669c0 in sync.init.1 at /usr/lib/golang/src/sync/runtime.go:14 (gdb) (gdb) step sync.runtime_Semacquire (addr=0xc00007e014) at /usr/lib/golang/src/runtime/sema.go:55 55\tfunc sync_runtime_Semacquire(addr *uint32) { (gdb) frame #0 sync.runtime_Semacquire (addr=0xc00007e014) at /usr/lib/golang/src/runtime/sema.go:55 55\tfunc sync_runtime_Semacquire(addr *uint32) { sync_runtime_Semacquire # /src/runtime/sema.go\n//go:linkname sync_runtime_Semacquire sync.runtime_Semacquire func sync_runtime_Semacquire(addr *uint32) { semacquire1(addr, false, semaBlockProfile, 0) } func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int) { gp := getg() if gp != gp.m.curg { throw(\u0026#34;semacquire not on the G stack\u0026#34;) } // Easy case. if cansemacquire(addr) { return } // Harder case: //\tincrement waiter count //\ttry cansemacquire one more time, return if succeeded //\tenqueue itself as a waiter //\tsleep //\t(waiter descriptor is dequeued by signaler) s := acquireSudog() root := semroot(addr) t0 := int64(0) s.releasetime = 0 s.acquiretime = 0 s.ticket = 0 if profile\u0026amp;semaBlockProfile != 0 \u0026amp;\u0026amp; blockprofilerate \u0026gt; 0 { t0 = cputicks() s.releasetime = -1 } if profile\u0026amp;semaMutexProfile != 0 \u0026amp;\u0026amp; mutexprofilerate \u0026gt; 0 { if t0 == 0 { t0 = cputicks() } s.acquiretime = t0 } for { lock(\u0026amp;root.lock) // Add ourselves to nwait to disable \u0026#34;easy case\u0026#34; in semrelease. atomic.Xadd(\u0026amp;root.nwait, 1) // Check cansemacquire to avoid missed wakeup. if cansemacquire(addr) { atomic.Xadd(\u0026amp;root.nwait, -1) unlock(\u0026amp;root.lock) break } // Any semrelease after the cansemacquire knows we\u0026#39;re waiting // (we set nwait above), so go to sleep. root.queue(addr, s, lifo) goparkunlock(\u0026amp;root.lock, waitReasonSemacquire, traceEvGoBlockSync, 4+skipframes) // 进入睡眠 ---------------------here if s.ticket != 0 || cansemacquire(addr) { break } } if s.releasetime \u0026gt; 0 { blockevent(s.releasetime-t0, 3+skipframes) } releaseSudog(s) } 这里的sema变量, 如果是等于0(它是无符号整形,不会为负数),就代表它没有获取到sema,需要等待, 如果sema变量是大于0,可以直接运行,不需要等待\nfunc cansemacquire(addr *uint32) bool { for { v := atomic.Load(addr) if v == 0 { return false } if atomic.Cas(addr, v, v-1) { return true } } } 我们知道sync.WaitGroup()函数, 一般是先Add(), 然后Wait()等待所有任务Done()[Add(-1)] sema开始值是0,按照我们前面说的,他无法获得sema,会进行休眠,同理如果有多个Wait(), 那么它们都会 休眠; 当Add(-1)后到达0后, 就是所有任务都已经完成,它会调用runtime_Semrelease(semap, false, 0) // 进行V操作进行sema值加一, 然后唤醒一个goroutine, 当这个goroutine醒来[在一个loop里面], 发现sema不等于0了, 直接减一,然后跳出了semacquire1\n关于WaitGroup的分析, 参看我其他的文章.继续分析这个休眠和唤醒的;\ngopark # src/runtime/proc.go\n// Puts the current goroutine into a waiting state and unlocks the lock. // The goroutine can be made runnable again by calling goready(gp). func goparkunlock(lock *mutex, reason waitReason, traceEv byte, traceskip int) { gopark(parkunlock_c, unsafe.Pointer(lock), reason, traceEv, traceskip) } // Puts the current goroutine into a waiting state and calls unlockf. // If unlockf returns false, the goroutine is resumed. // unlockf must not access this G\u0026#39;s stack, as it may be moved between // the call to gopark and the call to unlockf. // Reason explains why the goroutine has been parked. // It is displayed in stack traces and heap dumps. // Reasons should be unique and descriptive. // Do not re-use reasons, add new ones. func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int) { if reason != waitReasonSleep { checkTimeouts() // timeouts may expire while two goroutines keep the scheduler busy } mp := acquirem() // 这里的作用是什么? ---------wait gp := mp.curg // 得到当前的Goroutine status := readgstatus(gp) if status != _Grunning \u0026amp;\u0026amp; status != _Gscanrunning { throw(\u0026#34;gopark: bad g status\u0026#34;) } mp.waitlock = lock mp.waitunlockf = unlockf gp.waitreason = reason mp.waittraceev = traceEv mp.waittraceskip = traceskip releasem(mp) // ---------wait // can\u0026#39;t do anything that might move the G between Ms here. mcall(park_m) } mcall就是切换到g0栈上去,gN的cpu寄存器等保存到g\u0026rsquo;sched,然后把gN作为实参给mcall的fn形参, 继续看park_m.\n// park continuation on g0. func park_m(gp *g) { _g_ := getg() // 得到g0 if trace.enabled { traceGoPark(_g_.m.waittraceev, _g_.m.waittraceskip) } casgstatus(gp, _Grunning, _Gwaiting) dropg()//解除gN与m的关系 if fn := _g_.m.waitunlockf; fn != nil { ok := fn(gp, _g_.m.waitlock) //解除lock _g_.m.waitunlockf = nil _g_.m.waitlock = nil if !ok { if trace.enabled { traceGoUnpark(gp, 2) } casgstatus(gp, _Gwaiting, _Grunnable) execute(gp, true) // Schedule it back, never returns. } } schedule() //重新进入调度 } dropg就是:g0---\u0026gt;m--X--\u0026gt;curg--X--\u0026gt;m这里的curg就是前面的gN,解除gN与m的关系.\nfunc dropg() { _g_ := getg() setMNoWB(\u0026amp;_g_.m.curg.m, nil) setGNoWB(\u0026amp;_g_.m.curg, nil) } 进入睡眠的步骤:goparkunlock --\u0026gt; gopark --\u0026gt; mcall(park_m) --\u0026gt; schedule.\n唤醒睡眠中的goroutine # func (wg *WaitGroup) Add(delta int) { statep, semap := wg.state() if race.Enabled { _ = *statep // trigger nil deref early if delta \u0026lt; 0 { // Synchronize decrements with Wait. race.ReleaseMerge(unsafe.Pointer(wg)) } race.Disable() defer race.Enable() } state := atomic.AddUint64(statep, uint64(delta)\u0026lt;\u0026lt;32) v := int32(state \u0026gt;\u0026gt; 32) w := uint32(state) if race.Enabled \u0026amp;\u0026amp; delta \u0026gt; 0 \u0026amp;\u0026amp; v == int32(delta) { // The first increment must be synchronized with Wait. // Need to model this as a read, because there can be // several concurrent wg.counter transitions from 0. race.Read(unsafe.Pointer(semap)) } if v \u0026lt; 0 { panic(\u0026#34;sync: negative WaitGroup counter\u0026#34;) } if w != 0 \u0026amp;\u0026amp; delta \u0026gt; 0 \u0026amp;\u0026amp; v == int32(delta) { panic(\u0026#34;sync: WaitGroup misuse: Add called concurrently with Wait\u0026#34;) } if v \u0026gt; 0 || w == 0 { return } // This goroutine has set counter to 0 when waiters \u0026gt; 0. // Now there can\u0026#39;t be concurrent mutations of state: // - Adds must not happen concurrently with Wait, // - Wait does not increment waiters if it sees counter == 0. // Still do a cheap sanity check to detect WaitGroup misuse. if *statep != state { panic(\u0026#34;sync: WaitGroup misuse: Add called concurrently with Wait\u0026#34;) } // Reset waiters count to 0. *statep = 0 for ; w != 0; w-- { runtime_Semrelease(semap, false, 0) // 进行V操作 --------------------------------here } } runtime_Semrelease # src/sync/runtime.go\n//go:linkname sync_runtime_Semrelease sync.runtime_Semrelease func sync_runtime_Semrelease(addr *uint32, handoff bool, skipframes int) { semrelease1(addr, handoff, skipframes) } func semrelease1(addr *uint32, handoff bool, skipframes int) { root := semroot(addr) atomic.Xadd(addr, 1) // Easy case: no waiters? // This check must happen after the xadd, to avoid a missed wakeup // (see loop in semacquire). if atomic.Load(\u0026amp;root.nwait) == 0 { return } // Harder case: search for a waiter and wake it. lock(\u0026amp;root.lock) if atomic.Load(\u0026amp;root.nwait) == 0 { // The count is already consumed by another goroutine, // so no need to wake up another goroutine. unlock(\u0026amp;root.lock) return } s, t0 := root.dequeue(addr) if s != nil { atomic.Xadd(\u0026amp;root.nwait, -1) } unlock(\u0026amp;root.lock) if s != nil { // May be slow, so unlock first acquiretime := s.acquiretime if acquiretime != 0 { mutexevent(t0-acquiretime, 3+skipframes) } if s.ticket != 0 { throw(\u0026#34;corrupted semaphore ticket\u0026#34;) } if handoff \u0026amp;\u0026amp; cansemacquire(addr) { s.ticket = 1 } readyWithTime(s, 5+skipframes) // ------------------------------here } } readyWithTime # func readyWithTime(s *sudog, traceskip int) { if s.releasetime != 0 { s.releasetime = cputicks() } goready(s.g, traceskip) } func goready(gp *g, traceskip int) { systemstack(func() { ready(gp, traceskip, true) }) } 前面都很好理解, 这里有两个函数, 一个是systemstack, 一个是ready函数\nready # // Mark gp ready to run. func ready(gp *g, traceskip int, next bool) { if trace.enabled { //忽略 traceGoUnpark(gp, traceskip) } status := readgstatus(gp) // return atomic.Load(\u0026amp;gp.atomicstatus)读atomicstatus的状态 // Mark runnable. _g_ := getg() mp := acquirem() // disable preemption because it can be holding p in a local var if status\u0026amp;^_Gscan != _Gwaiting { dumpgstatus(gp) throw(\u0026#34;bad g-\u0026gt;status in ready\u0026#34;) } // status is Gwaiting or Gscanwaiting, make Grunnable and put on runq casgstatus(gp, _Gwaiting, _Grunnable) // 修改状态到_Grunnable runqput(_g_.m.p.ptr(), gp, next) // 放入全局或本地队列,等待调度 if atomic.Load(\u0026amp;sched.npidle) != 0 \u0026amp;\u0026amp; atomic.Load(\u0026amp;sched.nmspinning) == 0 { wakep() // 有空闲的P,并且没有正在自旋状态的M(偷取其他线程的goroutine);那么需要唤醒P.万一就把上面的待运行的goroutine调度起来了呢? } releasem(mp) } sync_runtime_Semrelease ---\u0026gt; semrelease1 ---\u0026gt; readyWithTime ---\u0026gt; goready --systemstack-\u0026gt; ready ready---\u0026gt; runqput ---\u0026gt; releasem \\---\u0026gt; wakep --/\u0026gt; runqput # 其中runqput是,当从s, t0 := root.dequeue(addr)得到睡眠中的Goroutine,修改状态为_Grunnable,然后放入本地或全局队列\n// runqput tries to put g on the local runnable queue. // If next is false, runqput adds g to the tail of the runnable queue. // If next is true, runqput puts g in the _p_.runnext slot. // If the run queue is full, runnext puts g on the global queue. // Executed only by the owner P. func runqput(_p_ *p, gp *g, next bool) { if randomizeScheduler \u0026amp;\u0026amp; next \u0026amp;\u0026amp; fastrand()%2 == 0 { next = false } if next { retryNext: oldnext := _p_.runnext if !_p_.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) { goto retryNext } if oldnext == 0 { return } // Kick the old runnext out to the regular run queue. gp = oldnext.ptr() } retry: h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with consumers t := _p_.runqtail if t-h \u0026lt; uint32(len(_p_.runq)) { // TODO计算这个 _p_.runq[t%uint32(len(_p_.runq))].set(gp) atomic.StoreRel(\u0026amp;_p_.runqtail, t+1) // store-release, makes the item available for consumption return } if runqputslow(_p_, gp, h, t) { return } // the queue is not full, now the put above must succeed goto retry } runqput函数流程很清晰，它首先尝试把gp放入_p_的本地运行队列，如果本地队列满了，则通过runqputslow函数把gp放入全局运行队列。\n// Put g and a batch of work from local runnable queue on global queue. // Executed only by the owner P. func runqputslow(_p_ *p, gp *g, h, t uint32) bool { var batch [len(_p_.runq)/2 + 1]*g // 256/2+1 = 129个goroutine // First, grab a batch from local queue. n := t - h n = n / 2 if n != uint32(len(_p_.runq)/2) { // 得到现有队列中的一半G throw(\u0026#34;runqputslow: queue is not full\u0026#34;) } for i := uint32(0); i \u0026lt; n; i++ { batch[i] = _p_.runq[(h+i)%uint32(len(_p_.runq))].ptr() /* --------- | | | --------- head[1] tail[2], TODO 应该head, tail应该不是地址 */ } if !atomic.CasRel(\u0026amp;_p_.runqhead, h, h+n) { // cas-release, commits consume return false } batch[n] = gp if randomizeScheduler { // 打乱将要插入全局的G for i := uint32(1); i \u0026lt;= n; i++ { j := fastrandn(i + 1) batch[i], batch[j] = batch[j], batch[i] } } // Link the goroutines. for i := uint32(0); i \u0026lt; n; i++ { batch[i].schedlink.set(batch[i+1]) // TODO 通过schedlink来进行连接? 全局运行队列是一个链表,把将要放入全局的G,链接起来. } var q gQueue q.head.set(batch[0]) q.tail.set(batch[n]) // Now put the batch on global queue. lock(\u0026amp;sched.lock) globrunqputbatch(\u0026amp;q, int32(n+1)) unlock(\u0026amp;sched.lock) return true } https://www.bing.com/search?q=WaitGroup+source+code\u0026PC=U316\u0026FORM=CHROMN\n唤醒一个空闲P,获得(创建)一个M,开始轮询调度,可能把上面已唤醒的G调度起来 # 我们继续看,wakep() // 有空闲的P,并且没有正在自旋状态的M(偷取其他线程的goroutine)\nwakep\u0026ndash;\u0026gt;startm # 当唤醒一个G,放到本地或全局了,这时如果发现有空闲的P,而且没有正在偷其他的P队列中goroutine的M,那么此时就需要唤醒一个空闲P,来工作, 一个P只能找到M,从空闲的M中找?, 然后通过m.g0找到. 生成新的M,此时,创造新的线程,因为M与线程是一一对应的,同时它也会创造一个g0. 然后开始schedule()工作. // Tries to add one more P to execute G\u0026#39;s. // Called when a G is made runnable (newproc, ready). func wakep() { // be conservative about spinning threads if !atomic.Cas(\u0026amp;sched.nmspinning, 0, 1) { return } startm(nil, true) } 这里又增加了全局的自旋线程的个数,因为我们在前面已经判断了,wakep() // 有空闲的P,并且没有正在自旋状态的M(偷取其他线程的goroutine),先再次判断下,是否还是没有自旋的线程在运行,如果没有就加一,后面准备创建一个M,startm.\n继续看下startm 如果p==nil，尝试获取一个空闲的P，如果没有空闲的P，则什么也不做。 安排一些M来运行p（必要时创建一个M）。 可以在m.p==nil的情况下运行，所以不允许写障碍。 如果设置了spinning，则调用者已经增量了nmspinning，startm将会在新启动的M中减去nmspinning或设置m.spinning。 上面我们调用的startm(nil,true) func startm(_p_ *p, spinning bool) { lock(\u0026amp;sched.lock) if _p_ == nil { _p_ = pidleget() if _p_ == nil { unlock(\u0026amp;sched.lock) if spinning { // The caller incremented nmspinning, but there are no idle Ps, // so it\u0026#39;s okay to just undo the increment and give up. if int32(atomic.Xadd(\u0026amp;sched.nmspinning, -1)) \u0026lt; 0 { throw(\u0026#34;startm: negative nmspinning\u0026#34;) } } return } } mp := mget() unlock(\u0026amp;sched.lock) // 全局空闲M队列 if mp == nil { //从全局的空闲队列没找到M var fn func() if spinning { // The caller incremented nmspinning, so set m.spinning in the new M. fn = mspinning } newm(fn, _p_) //新创建一个M,其中里面也有新g0. return // 这里如果是新创建的线程,那么直接返回,不需要继续下面的唤醒notewakeup(\u0026amp;mp.park) } if mp.spinning { throw(\u0026#34;startm: m is spinning\u0026#34;) } if mp.nextp != 0 { throw(\u0026#34;startm: m has p\u0026#34;) } if spinning \u0026amp;\u0026amp; !runqempty(_p_) { throw(\u0026#34;startm: p has runnable gs\u0026#34;) } // The caller incremented nmspinning, so set m.spinning in the new M. mp.spinning = spinning mp.nextp.set(_p_) notewakeup(\u0026amp;mp.park) // 到这一步,那么M是从全局空闲M队列获得的Ms,那么需要唤醒. } 关注三点: mget()\u0026mdash;得到全局一个M newm(fn, p),创建一个M notewakeup(\u0026amp;mp.park) mget # 这个全局的空闲M队列,从下面的函数取出,它就是一个链表,通过schedlink指针来串联所有空闲的M.\n/* //尝试从全局的(sched.midle列表中得到一个m) // Try to get an m from midle list. // Sched must be locked. // May run during STW, so write barriers are not allowed. //go:nowritebarrierrec func mget() *m { mp := sched.midle.ptr() if mp != nil { sched.midle = mp.schedlink sched.nmidle-- } return mp } */ type schedt struct { //... midle muintptr // idle m\u0026#39;s waiting for work nmidle int32 // number of idle m\u0026#39;s waiting for work //... } // muintptr类型 type muintptr uintptr //go:nosplit func (mp muintptr) ptr() *m { return (*m)(unsafe.Pointer(mp)) } //go:nosplit func (mp *muintptr) set(m *m) { *mp = muintptr(unsafe.Pointer(m)) } newm创建新的线程与M # newm(fn, _p_) //新创建一个M,其中里面也有新g0.\nfunc newm(fn func(), _p_ *p) { mp := allocm(_p_, fn) //分配一个与任何线程无关的M结构体mp,mp.g0=Gorooutine(也是从这里面分配出来的) mp.nextp.set(_p_) //把M关联上P mp.sigmask = initSigmask //初始化信号 if gp := getg(); gp != nil \u0026amp;\u0026amp; gp.m != nil \u0026amp;\u0026amp; (gp.m.lockedExt != 0 || gp.m.incgo) \u0026amp;\u0026amp; GOOS != \u0026#34;plan9\u0026#34; { // We\u0026#39;re on a locked M or a thread that may have been // started by C. The kernel state of this thread may // be strange (the user may have locked it for that // purpose). We don\u0026#39;t want to clone that into another // thread. Instead, ask a known-good thread to create // the thread for us. // // This is disabled on Plan 9. See golang.org/issue/22227. // // TODO: This may be unnecessary on Windows, which // doesn\u0026#39;t model thread creation off fork. lock(\u0026amp;newmHandoff.lock) if newmHandoff.haveTemplateThread == 0 { throw(\u0026#34;on a locked thread with no template thread\u0026#34;) } mp.schedlink = newmHandoff.newm newmHandoff.newm.set(mp) if newmHandoff.waiting { newmHandoff.waiting = false notewakeup(\u0026amp;newmHandoff.wake) } unlock(\u0026amp;newmHandoff.lock) return } newm1(mp) } newm函数主要是两件事: 创建M结构体,G结构体(用于前面m.g0)在stack上开辟空间给g0.stack. allocm() 创建一个真正的线程,与上面的M结构相关联. newm1()方式, 直接从当前线程clone出一个新的线程. newmHandoff方式 [TODO zxc:可以新开一章来讲解] 给一个一直在for循环的创建线程,然后休眠的函数来创建线程,关联上面的M结构体 allocm函数 # 因为函数需要new小对象,需要用到P.mcache,需要判断当前的M是否关联着P,如果没有就要向参数借,但是为什么没有判断如果参数_P_是nil的情况?\n释放不需要等待的全局的M\u0026rsquo;s stack资源,这里是防止新生成线程又需要stack空间,所以先释放一些?\nnew M, g0结构体,并且给g0分配stack空间.\n遗留\n为什么需要locks\u0026ndash;? // Allocate a new m unassociated with any thread. // Can use p for allocation context if needed. // fn is recorded as the new m\u0026#39;s m.mstartfn. // // This function is allowed to have write barriers even if the caller // isn\u0026#39;t because it borrows _p_. // //go:yeswritebarrierrec /* */ func allocm(_p_ *p, fn func()) *m { _g_ := getg() acquirem() // disable GC because it can be called from sysmon /* func acquirem() *m { _g_ := getg() _g_.m.locks++ return _g_.m } TODO zxc: 这里很奇怪的,只是把M结构体里面的locks字段++1; 后面的releasem又把这个locks--? func releasem(mp *m) { _g_ := getg() mp.locks-- if mp.locks == 0 \u0026amp;\u0026amp; _g_.preempt { // restore the preemption request in case we\u0026#39;ve cleared it in newstack _g_.stackguard0 = stackPreempt } } */ if _g_.m.p == 0 { //如果当前m的P不存在,acquirep(_p_)关联_p_和当前的M;只是用于小对象的分配到堆上? TODO zxc: 这里P.mcahce,就是M.mcache acquirep(_p_) // temporarily borrow p for mallocs in this function } // Release the free M list. We need to do this somewhere and // this may free up a stack we can use. if sched.freem != nil { lock(\u0026amp;sched.lock) var newList *m for freem := sched.freem; freem != nil; { if freem.freeWait != 0 { // if == 0, 安全释放g0并删除m; 这里如果不等于0,就说明需要等待,还不能立即释放,所以这个if里面就好像链表反转算法,转移到新链表. next := freem.freelink freem.freelink = newList //freem.freelink = newList(nil); so, freem.freelink == nil newList = freem // newList.freelink == nil; freem == newList freem = next // continue } stackfree(freem.g0.stack) // 说明freem.freeWait==0;可以立即g0 stack释放 freem = freem.freelink // freem等于它的next指针 } sched.freem = newList //这里就把不能释放的重新放入全局释放列表 unlock(\u0026amp;sched.lock) } mp := new(m) // new一个M结构体 mp.mstartfn = fn mcommoninit(mp) // In case of cgo or Solaris or illumos or Darwin, pthread_create will make us a stack. // Windows and Plan 9 will layout sched stack on OS stack. if iscgo || GOOS == \u0026#34;solaris\u0026#34; || GOOS == \u0026#34;illumos\u0026#34; || GOOS == \u0026#34;windows\u0026#34; || GOOS == \u0026#34;plan9\u0026#34; || GOOS == \u0026#34;darwin\u0026#34; { mp.g0 = malg(-1) } else { mp.g0 = malg(8192 * sys.StackGuardMultiplier) //约等于1024*8=8192 ==\u0026gt; 8k } mp.g0.m = mp if _p_ == _g_.m.p.ptr() { //如果是借P来进行malloc,那么需要恢复原样. releasep() } releasem(_g_.m) // TODO zxc: locks ? return mp } newm1 # func newm1(mp *m) { if iscgo { // 暂时忽略. //... } execLock.rlock() // Prevent process clone. newosproc(mp) //准备去clone. execLock.runlock() } func newosproc(mp *m) { stk := unsafe.Pointer(mp.g0.stack.hi) /* * note: strace gets confused if we use CLONE_PTRACE here. */ if false { print(\u0026#34;newosproc stk=\u0026#34;, stk, \u0026#34; m=\u0026#34;, mp, \u0026#34; g=\u0026#34;, mp.g0, \u0026#34; clone=\u0026#34;, funcPC(clone), \u0026#34; id=\u0026#34;, mp.id, \u0026#34; ostk=\u0026#34;, \u0026amp;mp, \u0026#34;\\n\u0026#34;) } // Disable signals during clone, so that the new thread starts // with signals disabled. It will enable them in minit. // 在clone,关闭信号,所以创建出来的thread信号都是关闭的,minit函数再打开. var oset sigset sigprocmask(_SIG_SETMASK, \u0026amp;sigset_all, \u0026amp;oset) ret := clone(cloneFlags, stk, unsafe.Pointer(mp), unsafe.Pointer(mp.g0), unsafe.Pointer(funcPC(mstart))) sigprocmask(_SIG_SETMASK, \u0026amp;oset, nil) if ret \u0026lt; 0 { print(\u0026#34;runtime: failed to create new OS thread (have \u0026#34;, mcount(), \u0026#34; already; errno=\u0026#34;, -ret, \u0026#34;)\\n\u0026#34;) if ret == -_EAGAIN { println(\u0026#34;runtime: may need to increase max user processes (ulimit -u)\u0026#34;) } throw(\u0026#34;newosproc\u0026#34;) } } 上面还是比较清晰,先关闭信号,防止clone时候被打断.\nclone # 这个clone函数位于src/runtime/sys_linux_amd64.s\n// int32 clone(int32 flags, void *stk, M *mp, G *gp, void (*fn)(void)); TEXT runtime·clone(SB),NOSPLIT,$0 // Linux系统调用约定，这四个参数需要分别放入rdi， rsi，rdx和r10寄存器中 MOVL\tflags+0(FP), DI MOVQ\tstk+8(FP), SI MOVQ\t$0, DX MOVQ\t$0, R10 // Copy mp, gp, fn off parent stack for use by child. // Careful: Linux system call clobbers CX and R11. // Linux系统调用会污染CX和R11; 所以我们参数不放在那里面 MOVQ\tmp+16(FP), R8 MOVQ\tgp+24(FP), R9 MOVQ\tfn+32(FP), R12 MOVL\t$SYS_clone, AX // 系统调用了; 返回值放入AX寄存器里面. SYSCALL // In parent, return. CMPQ\tAX, $0 // 如果返回值是0,证明是子进程返回 JEQ\t3(PC) MOVL\tAX, ret+40(FP) // 父进程返回,返回到父进程的AX不等于0 RET // In child, on new stack. MOVQ\tSI, SP // sp = stk+8(FP); 就是设置子线程的栈顶; // If g or m are nil, skip Go-related setup. CMPQ\tR8, $0 // m JEQ\tnog CMPQ\tR9, $0 // g JEQ\tnog // Initialize m-\u0026gt;procid to Linux tid MOVL\t$SYS_gettid, AX SYSCALL MOVQ\tAX, m_procid(R8) // 设置m.proc_id = sys_gettid() // Set FS to point at m-\u0026gt;tls. LEAQ\tm_tls(R8), DI // 不取引用,所以是DI=\u0026amp;m.tls[0];把m.tls[0]地址给DI寄存器. CALL\truntime·settls(SB) // FS寄存器里的值:就是m.tls[0]的地址. // In child, set up new stack get_tls(CX) // CX = \u0026amp;m.tls[0] MOVQ\tR8, g_m(R9) MOVQ\tR9, g(CX) // gp+24(FP) == R9; g(CX)---\u0026gt; *CX; m.tls[0]=g0; CALL\truntime·stackcheck(SB) nog: // Call fn CALL\tR12 // It shouldn\u0026#39;t return. If it does, exit that thread. MOVL\t$111, DI MOVL\t$SYS_exit, AX SYSCALL JMP\t-3(PC)\t// keep exiting gdb\nlist /tmp/kubernets/clone_test/main.go:1 list /usr/lib/golang/src/runtime/sys_linux_amd64.s:540 gdb调试自定义函数\ndefine zxc info threads info register rbp rsp pc end [New LWP 32548] [Switching to LWP 32548] Breakpoint 3, runtime.clone () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:562 562\tMOVQ\tSI, SP ---------------------------------------------------------here 这一行的确是没必要,clone完后,系统会把子线程的sp寄存器设置为传入参数stk; m.stack.hi. (gdb) zxc Id Target Id Frame * 2 LWP 32548 \u0026#34;test\u0026#34; runtime.clone () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:562 1 LWP 32547 \u0026#34;test\u0026#34; runtime.clone () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:556 rbp 0x7fffffffe3b0\t0x7fffffffe3b0 rsp 0xc000044000\t0xc000044000 pc 0x455568\t0x455568 \u0026lt;runtime.clone+56\u0026gt; (gdb) info register sp sp 0xc000044000\t0xc000044000 (gdb) step 565\tCMPQ\tR8, $0 // m (gdb) info register sp sp 0xc000044000\t0xc000044000 notewakeup(\u0026amp;mp.park) # func notewakeup(n *note) { var v uintptr for { v = atomic.Loaduintptr(\u0026amp;n.key) if atomic.Casuintptr(\u0026amp;n.key, v, locked) { break } } // Successfully set waitm to locked. // What was it before? switch { case v == 0: // 我们从上面知道 v == note.key; note.key == M结构体的指针; 如果是0,那么这个note是无效的,不需要唤醒. // Nothing was waiting. Done. case v == locked: // Two notewakeups! Not allowed. throw(\u0026#34;notewakeup - double wakeup\u0026#34;) // 不能调用这个函数两次, locked的值是1,唤醒一次,这个note.key就变成1了,下次再调用就报错. default: // Must be the waiting m. Wake it up. semawakeup((*m)(unsafe.Pointer(v))) //从这个semawakeup(mp *m)需要的参数,可以知道note.key字段是一个M的指针. } } 附录 # clone系统调用\n所以这里必须为子线程指定其使用的栈，否则父子线程会共享同一个栈从而造成混乱，从上面的newosproc函数可以看出，新线程使用的栈为m.g0.stack.lo～m.g0.stack.hi这段内存，而这段内存是newm函数在创建m结构体对象时从进程的堆上分配而来的。\n//\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\nalter table ai_course_mission_section_relation add type smallint default 0 not null;\ncomment on column ai_course_mission_section_relation.type is \u0026lsquo;任务组的类型;0:默认是任务组 1:主题组\u0026rsquo;;\ncomment on column ai_mission_section.name is \u0026lsquo;任务分组/主题组名称\u0026rsquo;;\nalter table ai_mission_section add project_resource_id bigint;\ncomment on column ai_mission_section.project_resource_id is \u0026lsquo;主题需要工程的resourceID\u0026rsquo;;\nalter table ai_mission_section add pic_resource_id bigint;\ncomment on column ai_mission_section.pic_resource_id is \u0026lsquo;主题图片resourceID\u0026rsquo;;\ncreate table ai_mission_theme_relation ( mission_id bigint not null constraint \u0026ldquo;fk_mission_theme_relation_mission_id\u0026rdquo; references ai_mission on delete cascade, ai_mission_section_id bigint not null constraint fk_mission_theme_relation_mission_section_id references ai_mission_section, update_time bigint not null, serial_number smallint default 1 not null, id bigserial not null constraint mission_theme_relation_pkey primary key );\ncomment on column ai_mission_theme_relation.mission_id is \u0026lsquo;任务ID\u0026rsquo;;\ncomment on column ai_mission_theme_relation.ai_mission_section_id is \u0026lsquo;主题(这节课自带的主题ID才可以关联)ID\u0026rsquo;;\ncomment on column ai_mission_theme_relation.update_time is \u0026lsquo;修改时间\u0026rsquo;;\ncomment on column ai_mission_theme_relation.serial_number is \u0026lsquo;主题顺序\u0026rsquo;;\nalter table ai_mission_theme_relation owner to postgres;\ncreate index idx_mission_theme_relation_mission_id on ai_mission_theme_relation (mission_id);\nTODO 删除的时候可以连着删除 1000000个数,白天只查,晚上更新\n一个无序的数组长度为1001 ，数组内的元素值在[1,1000]间，只用基本数据结构找出1001中唯一重复的元素 \u0026mdash; （不能用map）\n将IPv4（192.168.1.1）转换成longl诶行\n一个程序 接收1亿个数 最后返回前100个最大的数\n"},{"id":6,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/golang_gc/","title":"垃圾回收","section":"运行时","content":" golang垃圾回收 # golang垃圾回收 理论 About GC 三色 新分配对象的颜色 写屏障 写屏障的步骤推导 插入写屏障 删除写屏障 混合写屏障 其他的变种 golang实现 扩展 mutator? FQA 谈一谈golang的gc. 二：为什么清扫阶段不需要屏障了呢？ 三：golang的heap结构 四: 工作队列相关的问题：并发标记的分工问题？写屏障记录集的竞争问题？ 五： CUP utilization TODO: 并发GC如何缓解内存分配压力？ 附录 内存分配器 heapArena span archive 202104-conclusion what,why,how 理论 # About GC # what: # gc主要是释放那些不再需要的分配在堆（heap）上的数据 why: # 降低人的心智负担，从堆（heap）中申请的内存，不需要手动的释放。 how: # 一般判断对象是否存活是使用：是把可达性近视的认为存活性.\n可把栈（stack），数据段（data segment? bss?）的数据对象作为root Mark-Sweep.\nSTW(stop the world) 三色标记法。\n黑色：当基于当前节点的追踪任务完成后，表示为存活数据，后续无需再追踪。 灰色：基于当前节点展开的追踪还未完成。 白色： conflict:\n当gc使用三色标记法与用户程序并发执行的时候，可能让黑色对象重新引用了白色对象，且无其他灰色对象引用了这个白色对象。导致白色对象错误回收。(用户程序和垃圾收集器可以在交替工作的) 强三色：黑色对象只能引用灰色对象。 弱三色：所有被黑色对象引用的白色对象都处于灰色保护状态. 读写屏障。\n读屏障：非移动式垃圾回收器中，天然不需要读屏障。 写屏障：会在写操作中插入指令，把数据对象的修改通知到垃圾回收器。 插入写屏障： writePointer(slot, ptr): shade(ptr) //shade函数尝试改变指针的颜色--\u0026gt;改变ptr的颜色 *slot = ptr 删除写屏障： 会在老对象的引用被删除时，将白色的老对象涂成灰色。 writePointer(slot, ptr) shade(*slot) //shade函数尝试改变指针的颜色--\u0026gt;改变(*slot)的颜色---\u0026gt;注意这个是*slot,slot保存着它原先的保存的地址。 *slot = ptr conflict: 只有插入写屏障，但是这需要对所有堆、栈的写操作都开启写屏障，代价太大。为了改善这个问题，改为忽略协程栈上的写屏障，只在标记结束阶段重新扫描那些被激活的栈帧。但是Go语言通常会有大量活跃的协程，这就导致第二次STW时重新扫描协程栈的时间太长。 混合写屏障：将被覆盖的对象(老对象)标记成灰色并在当前栈没有扫描时将新对象也标记成灰色。 writePointer(slot, ptr): shade(*slot) //将老对象标记为灰色。 if current stack is grey: // The insertion part of the barrier is necessary while the calling goroutine\u0026#39;s stack is grey. shade(ptr) *slot = ptr 既可忽略当前栈帧的写屏障。(不管是插入写屏障，还是删除写屏障.) 这里模拟很容易，黑色关联白色，且没有其他灰色关联这个黑色；就会出现hiding object。 模拟heap上触发删除写屏障。\u0026mdash;》所以stack上一个black object关联heap上面的white object；这时候heap上的其他颜色object断开与它的连接。 模拟heap上触发插入写屏障。 又不需要在第二次STW的时，重新扫描所有活跃G的栈帧。 三色 # 新分配对象的颜色 # 黑色赋值器：已经由回收器扫描过，不会再次对其进行扫描。 灰色赋值器：尚未被回收器扫描过，或尽管已经扫描过但仍需要重新扫描。\n如果新分配的对象为黑色或者灰色，则赋值器直接将其视为无需回收的对象，写入堆中； 如果新分配的对象为白色，则可以避免无意义的新对象保留到下一个垃圾回收的周期。\n由于黑色赋值器由于已经被回收器扫描过， 不会再对其进行任何扫描，一旦其分配新的白色对象 则意味着会导致错误的回收。\n写屏障 # 背景：如果对栈和堆都执行写屏障(不管是插入写屏障，还是删除写屏障)，那么按照三色原则，插入写屏障使它不满足强三色不变式，删除写屏障使它一定不满足弱三色不变式。两种写屏障都不会错误回收。但是基于性能考虑，栈上不触发写屏障。\nwritePointer(slot, ptr): *slot可能是null。 *slot可能是它以前关联的那个值，现在要断开。 ptr是现在将要关联的这个值。 引申的插入写屏障，删除写屏障\nwritePointer(slot, ptr): // 插入写屏障 shade(ptr) //shade函数尝试改变指针的颜色--\u0026gt;改变ptr的颜色 *slot = ptr writePointer(slot, ptr) // 删除写屏障 shade(*slot) //shade函数尝试改变指针的颜色--\u0026gt;改变(*slot)的颜色---\u0026gt;注意这个是*slot,slot保存着它原先的保存的地址。 *slot = ptr 写屏障的步骤推导 # |........标记清扫阶段...........| STW begin STW end 分类步骤:\nSTW扫描的位置点: 是在开始扫描还是在最后来一次扫描? STW扫描可以按照每个goroutine的方式来一个个扫描? 不必一次暂停所有goroutine栈 STW goroutine栈扫描: 栈上对象都变成黑色,一些堆对象(与栈对联直连)为灰色,其他的为白色.// TODO picture 创建的新对象是黑色还是白色? 防止类型就是:\nheap_black_object -\u0026gt; heap_white_object stack_black_object -\u0026gt; heap_white_object 插入写屏障 # 开始步骤 步骤 \\ 新生成对象的颜色 STW全栈扫描 堆上使用插入写屏障 \\ 新生成的对象都是黑色 STW goroutine栈扫描 堆上使用插入写屏障 \\ 新生成的对象都是黑色 堆上使用插入写屏障 STW全栈扫描 \\ 新生成的对象黑白都可 堆上使用插入写屏障 STW goroutine栈扫描 \\ 新生成的对象黑白都可 /**************(1)**************/ { STW全栈扫描 堆上使用插入写屏障 | [创建新对象为黑色] } /**************(2)**************/ { STW goroutine栈扫描 堆上使用插入写屏障 | [创建新对象为黑色] } /**************(3)**************/ { 堆上使用插入写屏障 | [创建新对象为可以为白色,也可以为黑色] STW全栈扫描 } /**************(4)**************/ { 堆上使用插入写屏障 | [创建新对象为可以为白色,也可以为黑色] STW goroutine栈扫描 } (1)(2)不行,如图: (3)可以,1.5到1.7就是使用的这种方式 (4)没必要了,都是最后STW,重新扫描一次灰色的goroutine栈\n删除写屏障 # 删除写屏障 起始快照整栈跨找，扫黑，使得整个堆上的在用对象都处于灰色保护； 整栈扫黑，那么在用的堆上的对象是一定处于灰色堆对象的保护下的，之后配合堆对象删除写屏障就能保证在用对象不丢失。 加入插入写屏障的逻辑，C 指向 D 的时候，把 D 置灰，这样扫描也没问题。这样就能去掉起始 STW 扫描，从而可以并发，一个一个栈扫描。 开始步骤 步骤 \\ 新生成对象的颜色 STW全栈扫描 堆上使用删除写屏障 \\ 新生成的对象都是黑色 STW goroutine栈扫描 堆上使用删除写屏障 \\ 新生成的对象都是黑色 堆上使用删除写屏障 STW全栈扫描 \\ 新生成的对象黑白都可 堆上使用删除写屏障 STW goroutine栈扫描 \\ 新生成的对象黑白都可 /**************(1)**************/ { STW全栈扫描 堆上使用删除写屏障 | [创建新对象为黑色] } /**************(2)**************/ { STW goroutine栈扫描 堆上使用删除写屏障 | [创建新对象为黑色] } /**************(3)**************/ { 堆上使用删除写屏障 | [创建新对象为可以为白色/黑色] STW全栈扫描 } /**************(4)**************/ { 堆上使用删除写屏障 | [创建新对象为可以为白色/黑色] STW goroutine栈扫描 } (1)可以 (2)不行,如图: (3)(4)的话,因为是最后进行栈重新扫描,可以知道不会出现stack_black_object -\u0026gt; heap_white_object这种情况. 如图: // TODO\n混合写屏障 # hybrid write barrier 由前面的删除写屏障(2),我们只要加上插入写屏障就能避免出现隐藏白色对象这种情况.\n这个也是if current stack is grey:的由来.\n步骤 步骤 \\ 新生成对象的颜色 STW goroutine栈扫描(要么全黑要么全白) 堆上使用删除写屏障+堆上使用插入写屏障 \\ 新生成的对象都是黑色 writePointer(slot, ptr): shade(*slot) if current stack is grey: shade(ptr) *slot = ptr The hybrid barrier requires that objects be allocated black (allocate-white is a common policy, but incompatible with this barrier).\n其他的变种 # writePointer(slot, ptr): shade(*slot) shade(ptr) *slot = ptr writePointer(slot, ptr): shade(*slot) if any stack is grey: shade(ptr) *slot = ptr writePointer(slot, ptr): shade(*slot) *slot = ptr golang实现 # https://docs.go101.org/std/src/runtime/mbarrier.go.html\n为什么gc也要扫描栈，不是都在堆上面吗？\n因为堆上的地址，可能保存在栈上某个变量里，所以需要扫描。 The hybrid barrier requires that objects be allocated black (allocate-white is a common policy, but incompatible with this barrier). 混合写屏障的时候要求新对象被分配为黑色（这里我猜是所有的对象，栈对象，或者堆对象也好） once a stack has been scanned and blackened, it remains black. 一旦栈被扫描（这个的扫描是第一次扫描，查找根对象的时候）和置为黑，那么它一直保持为黑 说明栈在第一次扫描的时候就会把栈上对象置为黑色？ In the hybrid barrier, the two shades and the condition work together shade(*slot): 从heap到stack移动白色对象，染为灰色。（尝试如果它试图从heap中解开一个对象的链接，就会对其进行遮挡。） shade(ptr): 从stack到一个黑色的heap对象。 Once a goroutine\u0026rsquo;s stack is black, the shade(ptr) becomes unnecessary. shade(ptr) prevents hiding an object by moving it from the stack to the heap, but this requires first having a pointer hidden on the stack. Immediately after a stack is scanned, it only points to shaded objects, so it\u0026rsquo;s not hiding anything, and the shade(*slot) prevents it from hiding any other pointers on its stack. 一个Goroutine写到另一个Goroutine: 我就是觉得混合写屏障好像也没法 解决重新扫描栈的问题。我举个例子： 现在有 A, B, C三个对象，A(黑色，栈上)，B（灰色，栈上），C（白色，堆上）； 当前引用关系是： A（黑） -\u0026gt; nil B（灰） -\u0026gt; C（白） 现在应用程序赋值修改，把A指向C： A（黑） -\u0026gt; C（白） B（灰） -\u0026gt; nil 由于A，B是栈上的对象，栈上对象赋值这里可是没有写屏障的；那么岂不是黑色对象指向白色对象了，C会回收了，就悬挂指针了？？？ Goroutine 栈扫描的过程需要 STW，所以你描述的这种状况是不存在的，栈上的对象要么全白要么全黑 你说的“栈上的对象要么全白，要么全黑“ ，这个只是对一个 goroutine 栈来说的（golang 暂停业务扫描栈也是一个一个来的）。如果场景是 A 在 Goroutine1，B在Goroutine2呢？这种情况就是A是黑色，B是白色或者灰色。这样会不会就有我说的原本那个问题呢？ The hybrid barrier assumes a goroutine cannot write to another goroutine\u0026rsquo;s stack.(混合写屏障假设一个goroutine,不能写到另一个goroutine的栈) 除了从一个Goroutine通过chan发送/生成一个新Goroutine 如果两个Goroutine 栈只要有一个是灰色的，那么就会有shade(ptr) 新生成的Goroutine栈都是黑色的（由前面的条件保证）,如果父Goroutine是灰色的，那么需使用shade(ptr) https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md\n扩展 # mutator? # “同GC并发执行的用户程序，源码与GC相关书籍中都称其为“mutator”（赋值器）”\nFQA # 谈一谈golang的gc. # 一：Golang中GC的实现采用的是标记——清扫算法，支持增量与并发式回收。非完全并发操作\n标记清除，不是像原始的，STW（stop the world）, 一个是暂停的时间太长，二个是用户的程序需暂停。它采用的是基于三色标记的混合写屏障。 增量和并发，一个是横向一个是纵向，\u0008gc与mutator交替与运行。所以这里也需要屏障技术。 横向：增量垃圾收集 — 增量地标记和清除垃圾，降低应用程序暂停的最长时间； 纵向：并发垃圾收集 — 利用多核的计算资源，在用户程序执行时并发标记和清除垃圾； 因为增量和并发两种方式都可以与用户程序交替运行，所以我们需要使用屏障技术保证垃圾收集的正确性. 使用混合写屏障的原因是缩短gc暂停的时间。\n因为栈上使用写屏障，会导致耗时太多。但是如果栈上不使用写屏障，等到第二次STW重新扫描栈空间，goroutine数目多，需要扫描的stack耗时也多。 忽略协程栈的写屏障;其他的使用删除写屏障，插入写屏障。 如果没有任何STW的时间，也就是说垃圾回收程序与用户程序完全并发执行，其代价与实现难度可能都会高于短暂的STW。例如标记——清扫回收器中，若完全抛弃STW，那么垃圾回收开始的消息便很难准确及时地通知到所有线程，可能导致某些线程开启写屏障的动作有所延迟而无法保障双方执行的正确性。\n二：为什么清扫阶段不需要屏障了呢？ # 当标记完成了，那么它白色的对象都是不可达的对象，是可以删除的对象，程序不可能再找到已经不可达的对象。所以放心的清除。 三：golang的heap结构 # 参见下面的内存分配器 Golang中GC的三色标记 -（1）着为灰色对应的操作就是把指针对应的gcmarkBits标记位置为1并加入工作队列； -（2）着为黑色对应的操作就是把指针对应的gcmarkBits标记位置为1。 -（3）白色对象就是那些gcMarkBits中标记为0的对象。 知道标记在哪了，那么如果进行分工？\n四: 工作队列相关的问题：并发标记的分工问题？写屏障记录集的竞争问题？ # 这里的竞争问题，我怀疑是： 一个是gc从root Object到一直遍历，打标记(从上到下)，同时另一个用户程序也在更新Object的关联（比如从从一个灰色到白色节点断开）\n其中用户程序的写到每个P中的写屏障缓冲区。\n前面提到了全局变量work中存储着全局工作队列缓存（work.full），其实每个P都有一个本地工作队列（p.gcw）和一个写屏障缓冲（p.wbBuf）。 p.gcw中有两个workbuf：wbuf1和wbuf2，添加任务时总是从wbuf1添加，wbuf1满了就交换wbuf1和wbuf2，如果还是满的，就把当前wbuf1的工作flush到全局工作缓存中去。 知道分工了，不可能占用很多CPU进行gc,这样会限制用户程序。\n五： CUP utilization # GC默认的CPU目标使用率为25%，在GC执行的初始化阶段，会根据当前CPU核数乘以CPU目标使用率来计算需要启动的mark worker数量。 TODO: 并发GC如何缓解内存分配压力？ # 借贷偿还机制。也可以偷。 附录 # 内存分配器 # //go:notinheap type mheap struct { //... // arenas is the heap arena map. It points to the metadata for // the heap for every arena frame of the entire usable virtual // address space. // // Use arenaIndex to compute indexes into this array. // // For regions of the address space that are not backed by the // Go heap, the arena map contains nil. // // Modifications are protected by mheap_.lock. Reads can be // performed without locking; however, a given entry can // transition from nil to non-nil at any time when the lock // isn\u0026#39;t held. (Entries never transitions back to nil.) // // In general, this is a two-level mapping consisting of an L1 // map and possibly many L2 maps. This saves space when there // are a huge number of arena frames. However, on many // platforms (even 64-bit), arenaL1Bits is 0, making this // effectively a single-level map. In this case, arenas[0] // will never be nil. arenas [1 \u0026lt;\u0026lt; arenaL1Bits]*[1 \u0026lt;\u0026lt; arenaL2Bits]*heapArena //... } //A heapArena stores metadata for a heap arena. // 每一个heapArena管理一个heap arena. type heapArena struct { bitmap [heapArenaBitmapBytes]byte spans [pagesPerArena]*mspan pageInUse [pagesPerArena / 8]uint8 pageMarks [pagesPerArena / 8]uint8 zeroedBase uintptr } type mspan struct { next *mspan prev *mspan //... startAddr uintptr // 起始地址 npages uintptr // 页数 freeindex uintptr allocBits *gcBits gcmarkBits *gcBits allocCache uint64 //startAddr 和 npages — 确定该结构体管理的多个页所在的内存，每个页的大小都是 8KB； //freeindex — 扫描页中空闲对象的初始索引； //allocBits 和 gcmarkBits — 分别用于标记内存的占用和回收情况； //allocCache — allocBits 的补码，可以用于快速查找内存中未被使用的内存； //... } mheap\nheapArena\nheapArena # mheap中每个arena对应一个HeapArena，记录arena的元数据信息。HeapArena中有一个bitmap和一个spans字段。 1.bitmap bitmap中每两个bit对应标记arena中一个指针大小的word，也就是说bitmap中一个byte可以标记arena中连续四个指针大小的内存。 每个word对应的两个bit中，低位bit用于标记是否为指针，0为非指针，1为指针；高位bit用于标记是否要继续扫描，高位bit为1就代表扫描完当前word并不能完成当前数据对象的扫描。 2.spans spans是一个*mspan类型的数组，用于记录当前arena中每一页对应到哪一个mspan。(看这个mspan的结构可以知道，它有startAddr与npages,说明一个mspan管理多个page) 基于HeapArena记录的元数据信息，我们只要知道一个对象的地址，\n就可以根据HeapArena.bitmap信息扫描它内部是否含有指针； 也可以根据对象地址计算出它在哪一页，然后通过HeapArena.spans信息查到该对象存在哪一个mspan中。\nspan # 而每个span都对应两个位图标记：mspan.allocBits和mspan.gcmarkBits。 （1）allocBits中每一位用于标记一个对象存储单元是否已分配。 （2）gcmarkBits中每一位用于标记一个对象是否存活。 archive # 堆（heap）上面的都有插入写屏障，不会发生hiding object。\n栈上是没有写屏障的， \u0026lt;堆与栈之间\u0026gt;或者\u0026lt;堆与堆\u0026gt;才会触发写屏障\n一个是插入写屏障 破坏第一个条件 记住插入，\u0026mdash;黑色\u0026mdash;-白色===》 黑色\u0026mdash;-灰色 writePointer(slot, ptr): shade(ptr) //shade函数尝试改变指针的颜色--\u0026gt;改变ptr的颜色 *slot = ptr 一个是删除写屏障 破坏第二个条件。 删除 writePointer(slot, ptr) shade(*slot) //shade函数尝试改变指针的颜色--\u0026gt;改变slot的颜色---\u0026gt;注意这个是*slot,slot保存着它原先的保存的地址。 *slot = ptr disadvantage: which means that many stacks must be re-scanned during STW. The garbage collector first scans all stacks at the beginning of the GC cycle to collect roots.(第一次扫描所有的栈在收集根对象的时候，不能保证在栈后面不会有黑对象引用了白对象) 混合写屏障 what: 混合写屏障为了消除栈的重扫过程. writePointer(slot, ptr): shade(*slot) if current stack is grey: shade(ptr) *slot = ptr why:\nhow: 通过几种方式来保证弱三色一致性\nSTW 扫描一次协程栈(扫一个Goroutine栈就变为黑色) + 创建对象默认黑色 开启写屏障期间创建的所有对象默认都是黑色 混合写屏障 + 两Goroutine之间交流 "},{"id":7,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/%E8%B0%83%E5%BA%A6/%E4%BA%8C-%E8%B0%83%E5%BA%A6%E7%9A%84%E6%97%B6%E6%9C%BA/4.2.3-%E5%89%A5%E5%A4%BA%E8%B0%83%E5%BA%A6/4.2.3.1-%E8%BF%90%E8%A1%8C%E7%94%A8%E6%88%B7%E4%BB%A3%E7%A0%81%E6%97%B6%E9%97%B4%E8%BF%87%E9%95%BF%E8%B0%83%E5%BA%A6/","title":"4.2.3.1 运行用户代码时间过长调度","section":"4.2.3 剥夺调度","content":" 运行时间过长被调度的情况 # 运行用户代码时间过长; 因为系统调用,导致时间过长. 系统监控 # 我们能想到有系统监控才能查看是否代码是否过长.\n开启系统监控 # sysmon永远for循环src/runtime/proc.go func main() { //... if GOARCH != \u0026#34;wasm\u0026#34; { // no threads on wasm yet, so no sysmon systemstack(func() { newm(sysmon, nil) }) } //... } 上一章我们也分析了这个newm函数;所以在进入schedule()函数之前,会先执行这个sysmon函数.\nsysmon # 这里我们只看抢占相关的,当retake返回非0,那么代表所有P不是空闲的状态,所以idle=0==\u0026gt;usleep(delay)只是休眠最少的时间,只有20us\n// Always runs without a P, so write barriers are not allowed. // //go:nowritebarrierrec func sysmon() { lock(\u0026amp;sched.lock) sched.nmsys++ //增加记录系统线程的值的个数 checkdead() unlock(\u0026amp;sched.lock) lasttrace := int64(0) idle := 0 // how many cycles in succession we had not wokeup somebody delay := uint32(0) for { if idle == 0 { // start with 20us sleep... delay = 20 } else if idle \u0026gt; 50 { // start doubling the sleep after 1ms... delay *= 2 } if delay \u0026gt; 10*1000 { // up to 10ms delay = 10 * 1000 } usleep(delay) //... // retake P\u0026#39;s blocked in syscalls // and preempt long running G\u0026#39;s // 抢占被系统调用阻塞的P和抢占长期运行的G if retake(now) != 0 { idle = 0 } else { idle++ } // check if we need to force a GC //... } } retake # 只有P是_Prunning or _Psyscall,才会进行抢占 _Prunning 连续运行超过10毫秒了，设置抢占请求. _Psyscall: 当程序没有工作需要做,且系统调用没有超过10ms就不进行系统调用抢占. 1和2说明这个程序没有工作需要做; 3说明系统调用还没超过10m func retake(now int64) uint32 { n := 0 // Prevent allp slice changes. This lock will be completely // uncontended unless we\u0026#39;re already stopping the world. lock(\u0026amp;allpLock) // We can\u0026#39;t use a range loop over allp because we may // temporarily drop the allpLock. Hence, we need to re-fetch // allp each time around the loop. for i := 0; i \u0026lt; len(allp); i++ { //遍历所有的P _p_ := allp[i] if _p_ == nil { // This can happen if procresize has grown // allp but not yet created new Ps. continue } pd := \u0026amp;_p_.sysmontick // 最后一次被sysmon观察到的tick s := _p_.status sysretake := false if s == _Prunning || s == _Psyscall { //只有当p处于 _Prunning 或 _Psyscall 状态时才会进行抢占 // Preempt G if it\u0026#39;s running for too long. t := int64(_p_.schedtick) // _p_.schedtick：每发生一次调度，调度器对该值加一 if int64(pd.schedtick) != t { // 监控线程监控到一次新的调度，所以重置跟sysmon相关的schedtick和schedwhen变量 pd.schedtick = uint32(t) pd.schedwhen = now } else if pd.schedwhen+forcePreemptNS \u0026lt;= now { // 1. 没有进第一个if语句内,说明:pd.schedtick == t; 说明(pd.schedwhen ～ now)这段时间未发生过调度; preemptone(_p_) // 2. 但是这个_P_上面的某个Goroutine被执行,一直在执行这个Goroutiine; 中间没有切换其他Goroutine,因为如果切会导致_P_.schedtick增长,导致进入第一个if语句内; // In case of syscall, preemptone() doesn\u0026#39;t // 3. 连续运行超过10毫秒了，设置抢占请求. // work, because there is no M wired to P. sysretake = true // 需要系统抢占 } } if s == _Psyscall { // P处于系统调用之中，需要检查是否需要抢占 // Retake P from syscall if it\u0026#39;s there for more than 1 sysmon tick (at least 20us). t := int64(_p_.syscalltick) // 用于记录系统调用的次数，主要由工作线程在完成系统调用之后加一 if !sysretake \u0026amp;\u0026amp; int64(pd.syscalltick) != t { // 不相等---说明已经不是上次观察到的系统调用,开始了一个新的系统调用,所以重置一下 pd.syscalltick = uint32(t) pd.syscallwhen = now continue } // On the one hand we don\u0026#39;t want to retake Ps if there is no other work to do, // but on the other hand we want to retake them eventually // because they can prevent the sysmon thread from deep sleep. // 1. _p_的本地运行队列没有Gs; runqempty(_p_)返回true // 2. 有空闲的P,或者有正在自旋状态的M(正在偷其他P队列的Gs); atomic.Load(\u0026amp;sched.nmspinning)+atomic.Load(\u0026amp;sched.npidle) \u0026gt; 0返回true // 3. 上次观测到的系统调用还没有超过10毫秒; pd.syscallwhen+10*1000*1000 \u0026gt; now返回true // - concluing: 当程序没有工作需要做,且系统调用没有超过10ms就不进行系统调用抢占. // - 1和2说明这个程序没有工作需要做; // - 3说明系统调用还没超过10ms if runqempty(_p_) \u0026amp;\u0026amp; atomic.Load(\u0026amp;sched.nmspinning)+atomic.Load(\u0026amp;sched.npidle) \u0026gt; 0 \u0026amp;\u0026amp; pd.syscallwhen+10*1000*1000 \u0026gt; now { continue } // Drop allpLock so we can take sched.lock. unlock(\u0026amp;allpLock) // Need to decrement number of idle locked M\u0026#39;s // (pretending that one more is running) before the CAS. // Otherwise the M from which we retake can exit the syscall, // increment nmidle and report deadlock. incidlelocked(-1) if atomic.Cas(\u0026amp;_p_.status, s, _Pidle) { // 需要抢占，则通过使用cas修改p的状态来获取p的使用权 if trace.enabled { // CAS: 工作线程此时此刻可能正好从系统调用返回了，也正在获取p的使用权 traceGoSysBlock(_p_) traceProcStop(_p_) } n++ _p_.syscalltick++ handoffp(_p_) // 寻找一个新的m出来接管P } incidlelocked(1) lock(\u0026amp;allpLock) } } unlock(\u0026amp;allpLock) return uint32(n) } 执行用户代码抢占 # preemptone设置了被抢占goroutine对应的g结构体中的 preempt成员为true和stackguard0成员为stackPreempt.\n// Tell the goroutine running on processor P to stop. // This function is purely best-effort. It can incorrectly fail to inform the // goroutine. It can send inform the wrong goroutine. Even if it informs the // correct goroutine, that goroutine might ignore the request if it is // simultaneously executing newstack. // No lock needs to be held. // Returns true if preemption request was issued. // The actual preemption will happen at some point in the future // and will be indicated by the gp-\u0026gt;status no longer being // Grunning func preemptone(_p_ *p) bool { mp := _p_.m.ptr() if mp == nil || mp == getg().m { return false } gp := mp.curg // gp == 被抢占的goroutine if gp == nil || gp == mp.g0 { return false } gp.preempt = true // 设置抢占信号preempt == true // Every call in a go routine checks for stack overflow by // comparing the current stack pointer to gp-\u0026gt;stackguard0. // Setting gp-\u0026gt;stackguard0 to StackPreempt folds // preemption into the normal stack overflow check. // (1\u0026lt;\u0026lt;(8*sys.PtrSize) - 1) \u0026amp; -1314 ---\u0026gt; 0xfffffffffffffade, 很大的数 gp.stackguard0 = stackPreempt //stackguard0==很大的数; 使被抢占的goroutine;在进行函数调用会去检查栈溢出;去处理抢占请求 return true } 如何读取标志,然后进行抢占 # 通过stackguard0以及preempt可以找到这个链路:morestack_noctxt()-\u0026gt;morestack()-\u0026gt;newstack().\n// TODO zxc: reference part0:汇编基础.md/go编译器加的函数头的部分.\nruntime·morestack # // morestack but not preserving ctxt. TEXT runtime·morestack_noctxt(SB),NOSPLIT,$0 MOVL\t$0, DX JMP\truntime·morestack(SB) /* * support for morestack */ // Called during function prolog when more stack is needed. // // The traceback routines see morestack on a g0 as being // the top of a stack (for example, morestack calling newstack // calling the scheduler calling newm calling gc), so we must // record an argument size. For that purpose, it has no arguments. TEXT runtime·morestack(SB),NOSPLIT,$0-0\t//开始是进行一些判断 // Cannot grow scheduler stack (m-\u0026gt;g0). //... // Cannot grow signal stack (m-\u0026gt;gsignal). //... //设置m-\u0026gt;morebuf的PC，SP，g为相对应的\u0026#39;main\u0026#39; // Called from f. // Set m-\u0026gt;morebuf to f\u0026#39;s caller. NOP\tSP\t// tell vet SP changed - stop checking offsets MOVQ\t8(SP), AX\t// f\u0026#39;s caller\u0026#39;s PC // 这里的路径比如我的： \u0026#39;main\u0026#39;---\u0026gt;\u0026#39;sub_function\u0026#39;。 // 但是抢占了，所以走下面的路径:-\u0026gt;morestack_noctxt()-\u0026gt;morestack()-\u0026gt;newstack() // 所以这里的f在我这里应该是main. // 需要注意morestack_noctxt与morestack使用的栈大小都是0，且他们的跳转没用call指令，使用的是JMP MOVQ\tAX, (m_morebuf+gobuf_pc)(BX) LEAQ\t16(SP), AX\t// f\u0026#39;s caller\u0026#39;s SP MOVQ\tAX, (m_morebuf+gobuf_sp)(BX) get_tls(CX) //... MOVQ\tg(CX), SI MOVQ\tSI, (m_morebuf+gobuf_g)(BX) //保存当前的寄存器信息到g-\u0026gt;sched中 // Set g-\u0026gt;sched to context in f. MOVQ\t0(SP), AX // f\u0026#39;s PC MOVQ\tAX, (g_sched+gobuf_pc)(SI) MOVQ\tSI, (g_sched+gobuf_g)(SI) LEAQ\t8(SP), AX // f\u0026#39;s SP MOVQ\tAX, (g_sched+gobuf_sp)(SI) // 在morestack里面就已经保存了sp的值。 MOVQ\tBP, (g_sched+gobuf_bp)(SI) MOVQ\tDX, (g_sched+gobuf_ctxt)(SI) //把g0设置为m当前运行的G; 把g0-\u0026gt;sched-\u0026gt;sp恢复到SP寄存器中; // Call newstack on m-\u0026gt;g0\u0026#39;s stack. MOVQ\tm_g0(BX), BX MOVQ\tBX, g(CX) MOVQ\t(g_sched+gobuf_sp)(BX), SP // 把g0的栈SP寄存器恢复到实际的寄存器中。所以下面就使用了g0的栈。 //调用newstack CALL\truntime·newstack(SB) CALL\truntime·abort(SB)\t// crash if newstack returns RET 总结就是： 如果它下一次被调度起来了，那么执行PC，又会重新到本函数头部执行，从上面分析也可以知道，这里的风险就是，如果执行过程没有调用其他函数，那么无法进行抢占，这个就是基于插入抢占，1.14基于信号抢占。 morestack类似于mcall 保存调用morestack函数的goroutine到它的sched成员。 将当前工作线程的g0与线程TLS关联； 将当前工作线程的g0栈恢复到CPU寄存器。 在g0栈中执行传入的参数。\u0026mdash;\u0026gt;这里是runtime·newstack(SB)函数。 newstack(SB) # func newstack() { thisg := getg() //到这里我们又是在g0栈里面。 //... gp := thisg.m.curg //这个就是原来的Goroutine. //... // NOTE: stackguard0 may change underfoot, if another thread // is about to try to preempt gp. Read it just once and use that same // value now and below. preempt := atomic.Loaduintptr(\u0026amp;gp.stackguard0) == stackPreempt //这里判断是否是抢占 打了stackguard0; // Be conservative about where we preempt. // We are interested in preempting user Go code, not runtime code. // If we\u0026#39;re holding locks, mallocing, or preemption is disabled, don\u0026#39;t // preempt. // This check is very early in newstack so that even the status change // from Grunning to Gwaiting and back doesn\u0026#39;t happen in this case. // That status change by itself can be viewed as a small preemption, // because the GC might change Gwaiting to Gscanwaiting, and then // this goroutine has to wait for the GC to finish before continuing. // If the GC is in some way dependent on this goroutine (for example, // it needs a lock held by the goroutine), that small preemption turns // into a real deadlock. if preempt { // 这里还检查了一系列的状态，如果满足就不抢占它了， 让它继续执行。 if thisg.m.locks != 0 || thisg.m.mallocing != 0 || thisg.m.preemptoff != \u0026#34;\u0026#34; || thisg.m.p.ptr().status != _Prunning { // Let the goroutine keep running for now. // gp-\u0026gt;preempt is set, so it will be preempted next time. gp.stackguard0 = gp.stack.lo + _StackGuard //还原stackguard0为正常值，表示我们已经处理过抢占请求了 gogo(\u0026amp;gp.sched) // never return } } if gp.stack.lo == 0 { throw(\u0026#34;missing stack in newstack\u0026#34;) } sp := gp.sched.sp if sys.ArchFamily == sys.AMD64 || sys.ArchFamily == sys.I386 || sys.ArchFamily == sys.WASM { // The call to morestack cost a word. sp -= sys.PtrSize } if stackDebug \u0026gt;= 1 || sp \u0026lt; gp.stack.lo { print(\u0026#34;runtime: newstack sp=\u0026#34;, hex(sp), \u0026#34; stack=[\u0026#34;, hex(gp.stack.lo), \u0026#34;, \u0026#34;, hex(gp.stack.hi), \u0026#34;]\\n\u0026#34;, \u0026#34;\\tmorebuf={pc:\u0026#34;, hex(morebuf.pc), \u0026#34; sp:\u0026#34;, hex(morebuf.sp), \u0026#34; lr:\u0026#34;, hex(morebuf.lr), \u0026#34;}\\n\u0026#34;, \u0026#34;\\tsched={pc:\u0026#34;, hex(gp.sched.pc), \u0026#34; sp:\u0026#34;, hex(gp.sched.sp), \u0026#34; lr:\u0026#34;, hex(gp.sched.lr), \u0026#34; ctxt:\u0026#34;, gp.sched.ctxt, \u0026#34;}\\n\u0026#34;) } if sp \u0026lt; gp.stack.lo { print(\u0026#34;runtime: gp=\u0026#34;, gp, \u0026#34;, goid=\u0026#34;, gp.goid, \u0026#34;, gp-\u0026gt;status=\u0026#34;, hex(readgstatus(gp)), \u0026#34;\\n \u0026#34;) print(\u0026#34;runtime: split stack overflow: \u0026#34;, hex(sp), \u0026#34; \u0026lt; \u0026#34;, hex(gp.stack.lo), \u0026#34;\\n\u0026#34;) throw(\u0026#34;runtime: split stack overflow\u0026#34;) } if preempt { if gp == thisg.m.g0 { throw(\u0026#34;runtime: preempt g0\u0026#34;) } if thisg.m.p == 0 \u0026amp;\u0026amp; thisg.m.locks == 0 { throw(\u0026#34;runtime: g is running but p is not\u0026#34;) } // Synchronize with scang. casgstatus(gp, _Grunning, _Gwaiting) // 设置gp状态变为等待状态。处理gc时把gp的状态修改成_Gwaiting if gp.preemptscan { //gc相关，暂时忽略。 for !castogscanstatus(gp, _Gwaiting, _Gscanwaiting) { // Likely to be racing with the GC as // it sees a _Gwaiting and does the // stack scan. If so, gcworkdone will // be set and gcphasework will simply // return. } if !gp.gcscandone { // gcw is safe because we\u0026#39;re on the // system stack. gcw := \u0026amp;gp.m.p.ptr().gcw scanstack(gp, gcw) gp.gcscandone = true } gp.preemptscan = false gp.preempt = false casfrom_Gscanstatus(gp, _Gscanwaiting, _Gwaiting) // This clears gcscanvalid. casgstatus(gp, _Gwaiting, _Grunning) gp.stackguard0 = gp.stack.lo + _StackGuard gogo(\u0026amp;gp.sched) // never return } // Act like goroutine called runtime.Gosched. casgstatus(gp, _Gwaiting, _Grunning) //恢复状态。 gopreempt_m(gp) // 放入全局队列，重新schedule(); never return === gopreempt_m(gp)---call---\u0026gt;goschedImpl(gp)----call--\u0026gt;globrunqput()放入全局队列/schedule() } //... } 这里就继续上面的，把替换掉的Goroutine重新放入全局队列：\ngopreempt_m(gp)---call---\u0026gt;goschedImpl(gp)----call--\u0026gt;globrunqput()放入全局队列/schedule()\n栈增长相关代码 # func newstack() { //...省略抢占的代码 // Allocate a bigger segment and move the stack. oldsize := gp.stack.hi - gp.stack.lo newsize := oldsize * 2 // 新的栈大小直接*2 if newsize \u0026gt; maxstacksize { print(\u0026#34;runtime: goroutine stack exceeds \u0026#34;, maxstacksize, \u0026#34;-byte limit\\n\u0026#34;) throw(\u0026#34;stack overflow\u0026#34;) } // The goroutine must be executing in order to call newstack, // so it must be Grunning (or Gscanrunning). casgstatus(gp, _Grunning, _Gcopystack) // The concurrent GC will not scan the stack while we are doing the copy since // the gp is in a Gcopystack status. copystack(gp, newsize, true) if stackDebug \u0026gt;= 1 { print(\u0026#34;stack grow done\\n\u0026#34;) } casgstatus(gp, _Gcopystack, _Grunning) gogo(\u0026amp;gp.sched) } func copystack(gp *g, newsize uintptr, sync bool) { //... // allocate new stack new := stackalloc(uint32(newsize)) //... } stackalloc // stackalloc allocates an n byte stack. // // stackalloc must run on the system stack because it uses per-P // resources and must not split the stack. // //go:systemstack func stackalloc(n uint32) stack { // Small stacks are allocated with a fixed-size free-list allocator. // If we need a stack of a bigger size, we fall back on allocating // a dedicated span. var v unsafe.Pointer if n \u0026lt; _FixedStack\u0026lt;\u0026lt;_NumStackOrders \u0026amp;\u0026amp; n \u0026lt; _StackCacheSize { //小堆栈用固定大小的自由列表分配器进行分配。 } else { //... if s == nil { // 如果我们需要一个更大的堆栈，我们会重新分配一个span. // Allocate a new stack from the heap. s = mheap_.allocManual(npage, \u0026amp;memstats.stacks_inuse) if s == nil { throw(\u0026#34;out of memory\u0026#34;) } osStackAlloc(s) s.elemsize = uintptr(n) } //... } //... } https://medium.com/a-journey-with-go/go-how-does-the-goroutine-stack-size-evolve-447fc02085e5#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjAzYjJkMjJjMmZlY2Y4NzNlZDE5ZTViOGNmNzA0YWZiN2UyZWQ0YmUiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MTIxNjkyMDEsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwMzk2OTgxODc3ODk3MjQyMjU0OSIsImVtYWlsIjoiZmZ6eGMuZG9AZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJ0aW0gWmhhbyIsInBpY3R1cmUiOiJodHRwczovL2xoNS5nb29nbGV1c2VyY29udGVudC5jb20vLVVrRk9jak5NWUJzL0FBQUFBQUFBQUFJL0FBQUFBQUFBQUFBL0FNWnV1Y25LajAwY21MVkhuZGpYakxGVVZWX1JHWnJ0OXcvczk2LWMvcGhvdG8uanBnIiwiZ2l2ZW5fbmFtZSI6InRpbSIsImZhbWlseV9uYW1lIjoiWmhhbyIsImlhdCI6MTYxMjE2OTUwMSwiZXhwIjoxNjEyMTczMTAxLCJqdGkiOiI5ZWZhMzYwMDUwOGNhZjg0MWMyYjQ5YmE1NDQxMWNkMGQzNmE0YzliIn0.qaSr0IXzZ3BXiIndb18-qLZpwNMDi3fEGlR-OtbQryxR8MkhONlgB-BTN5vHjYuvmWdCdGywJn26T71jPqBRVuIXMNlriZLFwPvHKdTBRrvpkPoFs8LprEpsCyZbTN7qNU5-CfDzcC1fHZji2j7992Ngo3XVd9v-6LiKCGUeolZ7CGH6KvT1e67ckiCzEN2oG5q6v7zKW32FmF7cajuOHl12p2pn6LaHxlYm3o38N3O9c96cO04meQ7WzZKn6QVoZeDIvmzq1iIKYOCbU0edZiOXgRgGHkBeBOowi-DHCz9kSJ1HkNrdzyjEC2nKUqerVGTCAc08w9BjtA8o_RmA8g\n执行系统调用抢占 # handoffp函数:判断是否需要启动工作线程来接管_p_，如果不需要则把_p_放入P的全局空闲队列.\n// Hands off P from syscall or locked M. // Always runs without a P, so write barriers are not allowed. //go:nowritebarrierrec func handoffp(_p_ *p) { // handoffp must start an M in any situation where // findrunnable would return a G to run on _p_. // if it has local work, start it straight away if !runqempty(_p_) || sched.runqsize != 0 { // 运行队列不为空，需要获得一个m来接管,而不是创建一个M结构体,和创建一个线程; startm(_p_, false) // 这个我们前面讨论过, return } // if it has GC work, start it straight away if gcBlackenEnabled != 0 \u0026amp;\u0026amp; gcMarkWorkAvailable(_p_) { // 如果有GC工作，就立即开始 startm(_p_, false) return } // no local work, check that there are no spinning/idle M\u0026#39;s, // otherwise our help is not required // 没有空闲的P,没有自旋状态的Ms;所有其它p都在运行goroutine，说明系统比较忙，需要启动m if atomic.Load(\u0026amp;sched.nmspinning)+atomic.Load(\u0026amp;sched.npidle) == 0 \u0026amp;\u0026amp; atomic.Cas(\u0026amp;sched.nmspinning, 0, 1) { // TODO: fast atomic startm(_p_, true) return } lock(\u0026amp;sched.lock) if sched.gcwaiting != 0 { _p_.status = _Pgcstop sched.stopwait-- if sched.stopwait == 0 { notewakeup(\u0026amp;sched.stopnote) } unlock(\u0026amp;sched.lock) return } if _p_.runSafePointFn != 0 \u0026amp;\u0026amp; atomic.Cas(\u0026amp;_p_.runSafePointFn, 1, 0) { sched.safePointFn(_p_) sched.safePointWait-- if sched.safePointWait == 0 { notewakeup(\u0026amp;sched.safePointNote) } } if sched.runqsize != 0 { // 全局运行队列大小不是0; 说明Goroutine需要运行,有工作要做. unlock(\u0026amp;sched.lock) startm(_p_, false) return } // If this is the last running P and nobody is polling network, // need to wakeup another M to poll network. // 所有其它P都已经处于空闲状态,只有自己一个P还在运行; // 且这时候需要监控网络连接读写事件，则需要启动新的m来poll网络连接 if sched.npidle == uint32(gomaxprocs-1) \u0026amp;\u0026amp; atomic.Load64(\u0026amp;sched.lastpoll) != 0 { unlock(\u0026amp;sched.lock) startm(_p_, false) return } pidleput(_p_) // 无事可做，把p放入全局空闲队列 unlock(\u0026amp;sched.lock) } 需要启动工作线程来接管P.\n_p_的本地运行队列或全局运行队列里面有待运行的goroutine； 需要帮助gc完成标记工作； 系统比较忙，所有其它_p_都在运行goroutine，需要帮忙； 所有其它P都已经处于空闲状态，如果需要监控网络连接读写事件，则需要启动新的m来poll网络连接。 其中startm函数我们前面介绍过. 额外的例子 # 定义程序 # main.go\npackage main import \u0026#34;fmt\u0026#34; func call_some_job() { fmt.Println(\u0026#34;complete this job\u0026#34;) } func main() { for i:=0; i\u0026lt;100000; i++{ i=i } call_some_job() } gdb调试前准备 # 编译程序 # 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n准备mcall函数断点的文件 # gdb list /usr/lib/golang/src/runtime/proc.go:267 list /tmp/kubernets/test_preempt/main.go:1 list /usr/lib/golang/src/runtime/asm_amd64.s:454 gdb调试自定义函数 # define zxc info threads info register rbp rsp pc end gdb # [root@gitlab test_preempt]# gdb ./test GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-119.el7 Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \u0026#34;show copying\u0026#34; and \u0026#34;show warranty\u0026#34; for details. This GDB was configured as \u0026#34;x86_64-redhat-linux-gnu\u0026#34;. For bug reporting instructions, please see: \u0026lt;http://www.gnu.org/software/gdb/bugs/\u0026gt;... Reading symbols from /tmp/kubernets/test_preempt/test...done. Loading Go Runtime support. (gdb) list 1\tpackage main 2 3\timport \u0026#34;fmt\u0026#34; 4 5\tfunc call_some_job() { 6 7\tfmt.Println(\u0026#34;complete this job\u0026#34;) 8\t} 9 10\tfunc main() { (gdb) 11\tcall_some_job() 12\t} (gdb) b 10 Breakpoint 1 at 0x48cf90: file /tmp/kubernets/test_preempt/main.go, line 10. (gdb) run Starting program: /tmp/kubernets/test_preempt/./test Breakpoint 1, main.main () at /tmp/kubernets/test_preempt/main.go:10 10\tfunc main() { (gdb) disas Dump of assembler code for function main.main: =\u0026gt; 0x000000000048cf90 \u0026lt;+0\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rcx --------------------------------here 0x000000000048cf99 \u0026lt;+9\u0026gt;: cmp 0x10(%rcx),%rsp --------------------------------here 0x000000000048cf9d \u0026lt;+13\u0026gt;:\tjbe 0x48cfb9 \u0026lt;main.main+41\u0026gt; 0x000000000048cf9f \u0026lt;+15\u0026gt;:\tsub $0x8,%rsp 0x000000000048cfa3 \u0026lt;+19\u0026gt;:\tmov %rbp,(%rsp) 0x000000000048cfa7 \u0026lt;+23\u0026gt;:\tlea (%rsp),%rbp 0x000000000048cfab \u0026lt;+27\u0026gt;:\tcallq 0x48cef0 \u0026lt;main.call_some_job\u0026gt; 0x000000000048cfb0 \u0026lt;+32\u0026gt;:\tmov (%rsp),%rbp 0x000000000048cfb4 \u0026lt;+36\u0026gt;:\tadd $0x8,%rsp 0x000000000048cfb8 \u0026lt;+40\u0026gt;:\tretq 0x000000000048cfb9 \u0026lt;+41\u0026gt;:\tcallq 0x4517d0 \u0026lt;runtime.morestack_noctxt\u0026gt; --------------------------------here 0x000000000048cfbe \u0026lt;+46\u0026gt;:\tjmp 0x48cf90 \u0026lt;main.main\u0026gt; End of assembler dump. 上面三个--------------------------------here,前面我们说的很清楚,就是g.stack.stackguard0与sp寄存器进行比较,如果sp小于g.stack.stackguard0 就跳转到runtime.morestack_noctxt;而我们前面设置preempt:gp.stackguard0 = stackPreempt //stackguard0==很大的数; 使被抢占的goroutine;在进行函数调用会去检查栈溢出;去处理抢占请求,它必定比sp要大,所以肯定跳转到了runtime.morestack_noctxt\nMOVQ\t0(SP), AX // f's PC,就是caller\u0026rsquo;s pc是因为它的rbp在那一步还没有保存到callee‘s stack空间.\n那继续来看如果如果调用\u0026lt;runtime.morestack_noctxt\u0026gt;,它的下一个PC就是jmp 0x48cf90 \u0026lt;main.main\u0026gt;又重新跳回来了. 看这个 "},{"id":8,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/%E8%B0%83%E5%BA%A6/%E4%BA%8C-%E8%B0%83%E5%BA%A6%E7%9A%84%E6%97%B6%E6%9C%BA/4.2.3-%E5%89%A5%E5%A4%BA%E8%B0%83%E5%BA%A6/4.2.3.2-%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%94%B6%E5%B0%BE%E5%A6%82%E6%9E%9C%E4%BB%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E8%BF%94%E5%9B%9E%E5%A6%82%E4%BD%95%E9%87%8D%E6%96%B0%E5%BE%97%E5%88%B0P/","title":"4.2.3.2 系统调用收尾,如从系统调用返回,如何重新得到P","section":"4.2.3 剥夺调度","content":" 定义程序 # main.go\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) var path = \u0026#34;appss.txt\u0026#34; func isError(err error) bool { if err != nil { fmt.Println(err.Error()) } return (err != nil) } func main() { var file, err = os.OpenFile(path, os.O_RDWR, 0644) if isError(err) { return } defer file.Close() } gdb调试前准备 # 编译程序 # 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n准备mcall函数断点的文件 # gdb list /usr/lib/golang/src/syscall/zsyscall_linux_amd64.go:62 list /usr/lib/golang/src/syscall/asm_linux_amd64.s:44 gdb # // func Syscall6(trap, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr) TEXT ·Syscall6(SB),NOSPLIT,$0-80 CALL\truntime·entersyscall(SB) MOVQ\ta1+8(FP), DI MOVQ\ta2+16(FP), SI MOVQ\ta3+24(FP), DX MOVQ\ta4+32(FP), R10 MOVQ\ta5+40(FP), R8 MOVQ\ta6+48(FP), R9 SYSCALL CMPQ\tAX, $0xfffffffffffff001 JLS\tok6 MOVQ\t$-1, r1+56(FP) MOVQ\t$0, r2+64(FP) NEGQ\tAX MOVQ\tAX, err+72(FP) CALL\truntime·exitsyscall(SB) RET ok6: MOVQ\tAX, r1+56(FP) MOVQ\tDX, r2+64(FP) MOVQ\t$0, err+72(FP) CALL\truntime·exitsyscall(SB) RET entersyscall # src/runtime/proc.go\nfunc entersyscall() { reentersyscall(getcallerpc(), getcallersp()) // 这个是Goroutine的pc, sp,不是g0的，因为还没有切换栈。 } reentersyscall # reentersyscall主要是做三件事: 把PC,SP保存到当前Goroutine.sched里面; 解除M与P两者之间的关系; 设置P的状态为_Psyscall /* - 把PC,SP保存到当前Goroutine.sched里面; - 解除M与P两者之间的关系; - 设置P的状态为_Psyscall */ func reentersyscall(pc, sp uintptr) { _g_ := getg() // get Goroutine的g // Disable preemption because during this function g is in Gsyscall status, // but can have inconsistent g-\u0026gt;sched, do not let GC observe it. _g_.m.locks++ // ++就能让GC不能观察到？TODO zxc: // Entersyscall must not call any function that might split/grow the stack. // (See details in comment above.) // Catch calls that might, by replacing the stack guard with something that // will trip any stack check and leaving a flag to tell newstack to die. _g_.stackguard0 = stackPreempt //进入系统调用前就设置了抢占标志。 _g_.throwsplit = true // Leave SP around for GC and traceback. save(pc, sp) //保存寄存器的值到当前Goroutine的sched结构体。 _g_.syscallsp = sp //gc使用 _g_.syscallpc = pc //gc使用 casgstatus(_g_, _Grunning, _Gsyscall) // 修改状态 if _g_.syscallsp \u0026lt; _g_.stack.lo || _g_.stack.hi \u0026lt; _g_.syscallsp { systemstack(func() { print(\u0026#34;entersyscall inconsistent \u0026#34;, hex(_g_.syscallsp), \u0026#34; [\u0026#34;, hex(_g_.stack.lo), \u0026#34;,\u0026#34;, hex(_g_.stack.hi), \u0026#34;]\\n\u0026#34;) throw(\u0026#34;entersyscall\u0026#34;) }) } if trace.enabled { systemstack(traceGoSysCall) // systemstack itself clobbers g.sched.{pc,sp} and we might // need them later when the G is genuinely blocked in a // syscall save(pc, sp) } if atomic.Load(\u0026amp;sched.sysmonwait) != 0 { systemstack(entersyscall_sysmon) save(pc, sp) } if _g_.m.p.ptr().runSafePointFn != 0 { // runSafePointFn may stack split if run on this stack systemstack(runSafePointFn) save(pc, sp) } _g_.m.syscalltick = _g_.m.p.ptr().syscalltick //把P的syscalltick,放到m中。 _g_.sysblocktraced = true _g_.m.mcache = nil pp := _g_.m.p.ptr() pp.m = 0 // 解除P与M的关系。 _g_.m.oldp.set(pp) // 把现在的P放到M中的oldp中。 _g_.m.p = 0 // 解除M与P的关系。 atomic.Store(\u0026amp;pp.status, _Psyscall) // 修改P的状态为系统调用。 if sched.gcwaiting != 0 { systemstack(entersyscall_gcwait) save(pc, sp) } _g_.m.locks-- // --解除锁定。 } 这里需要注意的是：在进入系统调用的时候，它是没有进行自增的，它是在exitsyscall()函数才开始进行自增的；\n这个就是为了判断P，在当前Goroutine进入系统调用，到返回的那一段时间，这个P有可能又被其他M关联，然后又进入_Psyscall状态，_g_.m.syscalltick = _g_.m.p.ptr().syscalltick //把P的syscalltick,放到m中。\nexitsyscall # 这个退出系统调用： 尝试重新绑定oldp,如果没有成功，从全局空闲P队列获得一个P。 如果还是失败，mcall\u0026ndash;\u0026gt;exitsyscall0()， 在这个里面再次从全局空闲P队列中尝试下，如果失败就把Goroutine放入全局空闲G队列; M放入全局空闲M队列,休眠M; schedule(). /* 这个退出系统调用： - 尝试重新绑定oldp,如果没有成功，从全局空闲P队列获得一个P。 - 如果还是失败，mcall--\u0026gt;exitsyscall0()， - 在这个里面再次从全局空闲P队列中尝试下，如果失败就把Goroutine放入全局空闲G队列; - M放入全局空闲M队列,休眠M; - schedule(). */ func exitsyscall() { _g_ := getg() _g_.m.locks++ // see comment in entersyscall 防止GC？ TODO zxc: if getcallersp() \u0026gt; _g_.syscallsp { throw(\u0026#34;exitsyscall: syscall frame is no longer valid\u0026#34;) } _g_.waitsince = 0 oldp := _g_.m.oldp.ptr() //重新取出oldp _g_.m.oldp = 0 if exitsyscallfast(oldp) { //如果返回true，那么M与P在这个里面已经重新关联了。 if _g_.m.mcache == nil { throw(\u0026#34;lost mcache\u0026#34;) } if trace.enabled { if oldp != _g_.m.p.ptr() || _g_.m.syscalltick != _g_.m.p.ptr().syscalltick { systemstack(traceGoStart) } } // There\u0026#39;s a cpu for us, so we can run. _g_.m.p.ptr().syscalltick++ //系统调用完成，syscalltick自增。 // We need to cas the status and scan before resuming... casgstatus(_g_, _Gsyscall, _Grunning) // Garbage collector isn\u0026#39;t running (since we are), // so okay to clear syscallsp. _g_.syscallsp = 0 _g_.m.locks-- if _g_.preempt { // restore the preemption request in case we\u0026#39;ve cleared it in newstack _g_.stackguard0 = stackPreempt } else { // otherwise restore the real _StackGuard, we\u0026#39;ve spoiled it in entersyscall/entersyscallblock _g_.stackguard0 = _g_.stack.lo + _StackGuard //在entersyscall里面我们设置_g_.stackguard0 = stackPreempt //进入系统调用前就设置了抢占标志。这里要恢复。 } _g_.throwsplit = false if sched.disable.user \u0026amp;\u0026amp; !schedEnabled(_g_) { // Scheduling of this goroutine is disabled. Gosched() } return } _g_.sysexitticks = 0 if trace.enabled { // Wait till traceGoSysBlock event is emitted. // This ensures consistency of the trace (the goroutine is started after it is blocked). for oldp != nil \u0026amp;\u0026amp; oldp.syscalltick == _g_.m.syscalltick { osyield() } // We can\u0026#39;t trace syscall exit right now because we don\u0026#39;t have a P. // Tracing code can invoke write barriers that cannot run without a P. // So instead we remember the syscall exit time and emit the event // in execute when we have a P. _g_.sysexitticks = cputicks() } _g_.m.locks-- // Call the scheduler. mcall(exitsyscall0) if _g_.m.mcache == nil { throw(\u0026#34;lost mcache\u0026#34;) } // Scheduler returned, so we\u0026#39;re allowed to run now. // Delete the syscallsp information that we left for // the garbage collector during the system call. // Must wait until now because until gosched returns // we don\u0026#39;t know for sure that the garbage collector // is not running. _g_.syscallsp = 0 _g_.m.p.ptr().syscalltick++ _g_.throwsplit = false } exitsyscallfast # //go:nosplit func exitsyscallfast(oldp *p) bool { _g_ := getg() // Freezetheworld sets stopwait but does not retake P\u0026#39;s. if sched.stopwait == freezeStopWait { return false } // Try to re-acquire the last P. if oldp != nil \u0026amp;\u0026amp; oldp.status == _Psyscall \u0026amp;\u0026amp; atomic.Cas(\u0026amp;oldp.status, _Psyscall, _Pidle) { /* - 查看老的P的状态是否是正处于_Psyscall; - 从reentersyscall里面的三个步骤，当它设置为_Psyscall, 它这个时候是没有与任何M相关联。 - 所以这里如果发现P又处于_psyscall，直接关联。 */ // There\u0026#39;s a cpu for us, so we can run. wirep(oldp) // 关联M和P；当前的M和这个oldp。 exitsyscallfast_reacquired() return true } // Try to get any other idle P. if sched.pidle != 0 { var ok bool systemstack(func() { ok = exitsyscallfast_pidle() if ok \u0026amp;\u0026amp; trace.enabled { if oldp != nil { // Wait till traceGoSysBlock event is emitted. // This ensures consistency of the trace (the goroutine is started after it is blocked). for oldp.syscalltick == _g_.m.syscalltick { osyield() } } traceGoSysExit(0) } }) if ok { return true } } return false } exitsyscallfast_reacquired # func exitsyscallfast_reacquired() { _g_ := getg() if _g_.m.syscalltick != _g_.m.p.ptr().syscalltick { // 如果他们两者不相等，那么说明该p被收回，然后再次进入syscall(因为_g_.m.syscalltick变了) if trace.enabled { // The p was retaken and then enter into syscall again (since _g_.m.syscalltick has changed). // traceGoSysBlock for this syscall was already emitted, // but here we effectively retake the p from the new syscall running on the same p. systemstack(func() { // Denote blocking of the new syscall. traceGoSysBlock(_g_.m.p.ptr()) // Denote completion of the current syscall. traceGoSysExit(0) }) } _g_.m.p.ptr().syscalltick++ // 这里又开始自增了---\u0026gt;因为它在进入reentersyscall()函数是不能增加这个值的。只有当退出exitsyscall()函数才会自增，所以如果 } } mcall(exitsyscall0) # // exitsyscall slow path on g0. // Failed to acquire P, enqueue gp as runnable. // //go:nowritebarrierrec func exitsyscall0(gp *g) { _g_ := getg() casgstatus(gp, _Gsyscall, _Grunnable) //从系统调用状态转变为可运行状态 dropg() //断开M与G之间的关系 lock(\u0026amp;sched.lock) //要修改全局的sched,先加锁 var _p_ *p if schedEnabled(_g_) { _p_ = pidleget() //从全局空闲P队列获取一个P } if _p_ == nil { globrunqput(gp) //如果没有获取P，那么把Goroutine放入全局空闲g队列。 } else if atomic.Load(\u0026amp;sched.sysmonwait) != 0 { atomic.Store(\u0026amp;sched.sysmonwait, 0) notewakeup(\u0026amp;sched.sysmonnote) } unlock(\u0026amp;sched.lock) if _p_ != nil { //如果有获取到P。 acquirep(_p_) // 关联P与M execute(gp, false) // Never returns. 直接执行 } if _g_.m.lockedg != 0 { // TODO zxc: 我记得是这个某个g,必须运行在某个线程上面，比如，main.main. // Wait until another thread schedules gp and so m again. stoplockedm() execute(gp, false) // Never returns. } stopm() //停止M。 schedule() // Never returns. } syscalltick # 这个syscalltick;发现不是每次系统调用一次，才增加一次。\n在我们这里，\nentersystem g.m.syscalltick = g.m.p.ptr().syscalltick existsystem exitsyscall主函数里面有一次; exitsyscallfast_reacquired函数又增加了一次. // To ensure that traceGoSysExit is emitted strictly after traceGoSysBlock, // we remember current value of syscalltick in m (g.m.syscalltick = g.m.p.ptr().syscalltick), // whoever emits traceGoSysBlock increments p.syscalltick afterwards; // and we wait for the increment before emitting traceGoSysExit. // Note that the increment is done even if tracing is not enabled, // because tracing can be enabled in the middle of syscall. We don\u0026rsquo;t want the wait to hang.\n// 为了确保traceGoSysExit严格在traceGoSysBlock之后发出。 // 我们记住m中syscalltick的当前值(g.m.syscalltick = g.m.p.ptr().syscalltick)。 // 不管是谁发出traceGoSysBlock，都会在之后增量p.syscalltick。 // 我们等待增量后再发出 traceGoSysExit。 // 注意，即使没有启用跟踪，增量也会被完成。 // 因为跟踪可以在syscall中间启用。我们不希望等待被挂起。\n在这个解释里面,发现跟踪的时候也会syscalltick\n"},{"id":9,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/%E8%B0%83%E5%BA%A6/","title":"调度","section":"运行时","content":" 基本框架 # part1: 进入main函数之前的初始化: begin \u0026mdash;\u0026gt; main的过程; 会包括gorountine是如何产生的 part2: 退出[exit]: 非main函数goroutine退出 part3: 调度: 调度循环 从一个goroutine,到如何进入下一个goroutine main函数运行go关键字, 创建新的goroutine, 放入队列, 等待调度.(这里可以参考part1的main Goroutine的创建过程) 调度的时机[调度时机，即什么时候会发生调度] 被动调度: goroutine执行某个操作因条件不满足需要等待而发生的调度； goroutine进入睡眠[比如是阻塞在chan上面,那么睡眠是:放在chan的goroutine挂入channel的读取队列]; 重新运行schedule() 唤醒睡眠中的goroutine; 唤醒空闲的P和唤醒创建工作线程; goroutine(被创建出来后/创建运行了一段时间后)如何放入运行队列[P中]; 主动调度: goroutine主动调用Gosched()函数让出CPU而发生的调度； 抢占调度: goroutine运行时间太长或长时间处于系统调用之中而被调度器剥夺运行权而发生的调度。 最终的路线图 # 为什么需要P?\nP的本地运行队列\n双端队列 怎么排列的? 是否是浪费了一个index空间. 本地运行队列是255还是256? // runqput tries to put g on the local runnable queue. // If next is false, runqput adds g to the tail of the runnable queue. // If next is true, runqput puts g in the _p_.runnext slot. // If the run queue is full, runnext puts g on the global queue. // Executed only by the owner P. func runqput(_p_ *p, gp *g, next bool) { if randomizeScheduler \u0026amp;\u0026amp; next \u0026amp;\u0026amp; fastrand()%2 == 0 { next = false } if next { retryNext: oldnext := _p_.runnext if !_p_.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) { goto retryNext } if oldnext == 0 { return } // Kick the old runnext out to the regular run queue. gp = oldnext.ptr() } retry: h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with consumers t := _p_.runqtail if t-h \u0026lt; uint32(len(_p_.runq)) { _p_.runq[t%uint32(len(_p_.runq))].set(gp) atomic.StoreRel(\u0026amp;_p_.runqtail, t+1) // store-release, makes the item available for consumption return } if runqputslow(_p_, gp, h, t) { return } // the queue is not full, now the put above must succeed goto retry } h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with consumers t := _p_.runqtail if t-h \u0026lt; uint32(len(_p_.runq)) { _p_.runq[t%uint32(len(_p_.runq))].set(gp) atomic.StoreRel(\u0026amp;_p_.runqtail, t+1) // store-release, makes the item available for consumption return } P初始化的时候看下这个,这三个字段是怎么初始化的? runqhead runqtail\n把golang assmble example 作为一篇博客 + golang的stack的结构是怎么样的. 初始化全局变量g0 runtime/asm_amd64.s // create istack out of the given (operating system) stack. // _cgo_init may update stackguard. MOVQ\t$runtime·g0(SB), DI 这个 $runtime.g0\u0026mdash;\u0026gt;\nhttps://github.com/golang/go/blob/d029058b5912312963225c40ae4bf44e3cb4be76/src/runtime/HACKING.md\n"},{"id":10,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/%E8%B0%83%E5%BA%A6/%E4%BA%8C-%E8%B0%83%E5%BA%A6%E7%9A%84%E6%97%B6%E6%9C%BA/","title":"调度时机","section":"调度","content":" 调度时机总结: 到这里我们的调度时机就总结完了，主要是什么时候发生调度: 主动调度 被动调度 剥夺调度 运行时间过长 系统调用被剥夺P "},{"id":11,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/%E8%B0%83%E5%BA%A6/3_exit_goroutine/","title":"goroutine退出","section":"调度","content":" goroutine退出过程 # 退出 # 非main goroutine 最后会运行goexit() main goroutine 虽然也定义了goexit(),模拟好像是是goexit函数调用的,但是这个main goroutine在src/runtime/proc.go文件里,这个函数返回到上一层. exit(0),就会退出. 如果上面没有退出,下面的for循环会再次保证\u0008程序将不会到上一层再继续执行. // The main goroutine. func main() { //... exit(0) for { var x *int32 *x = 0 } } 例子 # 我们首先来gdb调试一下这个程序\nmain.go\npackage main import \u0026#34;time\u0026#34; // the function\u0026#39;s body is empty func add(x, y int64) int64 func main() { go add(2, 3) time.Sleep(time.Minute) } add_amd.s\nTEXT ·add(SB),$0-24 MOVQ x+0(FP), BX MOVQ y+8(FP), BP ADDQ BP, BX MOVQ BX, ret+16(FP) RET 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n非main goroutine退出 # gdb断点\n//list /tmp/kubernets/main.go:3 list /tmp/kubernets/add_amd.s:3 list /usr/lib/golang/src/runtime/asm_amd64.s:1356 (gdb) disas Dump of assembler code for function main.add: 0x0000000000456b40 \u0026lt;+0\u0026gt;:\tmov 0x8(%rsp),%rbx =\u0026gt; 0x0000000000456b45 \u0026lt;+5\u0026gt;:\tmov 0x10(%rsp),%rbp 0x0000000000456b4a \u0026lt;+10\u0026gt;:\tadd %rbp,%rbx 0x0000000000456b4d \u0026lt;+13\u0026gt;:\tmov %rbx,0x18(%rsp) 0x0000000000456b52 \u0026lt;+18\u0026gt;:\tretq End of assembler dump. (gdb) step 4\tADDQ BP, BX (gdb) step 5\tMOVQ BX, ret+16(FP) (gdb) disas Dump of assembler code for function main.add: 0x0000000000456b40 \u0026lt;+0\u0026gt;:\tmov 0x8(%rsp),%rbx 0x0000000000456b45 \u0026lt;+5\u0026gt;:\tmov 0x10(%rsp),%rbp 0x0000000000456b4a \u0026lt;+10\u0026gt;:\tadd %rbp,%rbx 0x0000000000456b4d \u0026lt;+13\u0026gt;:\tmov %rbx,0x18(%rsp) =\u0026gt; 0x0000000000456b52 \u0026lt;+18\u0026gt;:\tretq End of assembler dump. (gdb) step Single stepping until exit from function main.add, which has no line number information. Breakpoint 1, runtime.goexit () at /usr/lib/golang/src/runtime/asm_amd64.s:1358 1358\tCALL\truntime·goexit1(SB)\t// does not return (gdb) list 1353 1354\t// The top-most function running on a goroutine 1355\t// returns to goexit+PCQuantum. 1356\tTEXT runtime·goexit(SB),NOSPLIT,$0-0 1357\tBYTE\t$0x90\t// NOP 1358\tCALL\truntime·goexit1(SB)\t// does not return 1359\t// traceback from goexit1 must hit code range of goexit 1360\tBYTE\t$0x90\t// NOP 1361 1362\t// This is called from .init_array and follows the platform, not Go, ABI. (gdb) (gdb) step runtime.goexit1 () at /usr/lib/golang/src/runtime/proc.go:2663 2663\tfunc goexit1() { 2670\tmcall(goexit0) // goexist1()--\u0026gt;mcall(goexit0) (gdb) step runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:294 294\tMOVQ\tfn+0(FP), DI (gdb) list 289\t// func mcall(fn func(*g)) 290\t// Switch to m-\u0026gt;g0\u0026#39;s stack, call fn(g). 291\t// Fn must never return. It should gogo(\u0026amp;g-\u0026gt;sched) 292\t// to keep running g. 293\tTEXT runtime·mcall(SB), NOSPLIT, $0-8 294\tMOVQ\tfn+0(FP), DI 295 296\tget_tls(CX) 297\tMOVQ\tg(CX), AX\t// save state in g-\u0026gt;sched 298\tMOVQ\t0(SP), BX\t// caller\u0026#39;s PC (gdb) 可以看到从add函数返回后,跳转到了\n第一步 第二步 第三步 src/runtime/asm_amd64.s src/runtime/proc.go src/runtime/asm_amd64.s runtime.goexit () goexit1() runtime.mcall() // The top-most function running on a goroutine // returns to goexit+PCQuantum. TEXT runtime·goexit(SB),NOSPLIT,$0-0 BYTE\t$0x90\t// NOP CALL\truntime·goexit1(SB)\t// does not return // traceback from goexit1 must hit code range of goexit BYTE\t$0x90\t// NOP // Finishes execution of the current goroutine. func goexit1() { if raceenabled { //忽略 racegoend() } if trace.enabled { //忽略 traceGoEnd() } mcall(goexit0) } // func mcall(fn func(*g)) // Switch to m-\u0026gt;g0\u0026#39;s stack, call fn(g). // Fn must never return. It should gogo(\u0026amp;g-\u0026gt;sched) // to keep running g. TEXT runtime·mcall(SB), NOSPLIT, $0-8 MOVQ\tfn+0(FP), DI //参数 get_tls(CX) MOVQ\tg(CX), AX // save state in gN-\u0026gt;sched MOVQ\t0(SP), BX // caller\u0026#39;s PC --\u0026gt;看下方的图 MOVQ\tBX, (g_sched+gobuf_pc)(AX) //保存caller\u0026#39;s pc到正在运行的gN.sched.pc LEAQ\tfn+0(FP), BX // caller\u0026#39;s SP MOVQ\tBX, (g_sched+gobuf_sp)(AX) //保存caller\u0026#39;s sp到正在运行的gN.sched.sp MOVQ\tAX, (g_sched+gobuf_g)(AX) //保存gN到正在运行的gN.sched.g MOVQ\tBP, (g_sched+gobuf_bp)(AX) //保存bp到正在运行的gN.sched.bp // switch to m-\u0026gt;g0 \u0026amp; its stack, call fn MOVQ\tg(CX), BX // bx=gN MOVQ\tg_m(BX), BX // bx=gN.m MOVQ\tm_g0(BX), SI // si=gN.m.g0 CMPQ\tSI, AX // if g == m-\u0026gt;g0 call badmcall; 这个gN不能等于g0, g0应该是用户调度用的. JNE\t3(PC) MOVQ\t$runtime·badmcall(SB), AX JMP\tAX MOVQ\tSI, g(CX)\t// g = m-\u0026gt;g0; 就是把m.tls[0](TLS)的值从gN的地址换为g0的地址,这样线程通过fs寄存器能找到g0继而找到m -----------here MOVQ\t(g_sched+gobuf_sp)(SI), SP // sp = m-\u0026gt;g0-\u0026gt;sched.sp,把g0的寄存器SP恢复到真实的SP ---------------here PUSHQ\tAX //gN压栈,作为后面call的参数 MOVQ\tDI, DX //dx = di(fn函数结构体) MOVQ\t0(DI), DI //所以这里是取真正的fn CALL\tDI //开始调用fn POPQ\tAX MOVQ\t$runtime·badmcall2(SB), AX JMP\tAX RET 分析mcall函数 # 我们主要来看下mcall()函数, 看下这个mcall函数的参数,函数指针\nMOVQ DI, DX //dx = di(fn函数) MOVQ 0(DI), DI //所以这里是取真正的fn 函数变量并不是一个直接指向函数代码的指针，而是一个指向funcval结构体对象的指针，funcval结构体对象的第一个成员fn才是真正指向函数代码的指针.\ntype funcval struct { fn uintptr // variable-size, fn-specific data here } 总结mcall\n保存当前g的调度信息,寄存器保存到g.sched; 把g0设置到tls中，修改CPU的rsp寄存器使其指向g0的栈; 以当前运行的g(我们这个场景是gN)为参数调用fn函数(此处为goexit0). mcall和gogo的代码非常相似，然而mcall和gogo在做切换时有个重要的区别: 都要切栈\ngogo函数在从g0切换到其它goroutine时首先切换了栈，然后通过跳转指令从runtime代码切换到了用户goroutine的代码; 而mcall函数在从其它goroutine切换回g0时只切换了栈，并未使用跳转指令跳转到runtime代码去执行. 为什么会有这个差别呢？\n原因在于在从g0切换到其它goroutine之前执行的是runtime的代码而且使用的是g0栈，所以切换时需要首先切换栈然后再从runtime代码跳转某个goroutine的代码去执行; (切换栈和跳转指令不能颠倒，因为跳转之后执行的就是用户的goroutine代码了，没有机会切换栈了); 然而从某个goroutine切换回g0时，goroutine使用的是call指令来调用mcall函数，mcall函数本身就是runtime的代码; 所以call指令其实已经完成了从goroutine代码到runtime代码的跳转，因此mcall函数自身的代码就不需要再跳转了，只需要把栈切换到g0栈即可. goexit0函数 # // goexit continuation on g0. func goexit0(gp *g) { _g_ := getg() // g0 casgstatus(gp, _Grunning, _Gdead) // 修改gN的状态 if isSystemGoroutine(gp, false) { atomic.Xadd(\u0026amp;sched.ngsys, -1) } gp.m = nil locked := gp.lockedm != 0 gp.lockedm = 0 _g_.m.lockedg = 0 gp.paniconfault = false gp._defer = nil // should be true already but just in case. gp._panic = nil // non-nil for Goexit during panic. points at stack-allocated data. gp.writebuf = nil gp.waitreason = 0 gp.param = nil gp.labels = nil gp.timer = nil //... // Note that gp\u0026#39;s stack scan is now \u0026#34;valid\u0026#34; because it has no // stack. gp.gcscanvalid = true dropg() //dropg函数解除g和m之间的关系，其实就是设置g-\u0026gt;m = nil, m-\u0026gt;currg = nil. //... gfput(_g_.m.p.ptr(), gp) //放在gfree列表中,如果本地列表太长，则将一个批次转移到全局列表中. //... schedule() } 退出流程 # goexit() goexit1() mcall() goexit0() schedule()\n"},{"id":12,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/%E8%B0%83%E5%BA%A6/%E4%B8%89-%E8%B0%83%E5%BA%A6%E5%BE%AA%E7%8E%AF/4.3.1-%E8%B0%83%E5%BA%A6%E5%BE%AA%E7%8E%AF/","title":"调度循环","section":"调度循环","content":" 调度循环 # 当go程序初始化到运行package main里面的main函数,g0已经被初始化,g.sched和stack被赋值,当下次切换Goroutine的时候,或者说再次调度的时候, 必然要重新使用g0,那么会重新使用g.sched.PC,g.sched.SP? g0栈是否重新使用初始时候mstart1函数的栈 # main.go\npackage main import \u0026#34;fmt\u0026#34; // the function\u0026#39;s body is empty func add(x, y int64) int64 func main() { gg:=add(2, 3) fmt.Println(gg) } add_amd.s\nTEXT ·add(SB),$0-24 MOVQ x+0(FP), BX MOVQ y+8(FP), BP ADDQ BP, BX MOVQ BX, ret+16(FP) RET 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n[root@gitlab kubernets]# gdb test GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-119.el7 Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \u0026#34;show copying\u0026#34; and \u0026#34;show warranty\u0026#34; for details. This GDB was configured as \u0026#34;x86_64-redhat-linux-gnu\u0026#34;. For bug reporting instructions, please see: \u0026lt;http://www.gnu.org/software/gdb/bugs/\u0026gt;... Reading symbols from /tmp/kubernets/test...done. Loading Go Runtime support. (gdb) list /usr/lib/golang/src/runtime/asm_amd64.s:294 289\t// func mcall(fn func(*g)) 290\t// Switch to m-\u0026gt;g0\u0026#39;s stack, call fn(g). 291\t// Fn must never return. It should gogo(\u0026amp;g-\u0026gt;sched) 292\t// to keep running g. 293\tTEXT runtime·mcall(SB), NOSPLIT, $0-8 294\tMOVQ\tfn+0(FP), DI 295 296\tget_tls(CX) 297\tMOVQ\tg(CX), AX\t// save state in g-\u0026gt;sched 298\tMOVQ\t0(SP), BX\t// caller\u0026#39;s PC (gdb) 299\tMOVQ\tBX, (g_sched+gobuf_pc)(AX) 300\tLEAQ\tfn+0(FP), BX\t// caller\u0026#39;s SP 301\tMOVQ\tBX, (g_sched+gobuf_sp)(AX) 302\tMOVQ\tAX, (g_sched+gobuf_g)(AX) 303\tMOVQ\tBP, (g_sched+gobuf_bp)(AX) 304 305\t// switch to m-\u0026gt;g0 \u0026amp; its stack, call fn 306\tMOVQ\tg(CX), BX 307\tMOVQ\tg_m(BX), BX 308\tMOVQ\tm_g0(BX), SI (gdb) 309\tCMPQ\tSI, AX\t// if g == m-\u0026gt;g0 call badmcall 310\tJNE\t3(PC) 311\tMOVQ\t$runtime·badmcall(SB), AX 312\tJMP\tAX 313\tMOVQ\tSI, g(CX)\t// g = m-\u0026gt;g0 314\tMOVQ\t(g_sched+gobuf_sp)(SI), SP\t// sp = m-\u0026gt;g0-\u0026gt;sched.sp 315\tPUSHQ\tAX 316\tMOVQ\tDI, DX 317\tMOVQ\t0(DI), DI 318\tCALL\tDI (gdb) 319\tPOPQ\tAX 320\tMOVQ\t$runtime·badmcall2(SB), AX 321\tJMP\tAX 322\tRET 323 324\t// systemstack_switch is a dummy routine that systemstack leaves at the bottom 325\t// of the G stack. We need to distinguish the routine that 326\t// lives at the bottom of the G stack from the one that lives 327\t// at the top of the system stack because the one at the top of 328\t// the system stack terminates the stack walk (see topofstack()). (gdb) b 318 Breakpoint 1 at 0x44b229: file /usr/lib/golang/src/runtime/asm_amd64.s, line 318. (gdb) c The program is not being run. (gdb) run Starting program: /tmp/kubernets/test [New LWP 29827] [Switching to LWP 29827] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) info register rbp rsp pc rbp 0xc000030fa0\t0xc000030fa0 rsp 0xc000045fd0\t0xc000045fd0 pc 0x44b229\t0x44b229 \u0026lt;runtime.mcall+89\u0026gt; (gdb) disas Dump of assembler code for function runtime.mcall: 0x000000000044b1d0 \u0026lt;+0\u0026gt;:\tmov 0x8(%rsp),%rdi 0x000000000044b1d5 \u0026lt;+5\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rax 0x000000000044b1de \u0026lt;+14\u0026gt;:\tmov (%rsp),%rbx 0x000000000044b1e2 \u0026lt;+18\u0026gt;:\tmov %rbx,0x40(%rax) 0x000000000044b1e6 \u0026lt;+22\u0026gt;:\tlea 0x8(%rsp),%rbx 0x000000000044b1eb \u0026lt;+27\u0026gt;:\tmov %rbx,0x38(%rax) 0x000000000044b1ef \u0026lt;+31\u0026gt;:\tmov %rax,0x48(%rax) 0x000000000044b1f3 \u0026lt;+35\u0026gt;:\tmov %rbp,0x68(%rax) 0x000000000044b1f7 \u0026lt;+39\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rbx 0x000000000044b200 \u0026lt;+48\u0026gt;:\tmov 0x30(%rbx),%rbx 0x000000000044b204 \u0026lt;+52\u0026gt;:\tmov (%rbx),%rsi 0x000000000044b207 \u0026lt;+55\u0026gt;:\tcmp %rax,%rsi 0x000000000044b20a \u0026lt;+58\u0026gt;:\tjne 0x44b215 \u0026lt;runtime.mcall+69\u0026gt; 0x000000000044b20c \u0026lt;+60\u0026gt;:\tlea -0x23343(%rip),%rax # 0x427ed0 \u0026lt;runtime.badmcall\u0026gt; 0x000000000044b213 \u0026lt;+67\u0026gt;:\tjmpq *%rax 0x000000000044b215 \u0026lt;+69\u0026gt;:\tmov %rsi,%fs:0xfffffffffffffff8 0x000000000044b21e \u0026lt;+78\u0026gt;:\tmov 0x38(%rsi),%rsp 0x000000000044b222 \u0026lt;+82\u0026gt;:\tpush %rax 0x000000000044b223 \u0026lt;+83\u0026gt;:\tmov %rdi,%rdx 0x000000000044b226 \u0026lt;+86\u0026gt;:\tmov (%rdi),%rdi =\u0026gt; 0x000000000044b229 \u0026lt;+89\u0026gt;:\tcallq *%rdi 0x000000000044b22b \u0026lt;+91\u0026gt;:\tpop %rax // callq的下一个指令 0x000000000044b22b -------------------here 0x000000000044b22c \u0026lt;+92\u0026gt;:\tlea -0x23323(%rip),%rax # 0x427f10 \u0026lt;runtime.badmcall2\u0026gt; 0x000000000044b233 \u0026lt;+99\u0026gt;:\tjmpq *%rax 0x000000000044b235 \u0026lt;+101\u0026gt;:\tretq End of assembler dump. (gdb) step runtime.park_m (gp=0xc000000780) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { (gdb) info register rbp rsp pc rbp 0xc000030fa0\t0xc000030fa0 rsp 0xc000045fc8\t0xc000045fc8 pc 0x42d490\t0x42d490 \u0026lt;runtime.park_m\u0026gt; (gdb) x/32xb 0xc000045fc0 0xc000045fc0:\t0x80\t0x0a\t0x00\t0x00\t0xc0\t0x00\t0x00\t0x00 0xc000045fc8:\t0x2b\t0xb2\t0x44\t0x00\t0x00\t0x00\t0x00\t0x00 // 0x000000000044b22b -------------------here 0xc000045fd0:\t0x80\t0x07\t0x00\t0x00\t0xc0\t0x00\t0x00\t0x00 0xc000045fd8:\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 (gdb) disas Dump of assembler code for function runtime.park_m: =\u0026gt; 0x000000000042d490 \u0026lt;+0\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rcx 0x000000000042d499 \u0026lt;+9\u0026gt;:\tcmp 0x10(%rcx),%rsp 0x000000000042d49d \u0026lt;+13\u0026gt;:\tjbe 0x42d642 \u0026lt;runtime.park_m+434\u0026gt; //判断是否越界 0x000000000042d4a3 \u0026lt;+19\u0026gt;:\tsub $0x28,%rsp 0x000000000042d4a7 \u0026lt;+23\u0026gt;:\tmov %rbp,0x20(%rsp) 0x000000000042d4ac \u0026lt;+28\u0026gt;:\tlea 0x20(%rsp),%rbp 0x000000000042d4b1 \u0026lt;+33\u0026gt;:\tmov %fs:0xfffffffffffffff8,%rax 0x000000000042d4ba \u0026lt;+42\u0026gt;:\tmov %rax,0x18(%rsp) 0x000000000042d4bf \u0026lt;+47\u0026gt;:\tcmpb $0x0,0xaf4ca(%rip) # 0x4dc990 \u0026lt;runtime.trace+16\u0026gt; 0x000000000042d4c6 \u0026lt;+54\u0026gt;:\tjne 0x42d61e \u0026lt;runtime.park_m+398\u0026gt; ---Type \u0026lt;return\u0026gt; to continue, or q \u0026lt;return\u0026gt; to quit---q Quit (gdb) 从\u0026mdash;-here可以看出来,当使用mcallg0\u0026rsquo;s stack\u0026gt;,只是使用了g0的栈,没有用g.sched.pc,而是用的mcall里面的callq的下一个指令 0x000000000044b22b -------------------here\n文件src/runtime/proc.go\n// park continuation on g0. func park_m(gp *g) { _g_ := getg() if trace.enabled { traceGoPark(_g_.m.waittraceev, _g_.m.waittraceskip) } casgstatus(gp, _Grunning, _Gwaiting) dropg() if fn := _g_.m.waitunlockf; fn != nil { ok := fn(gp, _g_.m.waitlock) _g_.m.waitunlockf = nil _g_.m.waitlock = nil if !ok { if trace.enabled { traceGoUnpark(gp, 2) } casgstatus(gp, _Gwaiting, _Grunnable) execute(gp, true) // Schedule it back, never returns. } } schedule() } 每个线程对应g0中g0.sched.SP会一直改变? # 在初始化后就不会改变了,下次调度,直接把g0.sched.SP重新赋值给寄存器SP.\ntype g struct { //... sched gobuf //... } type gobuf struct { sp uintptr -------------------------------here pc uintptr g guintptr ctxt unsafe.Pointer ret sys.Uintreg lr uintptr bp uintptr // for GOEXPERIMENT=framepointer } 示例: # 实践想法 # 有两个go关键字运行起来的Goroutine 当main goroutine睡眠的时候\u0026mdash;-[切换到g0栈]\u0026mdash;\u0026ndash;\u0026gt;其中一个Goroutine\u0026mdash;-[切换到g0栈]\u0026ndash;\u0026gt;另一个Goroutine或者main goroutine 所以只要看下这个切换的时候,mcall函数中(具体可看part2中:非main goroutine退出)MOVQ\t(g_sched+gobuf_sp)(SI), SP是否一直不变 // func mcall(fn func(*g)) // Switch to m-\u0026gt;g0\u0026#39;s stack, call fn(g). ------------------------here TEXT runtime·mcall(SB), NOSPLIT, $0-8 MOVQ\tfn+0(FP), DI get_tls(CX) MOVQ\tg(CX), AX\t// save state in g-\u0026gt;sched MOVQ\t0(SP), BX\t// caller\u0026#39;s PC MOVQ\tBX, (g_sched+gobuf_pc)(AX) LEAQ\tfn+0(FP), BX\t// caller\u0026#39;s SP MOVQ\tBX, (g_sched+gobuf_sp)(AX) MOVQ\tAX, (g_sched+gobuf_g)(AX) MOVQ\tBP, (g_sched+gobuf_bp)(AX) // switch to m-\u0026gt;g0 \u0026amp; its stack, call fn MOVQ\tg(CX), BX MOVQ\tg_m(BX), BX MOVQ\tm_g0(BX), SI CMPQ\tSI, AX\t// if g == m-\u0026gt;g0 call badmcall JNE\t3(PC) MOVQ\t$runtime·badmcall(SB), AX JMP\tAX MOVQ\tSI, g(CX)\t// g = m-\u0026gt;g0 MOVQ\t(g_sched+gobuf_sp)(SI), SP\t// sp = m-\u0026gt;g0-\u0026gt;sched.sp -------------------here PUSHQ\tAX MOVQ\tDI, DX MOVQ\t0(DI), DI CALL\tDI POPQ\tAX MOVQ\t$runtime·badmcall2(SB), AX JMP\tAX RET 定义程序 # main.go\npackage main import ( \u0026#34;runtime\u0026#34; \u0026#34;sync/atomic\u0026#34; ) var existFlag int32 = 2 // the function\u0026#39;s body is empty func addAssemble(x, y int64) int64 func add(a int64){ addAssemble(a, a) atomic.AddInt32(\u0026amp;existFlag, -1) } func main() { runtime.GOMAXPROCS(1) go add(1) go add(2) for { if atomic.LoadInt32(\u0026amp;existFlag) \u0026lt;=0{ break } runtime.Gosched() } } add_amd.s\nTEXT ·addAssemble(SB),$0-24 MOVQ x+0(FP), BX MOVQ y+8(FP), BP ADDQ BP, BX MOVQ BX, ret+16(FP) RET gdb调试前准备 # 编译程序 # 编译一下源代码: go build -gcflags \u0026quot;-N -l\u0026quot; -o test ..\n准备mcall函数断点的文件 # gdb list /usr/lib/golang/src/runtime/asm_amd64.s:300 list /tmp/kubernets/test_goroutine/add_amd.s:1 gdb调试自定义函数 # define zxc info threads info register rbp rsp pc step continue end gdb # [root@gitlab test_goroutine]# gdb ./test GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-119.el7 Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \u0026#34;show copying\u0026#34; and \u0026#34;show warranty\u0026#34; for details. This GDB was configured as \u0026#34;x86_64-redhat-linux-gnu\u0026#34;. For bug reporting instructions, please see: \u0026lt;http://www.gnu.org/software/gdb/bugs/\u0026gt;... Reading symbols from /tmp/kubernets/test_goroutine/test...done. Loading Go Runtime support. (gdb) list /usr/lib/golang/src/runtime/asm_amd64.s:300 295 296\tget_tls(CX) 297\tMOVQ\tg(CX), AX\t// save state in g-\u0026gt;sched 298\tMOVQ\t0(SP), BX\t// caller\u0026#39;s PC 299\tMOVQ\tBX, (g_sched+gobuf_pc)(AX) 300\tLEAQ\tfn+0(FP), BX\t// caller\u0026#39;s SP 301\tMOVQ\tBX, (g_sched+gobuf_sp)(AX) 302\tMOVQ\tAX, (g_sched+gobuf_g)(AX) 303\tMOVQ\tBP, (g_sched+gobuf_bp)(AX) 304 (gdb) 305\t// switch to m-\u0026gt;g0 \u0026amp; its stack, call fn 306\tMOVQ\tg(CX), BX 307\tMOVQ\tg_m(BX), BX 308\tMOVQ\tm_g0(BX), SI 309\tCMPQ\tSI, AX\t// if g == m-\u0026gt;g0 call badmcall 310\tJNE\t3(PC) 311\tMOVQ\t$runtime·badmcall(SB), AX 312\tJMP\tAX 313\tMOVQ\tSI, g(CX)\t// g = m-\u0026gt;g0 314\tMOVQ\t(g_sched+gobuf_sp)(SI), SP\t// sp = m-\u0026gt;g0-\u0026gt;sched.sp (gdb) 315\tPUSHQ\tAX 316\tMOVQ\tDI, DX 317\tMOVQ\t0(DI), DI 318\tCALL\tDI 319\tPOPQ\tAX 320\tMOVQ\t$runtime·badmcall2(SB), AX 321\tJMP\tAX 322\tRET 323 324\t// systemstack_switch is a dummy routine that systemstack leaves at the bottom (gdb) b 318 Breakpoint 1 at 0x44a2c9: file /usr/lib/golang/src/runtime/asm_amd64.s, line 318. (gdb) define zxc Type commands for definition of \u0026#34;zxc\u0026#34;. End with a line saying just \u0026#34;end\u0026#34;. \u0026gt;info threads \u0026gt;info register rbp rsp pc \u0026gt;step \u0026gt;continue \u0026gt;end (gdb) zxc No threads. The program has no registers now. (gdb) run Starting program: /tmp/kubernets/test_goroutine/./test Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) zxc Id Target Id Frame * 1 LWP 18958 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 rbp 0xc000030660\t0xc000030660 rsp 0x7fffffffe440\t0x7fffffffe440 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000300) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [New LWP 19129] [Switching to LWP 19129] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame * 2 LWP 19129 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 ------------------here 1 LWP 18958 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 rbp 0xc000031f30\t0xc000031f30 rsp 0xc00003ffd0\t0xc00003ffd0 ------------------here pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000d80) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [Switching to LWP 18958] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame 2 LWP 19129 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 * 1 LWP 18958 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 rbp 0xc000030660\t0xc000030660 rsp 0x7fffffffe440\t0x7fffffffe440 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000300) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [Switching to LWP 19129] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame * 2 LWP 19129 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 ------------------here 1 LWP 18958 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 rbp 0xc000031798\t0xc000031798 rsp 0xc00003ffd0\t0xc00003ffd0 ------------------here pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000c00) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [New LWP 18963] [Switching to LWP 18963] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame * 3 LWP 18963 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 2 LWP 19129 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 1 LWP 18958 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 rbp 0xc000030fa0\t0xc000030fa0 rsp 0xc000045fd0\t0xc000045fd0 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000780) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [Switching to LWP 18958] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame 3 LWP 18963 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 2 LWP 19129 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 * 1 LWP 18958 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 rbp 0xc000030720\t0xc000030720 rsp 0x7fffffffe440\t0x7fffffffe440 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.gosched_m (gp=0xc000000300) at /usr/lib/golang/src/runtime/proc.go:2635 2635\tfunc gosched_m(gp *g) { Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame 3 LWP 18963 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 2 LWP 19129 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 * 1 LWP 18958 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 rbp 0xc000032fc8\t0xc000032fc8 rsp 0x7fffffffe440\t0x7fffffffe440 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.goexit0 (gp=0xc000001380) at /usr/lib/golang/src/runtime/proc.go:2674 2674\tfunc goexit0(gp *g) { Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI (gdb) Id Target Id Frame 3 LWP 18963 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 2 LWP 19129 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 * 1 LWP 18958 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 rbp 0xc0000327c8\t0xc0000327c8 rsp 0x7fffffffe440\t0x7fffffffe440 pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.goexit0 (gp=0xc000001200) at /usr/lib/golang/src/runtime/proc.go:2674 2674\tfunc goexit0(gp *g) { [LWP 19129 exited] [LWP 18963 exited] [Inferior 1 (process 18958) exited normally] (gdb) 观察------------------here,可以知道每个M有自己的g0,其中的g.sched.SP初始化以后就不会改变 都是rsp 0xc00003ffd0\t0xc00003ffd0 如果不清楚可以看下线程1* 1 LWP 18958 \u0026quot;test\u0026quot; //... Id Target Id Frame * 2 LWP 19129 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 ------------------here 1 LWP 18958 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 rbp 0xc000031f30\t0xc000031f30 rsp 0xc00003ffd0\t0xc00003ffd0 ------------------here pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000d80) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [Switching to LWP 18958] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI //... Id Target Id Frame * 2 LWP 19129 \u0026#34;test\u0026#34; runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 ------------------here 1 LWP 18958 \u0026#34;test\u0026#34; runtime.futex () at /usr/lib/golang/src/runtime/sys_linux_amd64.s:536 rbp 0xc000031798\t0xc000031798 rsp 0xc00003ffd0\t0xc00003ffd0 ------------------here pc 0x44a2c9\t0x44a2c9 \u0026lt;runtime.mcall+89\u0026gt; runtime.park_m (gp=0xc000000c00) at /usr/lib/golang/src/runtime/proc.go:2594 2594\tfunc park_m(gp *g) { [New LWP 18963] [Switching to LWP 18963] Breakpoint 1, runtime.mcall () at /usr/lib/golang/src/runtime/asm_amd64.s:318 318\tCALL\tDI //... "},{"id":13,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/%E8%B0%83%E5%BA%A6/%E4%B8%80-%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/4.1.2-%E7%9B%97%E5%8F%96goroutine%E4%BB%8E%E5%85%B6%E4%BB%96%E9%98%9F%E5%88%97/","title":"盗取goroutine","section":"调度策略","content":" 从其他队列盗取goroutine # 盗取Goroutine的过程就是:尽力去各个运行队列中寻找goroutine，如果实在找不到则进入睡眠状态.\nfindrunnable函数 # // Finds a runnable goroutine to execute. // Tries to steal from other P\u0026#39;s, get g from global queue, poll network. func findrunnable() (gp *g, inheritTime bool) { _g_ := getg() // The conditions here and in handoffp must agree: if // findrunnable would return a G to run, handoffp must start // an M. top: _p_ := _g_.m.p.ptr() //... // local runq //再次看一下本地运行队列是否有需要运行的goroutine if gp, inheritTime := runqget(_p_); gp != nil { return gp, inheritTime } // global runq //再看看全局运行队列是否有需要运行的goroutine if sched.runqsize != 0 { lock(\u0026amp;sched.lock) gp := globrunqget(_p_, 0) unlock(\u0026amp;sched.lock) if gp != nil { return gp, false } } //... // Steal work from other P\u0026#39;s. // 使用所有P与空闲的P进行比较，如果除了自己，其他的P都是休眠状态。那么整个系统都没有工作需要做了。 procs := uint32(gomaxprocs) if atomic.Load(\u0026amp;sched.npidle) == procs-1 { // Either GOMAXPROCS=1 or everybody, except for us, is idle already. // New work can appear from returning syscall/cgocall, network or timers. // Neither of that submits to local run queues, so no point in stealing. goto stop //直接退出 } // If number of spinning M\u0026#39;s \u0026gt;= number of busy P\u0026#39;s, block. // This is necessary to prevent excessive CPU consumption // when GOMAXPROCS\u0026gt;\u0026gt;1 but the program parallelism is low. // 当这个M不是自旋状态，并且此时的二倍的自旋M大于当前正在工作的P; 说明此时有许多现在在寻找工作做. if !_g_.m.spinning \u0026amp;\u0026amp; 2*atomic.Load(\u0026amp;sched.nmspinning) \u0026gt;= procs-atomic.Load(\u0026amp;sched.npidle) { goto stop } if !_g_.m.spinning { //设置m的状态为spinning _g_.m.spinning = true //处于spinning状态的m数量加一 atomic.Xadd(\u0026amp;sched.nmspinning, 1) } //从其它p的本地运行队列盗取goroutine for i := 0; i \u0026lt; 4; i++ { for enum := stealOrder.start(fastrand()); !enum.done(); enum.next() { if sched.gcwaiting != 0 { goto top } stealRunNextG := i \u0026gt; 2 // first look for ready queues with more than 1 g if gp := runqsteal(_p_, allp[enum.position()], stealRunNextG); gp != nil { return gp, false } } } stop: //... // Before we drop our P, make a snapshot of the allp slice, // which can change underfoot once we no longer block // safe-points. We don\u0026#39;t need to snapshot the contents because // everything up to cap(allp) is immutable. allpSnapshot := allp // return P and block lock(\u0026amp;sched.lock) //... if sched.runqsize != 0 { gp := globrunqget(_p_, 0) unlock(\u0026amp;sched.lock) return gp, false } // 当前工作线程解除与p之间的绑定，准备去休眠 if releasep() != _p_ { throw(\u0026#34;findrunnable: wrong p\u0026#34;) } //把p放入空闲队列 pidleput(_p_) unlock(\u0026amp;sched.lock) // Delicate dance: thread transitions from spinning to non-spinning state, // potentially concurrently with submission of new goroutines. We must // drop nmspinning first and then check all per-P queues again (with // #StoreLoad memory barrier in between). If we do it the other way around, // another thread can submit a goroutine after we\u0026#39;ve checked all run queues // but before we drop nmspinning; as the result nobody will unpark a thread // to run the goroutine. // If we discover new work below, we need to restore m.spinning as a signal // for resetspinning to unpark a new worker thread (because there can be more // than one starving goroutine). However, if after discovering new work // we also observe no idle Ps, it is OK to just park the current thread: // the system is fully loaded so no spinning threads are required. // Also see \u0026#34;Worker thread parking/unparking\u0026#34; comment at the top of the file. wasSpinning := _g_.m.spinning if _g_.m.spinning { //m即将睡眠，状态不再是spinning _g_.m.spinning = false if int32(atomic.Xadd(\u0026amp;sched.nmspinning, -1)) \u0026lt; 0 { throw(\u0026#34;findrunnable: negative nmspinning\u0026#34;) } } // check all runqueues once again // 休眠之前再看一下是否有工作要做 for _, _p_ := range allpSnapshot { if !runqempty(_p_) { lock(\u0026amp;sched.lock) _p_ = pidleget() unlock(\u0026amp;sched.lock) if _p_ != nil { acquirep(_p_) if wasSpinning { _g_.m.spinning = true atomic.Xadd(\u0026amp;sched.nmspinning, 1) } goto top } break } } ...... //休眠 stopm() goto top } 随机遍历从其他的工作线程对应P上盗取G # 尽量减少M的自旋状态时间,只有在盗取的时候才把spinning标志位设为true,盗取退出后把spinning标志位重新设置为false, if !_g_.m.spinning { _g_.m.spinning = true atomic.Xadd(\u0026amp;sched.nmspinning, 1) } 随机遍历全局P中的P,如果有goroutines,就偷盗一半来运行. 工作线程进入睡眠 # src/runtime/proc.go:1910\n停止执行当前的m，直到有新的工作。带着获得的P返回。\n// Stops execution of the current m until new work is available. // Returns with acquired P. func stopm() { _g_ := getg() if _g_.m.locks != 0 { throw(\u0026#34;stopm holding locks\u0026#34;) } if _g_.m.p != 0 { throw(\u0026#34;stopm holding p\u0026#34;) } if _g_.m.spinning { throw(\u0026#34;stopm spinning\u0026#34;) } lock(\u0026amp;sched.lock) mput(_g_.m) //全局的sched.midle空闲队列 unlock(\u0026amp;sched.lock) notesleep(\u0026amp;_g_.m.park) //进入睡眠 noteclear(\u0026amp;_g_.m.park) acquirep(_g_.m.nextp.ptr()) _g_.m.nextp = 0 } note是go runtime实现的一次性睡眠和唤醒机制.\n这里的notesleep( *note)是一个抽象函数,根据不同的平台或系统有不同的实现.\ndragonfly freebsd linux[src/runtime/lock_futex.go] futex系统调用 futexsleep函数 aix darwin nacl netbsd openbsd plan9 solaris window[src/runtime/lock_sema.go] POSIX Threads js,was[src/runtime/lock_js.go] 不支持 从上面可以看出,note层增加对特定平台的支持,就可以复用上层代码. notesleep # func notesleep(n *note) { gp := getg() if gp != gp.m.g0 { throw(\u0026#34;notesleep not on g0\u0026#34;) } ns := int64(-1) if *cgo_yield != nil { // Sleep for an arbitrary-but-moderate interval to poll libc interceptors. ns = 10e6 } for atomic.Load(key32(\u0026amp;n.key)) == 0 { // --------here gp.m.blocked = true futexsleep(key32(\u0026amp;n.key), 0, ns) //进入睡眠 if *cgo_yield != nil { asmcgocall(*cgo_yield, nil) } gp.m.blocked = false } } 从这里可以看出来:如果futexsleep()退出,但是检查note.key还是为0,那么又会进入睡眠,for atomic.Load(key32(\u0026amp;n.key)) == 0 { // --------here,\n并不是其它工作线程唤醒了这个线程,所以我们知道当其他线程唤醒这个线程,需要改下这个线程对应的m结构体中note.key字段\ntype m struct { //... park note //... } "},{"id":14,"href":"/go-goroutine/docs/%E8%BF%90%E8%A1%8C%E6%97%B6/%E8%B0%83%E5%BA%A6/%E4%B8%80-%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/4.1.1-%E4%BB%8E%E6%9C%AC%E5%9C%B0%E9%98%9F%E5%88%97%E6%88%96%E5%85%A8%E5%B1%80%E9%98%9F%E5%88%97%E8%8E%B7%E5%8F%96goroutine/","title":"从队列获取goroutine","section":"调度策略","content":" 从本地队列或全局队列获取goroutine # 分析schedule函数 # src/runtime/proc.go\n// One round of scheduler: find a runnable goroutine and execute it. // Never returns. func schedule() { _g_ := getg() //... top: //... var gp *g var inheritTime bool //... if gp == nil { // Check the global runnable queue once in a while to ensure fairness. // Otherwise two goroutines can completely occupy the local runqueue // by constantly respawning each other. if _g_.m.p.ptr().schedtick%61 == 0 \u0026amp;\u0026amp; sched.runqsize \u0026gt; 0 { //schedtick就是调度次数,如果能被61整除且全局的Goroutine队列不为空就尝试获取 lock(\u0026amp;sched.lock) gp = globrunqget(_g_.m.p.ptr(), 1) //全局运行队列中获取goroutine unlock(\u0026amp;sched.lock) } } if gp == nil { gp, inheritTime = runqget(_g_.m.p.ptr()) //本地运行队列(当前线程)中获取goroutine if gp != nil \u0026amp;\u0026amp; _g_.m.spinning { throw(\u0026#34;schedule: spinning with local work\u0026#34;) } } if gp == nil { /* 则调用findrunnable函数从其它工作线程的运行队列中偷取，如果偷取不到，则当前工作线程进入睡眠, 直到获取到需要运行的goroutine之后findrunnable函数才会返回. */ gp, inheritTime = findrunnable() // blocks until work is available } // This thread is going to run a goroutine and is not spinning anymore, // so if it was marked as spinning we need to reset it now and potentially // start a new spinning M. if _g_.m.spinning { resetspinning() } //... execute(gp, inheritTime) } schedule函数按顺序分三步分别来获取可运行的Goroutine: 全局队列，本地G队列和其他P上面的G队列 第一步:从全局运行G队列中寻找Goroutine。 因为全局上面拿Goroutine是需要加锁的,尽量少从上面拿; 所以当一个P调度了61次之后且全局的Goroutine队列不为空，为了保证调度能运行全局G队列，所以尝试调度一下。 第二步:从工作线程本地运行队列[其实就是当前线程关联的P上面的运行G队列，因为一个P在同一时刻只能与一个M关联，此时就不需要加锁]中寻找Goroutine。 第三步:从P的运行队列中偷取Goroutine。如果上一步也没有，则调用findrunnable从其他工作线程的运行队列中偷取Goroutine，在偷取之前,findrunnable函数会再次尝试从全局运行G队列和当前P运行G队列中查找需要运行的Goroutine。 全局运行队列中获取goroutine # // Try get a batch of G\u0026#39;s from the global runnable queue. // Sched must be locked. func globrunqget(_p_ *p, max int32) *g { if sched.runqsize == 0 { //如果全局的G队列为空,直接返回nil return nil } /* - 如果gomaxprocs==1;sched.runqsize==1; - 导致(n==2)\u0026gt;sched.runqsize[全局的队列长度1]; - 需要判断下，取两者少的数; */ n := sched.runqsize/gomaxprocs + 1 //按照P的数量平分全局队列 if n \u0026gt; sched.runqsize { // n = sched.runqsize //取两者少的数 } if max \u0026gt; 0 \u0026amp;\u0026amp; n \u0026gt; max { n = max //取两者少的数 } if n \u0026gt; int32(len(_p_.runq))/2 { //取本地队列的一半长最多 n = int32(len(_p_.runq)) / 2 } sched.runqsize -= n gp := sched.runq.pop() //返回第一个,其他放入本地队列 n-- for ; n \u0026gt; 0; n-- { gp1 := sched.runq.pop() runqput(_p_, gp1, false) //-------------------------here /* 这里如果put G到本地满了,它又会put到全局. If the run queue is full, runnext puts g on the global queue. */ } return gp } 从上面的注释很容易看懂,只有一个需要注意,-------------------------here\n取本地队列的一半长最多 如果本身本地队列就是快满了,后面for循环继续加就会导致本地队列满了; 所以runqput函数又会把Goroutine加到全局里面去. runqput # /* - runqput尝试将g放在本地可运行队列中。 1. \u0026gt;如果 next 为 false，runqput 将 g 加到可运行队列的尾部。 2. 如果 next 为真，runqput 将 g 放在 _p_.runnext 槽中。 3. 如果运行队列满了，runnext就把g放到全局队列中。 */ func runqput(_p_ *p, gp *g, next bool) { if randomizeScheduler \u0026amp;\u0026amp; next \u0026amp;\u0026amp; fastrand()%2 == 0 { next = false } if next { //第二步所说的 retryNext: oldnext := _p_.runnext if !_p_.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) { goto retryNext } if oldnext == 0 { return } // Kick the old runnext out to the regular run queue. gp = oldnext.ptr() } retry: h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with consumers t := _p_.runqtail if t-h \u0026lt; uint32(len(_p_.runq)) { _p_.runq[t%uint32(len(_p_.runq))].set(gp) atomic.StoreRel(\u0026amp;_p_.runqtail, t+1) // store-release, makes the item available for consumption return } if runqputslow(_p_, gp, h, t) { return } // the queue is not full, now the put above must succeed goto retry } 本地运行队列(当前线程)中获取Goroutine # 本地运行队列其实分为两个部分: 一部分是由P的runq、runqhead和runqtail这三个成员组成的一个无锁循环队列，该队列最多可包含256个Goroutine; 另一部分是P的runnext成员，它是一个指向g结构体对象的指针，它最多只包含一个Goroutine. type p struct{ //... runqhead uint32 runqtail uint32 runq [256]guintptr // runnext, if non-nil, is a runnable G that was ready\u0026#39;d by // the current G and should be run next instead of what\u0026#39;s in // runq if there\u0026#39;s time remaining in the running G\u0026#39;s time // slice. It will inherit the time left in the current time // slice. If a set of goroutines is locked in a // communicate-and-wait pattern, this schedules that set as a // unit and eliminates the (potentially large) scheduling // latency that otherwise arises from adding the ready\u0026#39;d // goroutines to the end of the run queue. runnext guintptr //... } runqget # 实现是由runqget函数完成的: 首先查看runnext成员是否为空，如果不为空则返回runnext所指的Goroutine，并把runnext成员清零; 如果runnext为空，则继续从本地循环队列中查找Goroutine. // Get g from local runnable queue. // If inheritTime is true, gp should inherit the remaining time in the // current time slice. Otherwise, it should start a new time slice. // Executed only by the owner P. func runqget(_p_ *p) (gp *g, inheritTime bool) { // If there\u0026#39;s a runnext, it\u0026#39;s the next G to run. for { next := _p_.runnext if next == 0 { // runnext是空的,break for loop,然后从队列里面拿 break } if _p_.runnext.cas(next, 0) { return next.ptr(), true } } for { h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with other consumers t := _p_.runqtail if t == h { // 如果头等于尾，证明是队列是空的 return nil, false } gp := _p_.runq[h%uint32(len(_p_.runq))].ptr() if atomic.CasRel(\u0026amp;_p_.runqhead, h, h+1) { // cas-release, commits consume return gp, false } } } P的本地队列是怎么构成的 # 从上面已经知道本地运行G队列是由这三个field组成的，那么他们是怎么组织的？\ntype p struct{ //... runqhead uint32 runqtail uint32 runq [256]guintptr // runnext, if non-nil, is a runnable G that was ready\u0026#39;d by // the current G and should be run next instead of what\u0026#39;s in // runq if there\u0026#39;s time remaining in the running G\u0026#39;s time // slice. It will inherit the time left in the current time // slice. If a set of goroutines is locked in a // communicate-and-wait pattern, this schedules that set as a // unit and eliminates the (potentially large) scheduling // latency that otherwise arises from adding the ready\u0026#39;d // goroutines to the end of the run queue. runnext guintptr //... } 通过这两个函数:runqget与runqput,来看下它们是怎么运作的,我们先列出入队和出队的代码,来看一下。\n/* ----------------here: 这里是入队，把Goroutine放入本地运行队列 */ h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with consumers t := _p_.runqtail if t-h \u0026lt; uint32(len(_p_.runq)) { _p_.runq[t%uint32(len(_p_.runq))].set(gp) atomic.StoreRel(\u0026amp;_p_.runqtail, t+1) // store-release, makes the item available for consumption,runqhead不动,runqtail加一. return } //---------------------------------------------------------------------------------------------------------- for { h := atomic.LoadAcq(\u0026amp;_p_.runqhead) // load-acquire, synchronize with other consumers t := _p_.runqtail if t == h { // 如果头等于尾，证明是队列是空的 return nil, false } gp := _p_.runq[h%uint32(len(_p_.runq))].ptr() if atomic.CasRel(\u0026amp;_p_.runqhead, h, h+1) { // cas-release, commits consume,runqhead加一,runqtail不动. return gp, false } } 入队 runqhead不动,runqtail加一 出队 runqhead加一,runqtail不动. 为什么要用len # 它的那个gp := _p_.runq[h%uint32(len(_p_.runq))].ptr()为什么不直接用256，而需要用到len(_p_.runq),如果是为了以后的扩展，那么可以定义一个常量来替代就行了.\npackage main import ( \u0026#34;fmt\u0026#34; ) const lenghtForQueue = 256 var lenghtForQueueVar = 256 func main() { //var runq [lenghtForQueue]uintptr var runq [lenghtForQueueVar]uintptr fmt.Println(len(runq)) } 不能使用变量,只能使用常量：gdb/main.go:23:15: non-constant array bound lenghtForQueueVar.\n"}]